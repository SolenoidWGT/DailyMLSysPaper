[
    {
        "title": "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs",
        "url": "http://arxiv.org/abs/2407.03234v1",
        "pub_date": "2024-07-03",
        "summary": "When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\nmade available at https://github.com/Linlt-leon/Adversarial-Alignments.",
        "translated": "当 LLM 部署在敏感的、面向人的设置中时，关键是不要输出不安全的、有偏见的或侵犯隐私的输出。出于这个原因，模特们都接受过训练，也接受过指导，拒绝回答诸如“告诉我如何制造炸弹”之类的不安全提示我们发现，尽管有这些保护措施，仅仅通过在模型输入的末尾添加一个空格，就有可能打破模型的防御。在一项对八个开源模型的研究中，我们证明了这种攻击足够强大，足以导致大多数模型产生有害的输出，成功率非常高。我们检查了这种行为的原因，发现在标记化的训练数据中出现单个空格的上下文鼓励模型在提示时生成列表，覆盖训练信号以拒绝回答不安全的请求。我们的研究结果强调了当前模型对齐的脆弱状态，并提出了开发更强大的对齐方法的重要性。代码和数据将在 https://github.com/linlt-leon/adversarial-alignments 公布。"
    },
    {
        "title": "Bridging Model Heterogeneity in Federated Learning via Uncertainty-based\n  Asymmetrical Reciprocity Learning",
        "url": "http://arxiv.org/abs/2407.03247v1",
        "pub_date": "2024-07-03",
        "summary": "This paper presents FedType, a simple yet pioneering framework designed to\nfill research gaps in heterogeneous model aggregation within federated learning\n(FL). FedType introduces small identical proxy models for clients, serving as\nagents for information exchange, ensuring model security, and achieving\nefficient communication simultaneously. To transfer knowledge between large\nprivate and small proxy models on clients, we propose a novel uncertainty-based\nasymmetrical reciprocity learning method, eliminating the need for any public\ndata. Comprehensive experiments conducted on benchmark datasets demonstrate the\nefficacy and generalization ability of FedType across diverse settings. Our\napproach redefines federated learning paradigms by bridging model\nheterogeneity, eliminating reliance on public data, prioritizing client\nprivacy, and reducing communication costs.",
        "authors": "Jiaqi Wang, Chenxu Zhao, Lingjuan Lyu, Quanzeng You, Mengdi Huai, Fenglong Ma",
        "translated": "本文介绍了 FedType，这是一个简单而具有开创性的框架，旨在填补联邦学习(FL)中异构模型聚合的研究空白。FedType 为客户端引入了小型相同的代理模型，作为信息交换的代理，保证了模型的安全性，同时实现了高效的通信。针对大型私有代理模型和小型代理模型之间的知识传递问题，提出了一种基于不确定性的非对称互惠学习方法，该方法不需要任何公开数据。在基准数据集上进行的综合实验证明了 FedType 在不同环境下的有效性和推广能力。我们的方法通过桥接模型异构性、消除对公共数据的依赖、优先考虑客户隐私和降低通信成本来重新定义联邦学习范例。"
    },
    {
        "title": "Streaming Large-Scale Electron Microscopy Data to a Supercomputing\n  Facility",
        "url": "http://arxiv.org/abs/2407.03215v1",
        "pub_date": "2024-07-03",
        "summary": "Data management is a critical component of modern experimental workflows. As\ndata generation rates increase, transferring data from acquisition servers to\nprocessing servers via conventional file-based methods is becoming increasingly\nimpractical. The 4D Camera at the National Center for Electron Microscopy\n(NCEM) generates data at a nominal rate of 480 Gbit/s (87,000 frames/s)\nproducing a 700 GB dataset in fifteen seconds. To address the challenges\nassociated with storing and processing such quantities of data, we developed a\nstreaming workflow that utilizes a high-speed network to connect the 4D\nCamera's data acquisition (DAQ) system to supercomputing nodes at the National\nEnergy Research Scientific Computing Center (NERSC), bypassing intermediate\nfile storage entirely. In this work, we demonstrate the effectiveness of our\nstreaming pipeline in a production setting through an hour-long experiment that\ngenerated over 10 TB of raw data, yielding high-quality datasets suitable for\nadvanced analyses. Additionally, we compare the efficacy of this streaming\nworkflow against the conventional file-transfer workflow by conducting a\npost-mortem analysis on historical data from experiments performed by real\nusers. Our findings show that the streaming workflow significantly improves\ndata turnaround time, enables real-time decision-making, and minimizes the\npotential for human error by eliminating manual user interactions.",
        "authors": "Samuel S. Welborn, Chris Harris, Stephanie M. Ribet, Georgios Varnavides, Colin Ophus, Bjoern Enders, Peter Ercius",
        "translated": "数据管理是现代实验工作流的重要组成部分。随着数据生成率的提高，通过传统的基于文件的方法将数据从采集服务器传输到处理服务器正变得越来越不切实际。美国国家电子显微镜中心(NCEM)的4D 相机以480Gbit/s (87000帧/s)的标称速率生成数据，在15秒内产生700GB 的数据集。为了解决存储和处理这样大量数据的挑战，我们开发了一个流式工作流，利用高速网络将4D 摄像机的数据采集(DAQ)系统连接到国家能源研究科学计算中心(NERSC)的超级计算节点，完全绕过中间文件存储。在这项工作中，我们通过一个小时的实验，生成了超过10TB 的原始数据，产生了适合高级分析的高质量数据集，从而证明了我们的流式流水线在生产环境中的有效性。此外，我们通过对实际用户实验中的历史数据进行事后分析，比较了该流工作流与传统文件传输工作流的效率。我们的研究结果表明，流式工作流可以显著提高数据周转时间，实现实时决策，并通过消除人工用户交互，最大限度地减少潜在的人为错误。"
    },
    {
        "title": "Effective Heterogeneous Federated Learning via Efficient\n  Hypernetwork-based Weight Generation",
        "url": "http://arxiv.org/abs/2407.03086v1",
        "pub_date": "2024-07-03",
        "summary": "While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86 times compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.",
        "authors": "Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim, JeongGil Ko",
        "translated": "虽然联邦学习利用分布式客户端资源，但是由于异构客户端功能，它面临着挑战。这就需要分配适合客户端资源的模型和仔细的参数聚合来适应这种异构性。我们提出了一种新的联邦学习框架 HypeMeFed，它通过将多出口网络结构与基于超网络的模型权重生成相结合来支持客户端的异构性。该方法对异构模型层的特征空间进行对齐，解决了权重聚合过程中各层的信息差异问题。为了实现 HypeMeFed，我们还提出了一种低秩因子分解方法，以最小化与超网络相关的计算和内存开销。我们对现实世界异构设备测试台的评估表明，与天真的超网络方法相比，HypeMeFed 比 FedAvg 提高了5.12% 的准确性，减少了98.22% 的超网络内存需求，并将其操作加速了1.86倍。这些结果证明了 HypeMeFed 在利用和吸引异构客户机进行联合学习方面的有效性。"
    },
    {
        "title": "On the Client Preference of LLM Fine-tuning in Federated Learning",
        "url": "http://arxiv.org/abs/2407.03038v1",
        "pub_date": "2024-07-03",
        "summary": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained\nlarge language model (LLM) using preference datasets, enabling the LLM to\ngenerate outputs that align with human preferences. Given the sensitive nature\nof these preference datasets held by various clients, there is a need to\nimplement RLHF within a federated learning (FL) framework, where clients are\nreluctant to share their data due to privacy concerns. To address this, we\nintroduce a feasible framework in which clients collaboratively train a binary\nselector with their preference datasets using our proposed FedBis. With a\nwell-trained selector, we can further enhance the LLM that generates\nhuman-preferred completions. Meanwhile, we propose a novel algorithm,\nFedBiscuit, that trains multiple selectors by organizing clients into balanced\nand disjoint clusters based on their preferences. Compared to the FedBis,\nFedBiscuit demonstrates superior performance in simulating human preferences\nfor pairwise completions. Our extensive experiments on federated human\npreference datasets -- marking the first benchmark to address heterogeneous\ndata partitioning among clients -- demonstrate that FedBiscuit outperforms\nFedBis and even surpasses traditional centralized training.",
        "authors": "Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Jing Gao",
        "translated": "强化学习反馈(rlHF)使用偏好数据集对预先训练好的大语言模型(LLM)进行微调，使 LLM 能够生成符合人类偏好的输出。鉴于这些偏好数据集由不同的客户端持有的敏感性质，有必要在联邦学习(FL)框架内实现 RLHF，其中客户端由于隐私问题不愿意共享他们的数据。为了解决这个问题，我们引入了一个可行的框架，在这个框架中，客户端使用我们提出的 FedBis 协作地训练一个二进制选择器和他们的偏好数据集。使用训练有素的选择器，我们可以进一步增强生成人类首选完成的 LLM。同时，我们提出了一种新的算法，FedBiscookie，通过将客户端根据他们的偏好组织成平衡的和不相交的集群来训练多个选择器。与 FedBis 相比，FedBiscookie 在模拟人类对配对完成的偏好方面表现出了优越的性能。我们在联邦人类偏好数据集上的广泛实验——标志着解决客户端之间异构数据分区的第一个基准——证明了 FedBiscue 的性能优于 FedBis，甚至超过了传统的集中式训练。"
    },
    {
        "title": "DRLQ: A Deep Reinforcement Learning-based Task Placement for Quantum\n  Cloud Computing",
        "url": "http://arxiv.org/abs/2407.02748v1",
        "pub_date": "2024-07-03",
        "summary": "The quantum cloud computing paradigm presents unique challenges in task\nplacement due to the dynamic and heterogeneous nature of quantum computation\nresources. Traditional heuristic approaches fall short in adapting to the\nrapidly evolving landscape of quantum computing. This paper proposes DRLQ, a\nnovel Deep Reinforcement Learning (DRL)-based technique for task placement in\nquantum cloud computing environments, addressing the optimization of task\ncompletion time and quantum task scheduling efficiency. It leverages the Deep Q\nNetwork (DQN) architecture, enhanced with the Rainbow DQN approach, to create a\ndynamic task placement strategy. This approach is one of the first in the field\nof quantum cloud resource management, enabling adaptive learning and\ndecision-making for quantum cloud environments and effectively optimizing task\nplacement based on changing conditions and resource availability. We conduct\nextensive experiments using the QSimPy simulation toolkit to evaluate the\nperformance of our method, demonstrating substantial improvements in task\nexecution efficiency and a reduction in the need to reschedule quantum tasks.\nOur results show that utilizing the DRLQ approach for task placement can\nsignificantly reduce total quantum task completion time by 37.81% to 72.93% and\nprevent task rescheduling attempts compared to other heuristic approaches.",
        "authors": "Hoa T. Nguyen, Muhammad Usman, Rajkumar Buyya",
        "translated": "由于量子计算资源的动态性和异构性，量子云计算范式在任务配置中提出了独特的挑战。传统的启发式方法不能适应量子计算的快速发展。本文提出了基于深度强化学习的量子云计算任务分配技术 DRLQ，解决了任务完成时间和量子任务调度效率的优化问题。它利用 Deep Q Network (DQN)体系结构，并通过 Rainbow DQN 方法得到了增强，从而创建了一个动态任务分配策略。这种方法是量子云资源管理领域的首创之一，能够为量子云环境提供在线机机器学习和决策支持，并根据不断变化的条件和资源可用性有效地优化任务分配。我们使用 QSimPy 模拟工具包进行了广泛的实验，以评估我们的方法的性能，证明了任务执行效率的实质性改进和重新调度量子任务的需求的减少。实验结果表明，与其他启发式方法相比，利用 DRLQ 方法进行任务分配可以显著减少任务完成总时间37.81% 到72.93% ，并且可以防止任务重调度尝试。"
    },
    {
        "title": "Accelerating Distributed Optimization: A Primal-Dual Perspective on\n  Local Steps",
        "url": "http://arxiv.org/abs/2407.02689v1",
        "pub_date": "2024-07-02",
        "summary": "In distributed machine learning, efficient training across multiple agents\nwith different data distributions poses significant challenges. Even with a\ncentralized coordinator, current algorithms that achieve optimal communication\ncomplexity typically require either large minibatches or compromise on gradient\ncomplexity. In this work, we tackle both centralized and decentralized settings\nacross strongly convex, convex, and nonconvex objectives. We first demonstrate\nthat a basic primal-dual method, (Accelerated) Gradient Ascent Multiple\nStochastic Gradient Descent (GA-MSGD), applied to the Lagrangian of distributed\noptimization inherently incorporates local updates, because the inner loops of\nrunning Stochastic Gradient Descent on the primal variable require no\ninter-agent communication. Notably, for strongly convex objectives, we show\n(Accelerated) GA-MSGD achieves linear convergence in communication rounds\ndespite the Lagrangian being only linear in the dual variables. This is due to\na unique structural property where the dual variable is confined to the span of\nthe coupling matrix, rendering the dual problem strongly concave. When\nintegrated with the Catalyst framework, our approach achieves nearly optimal\ncommunication complexity across various settings without the need for\nminibatches. Moreover, in stochastic decentralized problems, it attains\ncommunication complexities comparable to those in deterministic settings,\nimproving over existing algorithms.",
        "authors": "Junchi Yang, Murat Yildirim, Qiu Feng",
        "translated": "在分布式机器学习中，对具有不同数据分布的多代理进行有效的训练是一个巨大的挑战。即使有一个集中的协调器，当前的算法，以实现最佳的通信复杂度通常需要或者大的微型批量或梯度复杂度折衷。在这项工作中，我们处理集中和分散设置跨强烈凸，凸和非凸的目标。我们首先证明了一个基本的原始-对偶方法，(加速)梯度上升多重随机梯度下降(GA-MSGD) ，应用于分布式优化的拉格朗日方法，固有地结合了局部更新，因为在原始变量上运行的随机梯度下降的内部循环不需要代理之间的通信。值得注意的是，对于强凸目标，我们显示(加速) GA-MSGD 实现线性收敛的通信轮，尽管拉格朗日只是线性对偶变量。这是由于一个独特的结构性质，其中对偶变量限制在耦合矩阵的跨度，使对偶问题强烈凹。当与 Catalyst 框架集成时，我们的方法在不需要小批处理的情况下实现了几乎最佳的跨各种设置的通信复杂性。此外，在随机分散问题中，它获得了与确定性设置中的通信复杂度相当的通信复杂度，比现有算法有所改进。"
    },
    {
        "title": "Towards Federated Learning with On-device Training and Communication in\n  8-bit Floating Point",
        "url": "http://arxiv.org/abs/2407.02610v1",
        "pub_date": "2024-07-02",
        "summary": "Recent work has shown that 8-bit floating point (FP8) can be used for\nefficiently training neural networks with reduced computational overhead\ncompared to training in FP32/FP16. In this work, we investigate the use of FP8\ntraining in a federated learning context. This brings not only the usual\nbenefits of FP8 which are desirable for on-device training at the edge, but\nalso reduces client-server communication costs due to significant weight\ncompression. We present a novel method for combining FP8 client training while\nmaintaining a global FP32 server model and provide convergence analysis.\nExperiments with various machine learning models and datasets show that our\nmethod consistently yields communication reductions of at least 2.9x across a\nvariety of tasks and models compared to an FP32 baseline.",
        "authors": "Bokun Wang, Axel Berg, Durmus Alp Emre Acar, Chuteng Zhou",
        "translated": "最近的研究表明，与 FP32/FP16相比，8位浮点数(FP8)可以用来有效地训练神经网络，减少计算开销。在这项工作中，我们研究了 FP8训练在联邦学习环境中的应用。这不仅带来了 FP8的通常好处，这对边缘设备上的培训是可取的，而且由于显著的权重压缩，还降低了客户机-服务器通信成本。我们提出了一种新的方法，结合 FP8客户端训练，同时维护一个全球性的 FP32服务器模型，并提供了收敛性分析。对各种机器学习模型和数据集的实验表明，与 FP32基线相比，我们的方法在各种任务和模型之间始终产生至少2.9倍的通信减少。"
    },
    {
        "title": "Decentralized Intelligence Network (DIN)",
        "url": "http://arxiv.org/abs/2407.02461v1",
        "pub_date": "2024-07-02",
        "summary": "Decentralized Intelligence Network (DIN) addresses the significant challenges\nof data sovereignty and AI utilization caused by the fragmentation and siloing\nof data across providers and institutions. This comprehensive framework\novercomes access barriers to scalable data sources previously hindered by silos\nby leveraging: 1) personal data stores as a prerequisite for data sovereignty;\n2) a scalable federated learning protocol implemented on a public blockchain\nfor decentralized AI training, where data remains with participants and only\nmodel parameter updates are shared; and 3) a scalable, trustless rewards\nmechanism to incentivize participation and ensure fair reward distribution.\nThis framework ensures that no entity can prevent or control access to training\non data offered by participants or determine financial benefits, as these\nprocesses operate on a public blockchain with an immutable record and without a\nthird party. It supports effective AI training, allowing participants to\nmaintain control over their data, benefit financially, and contribute to a\ndecentralized, scalable ecosystem that leverages collective AI to develop\nbeneficial algorithms.",
        "authors": "Abraham Nash",
        "translated": "分散智能网(DIN)解决了数据主权和人工智能利用方面的重大挑战，这些挑战是由于数据在供应商和机构之间的碎片化和孤立性造成的。这个全面的框架通过利用以下方面克服了以前受到竖井阻碍的可扩展数据源的访问障碍: 1)个人数据存储作为数据主权的先决条件; 2)在公共区块链上实施的可扩展联合学习协议，用于分散的 AI 培训，其中数据保留在参与者身上，只有模型参数更新被共享; 3)可扩展的、不可信任的奖励机制，以激励参与并确保公平的奖励分配。这一框架确保任何实体都无法防止或控制获得参与者提供的数据培训或确定经济利益，因为这些过程是在公共区块链上运作的，具有不可变的记录，没有第三方。它支持有效的人工智能培训，允许参与者保持对他们的数据的控制，从经济上获益，并促进一个分散的，可扩展的生态系统，利用集体人工智能开发有益的算法。"
    },
    {
        "title": "Uncertainty-Aware Decarbonization for Datacenters",
        "url": "http://arxiv.org/abs/2407.02390v1",
        "pub_date": "2024-07-02",
        "summary": "This paper represents the first effort to quantify uncertainty in carbon\nintensity forecasting for datacenter decarbonization. We identify and analyze\ntwo types of uncertainty -- temporal and spatial -- and discuss their system\nimplications. To address the temporal dynamics in quantifying uncertainty for\ncarbon intensity forecasting, we introduce a conformal prediction-based\nframework. Evaluation results show that our technique robustly achieves target\ncoverages in uncertainty quantification across various significance levels. We\nconduct two case studies using production power traces, focusing on temporal\nand spatial load shifting respectively. The results show that incorporating\nuncertainty into scheduling decisions can prevent a 5% and 14% increase in\ncarbon emissions, respectively. These percentages translate to an absolute\nreduction of 2.1 and 10.4 tons of carbon emissions in a 20 MW datacenter\ncluster.",
        "authors": "Amy Li, Sihang Liu, Yi Ding",
        "translated": "本文首次尝试对数据中心脱碳过程中碳强度预测的不确定性进行量化。我们识别和分析了两种类型的不确定性——时间和空间——并讨论了它们的系统含义。为了解决碳强度预测不确定性量化的时间动态问题，我们引入了一个基于保形预测的框架。评价结果表明，该方法在不同的显著性水平上均能稳健地实现不确定性量化的目标覆盖。我们使用生产功率轨迹进行了两个案例研究，分别侧重于时间和空间负荷转移。结果表明，将不确定性纳入调度决策可以分别防止5% 和14% 的碳排放增加。这些百分比意味着在一个20兆瓦的数据中心集群中，碳排放绝对减少了2.1吨和10.4吨。"
    },
    {
        "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across\n  Hybrid Devices",
        "url": "http://arxiv.org/abs/2407.02327v1",
        "pub_date": "2024-07-02",
        "summary": "A number of production deep learning clusters have attempted to explore\ninference hardware for DNN training, at the off-peak serving hours with many\ninference GPUs idling. Conducting DNN training with a combination of\nheterogeneous training and inference GPUs, known as hybrid device training,\npresents considerable challenges due to disparities in compute capability and\nsignificant differences in memory capacity. We propose QSync, a training system\nthat enables efficient synchronous data-parallel DNN training over hybrid\ndevices by strategically exploiting quantized operators. According to each\ndevice's available resource capacity, QSync selects a quantization-minimized\nsetting for operators in the distributed DNN training graph, minimizing model\naccuracy degradation but keeping the training efficiency brought by\nquantization. We carefully design a predictor with a bi-directional\nmixed-precision indicator to reflect the sensitivity of DNN layers on\nfixed-point and floating-point low-precision operators, a replayer with a\nneighborhood-aware cost mapper to accurately estimate the latency of\ndistributed hybrid mixed-precision training, and then an allocator that\nefficiently synchronizes workers with minimized model accuracy degradation.\nQSync bridges the computational graph on PyTorch to an optimized backend for\nquantization kernel performance and flexible support for various GPU\narchitectures. Extensive experiments show that QSync's predictor can accurately\nsimulate distributed mixed-precision training with &lt;5% error, with a consistent\n0.27-1.03% accuracy improvement over the from-scratch training tasks compared\nto uniform precision.",
        "authors": "Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Yibo Zhu, Chuan Wu",
        "translated": "一些生产深度学习集群试图探索 DNN 培训的推理硬件，在非高峰服务时间与许多推理 GPU 闲置。用异构训练和推理 GPU (称为混合设备训练)组合进行 DNN 训练，由于计算能力的差异和内存容量的显著差异，提出了相当大的挑战。我们提出 QSync，一个训练系统，使有效的同步数据并行 DNN 训练超过混合设备的战略利用量化操作员。QSync 根据每个设备的可用资源容量，在分布式 DNN 训练图中为操作者选择一个量化最小化的设置，最小化模型精度的降低，同时保持量化带来的训练效率。我们精心设计了一个双向混合精度指标的预测器来反映 DNN 层对定点和浮点低精度算子的敏感性，一个具有邻域感知成本映射器的中继器来精确估计分布式混合精度训练的延迟，然后一个分配器来有效地同步工人，最小化模型精度退化。QSync 将 PyTorch 上的计算图连接到一个优化的后端，用于量化内核性能和对各种 GPU 架构的灵活支持。大量实验表明，QSync 预测器能够准确地模拟分布式混合精度训练，误差小于5% ，与均匀精度训练相比，准确率提高了0.27 -1.03% 。"
    },
    {
        "title": "Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models\n  with Adaptive Expert Placement",
        "url": "http://arxiv.org/abs/2407.04656v1",
        "pub_date": "2024-07-05",
        "summary": "Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly\nbeen adopted to further scale large language models (LLMs) due to its\nsub-linear scaling for computation costs. However, frequent failures still pose\nsignificant challenges as training scales. The cost of even a single failure is\nsignificant, as all GPUs need to wait idle until the failure is resolved,\npotentially losing considerable training progress as training has to restart\nfrom checkpoints. Existing solutions for efficient fault-tolerant training\neither lack elasticity or rely on building resiliency into pipeline\nparallelism, which cannot be applied to MoE models due to the expert\nparallelism strategy adopted by the MoE architecture.\n  We present Lazarus, a system for resilient and elastic training of MoE\nmodels. Lazarus adaptively allocates expert replicas to address the inherent\nimbalance in expert workload and speeds-up training, while a provably optimal\nexpert placement algorithm is developed to maximize the probability of recovery\nupon failures. Through adaptive expert placement and a flexible token\ndispatcher, Lazarus can also fully utilize all available nodes after failures,\nleaving no GPU idle. Our evaluation shows that Lazarus outperforms existing MoE\ntraining systems by up to 5.7x under frequent node failures and 3.4x on a real\nspot instance trace.",
        "authors": "Yongji Wu, Wenjie Qu, Tianyang Tao, Zhuang Wang, Wei Bai, Zhuohao Li, Yuan Tian, Jiaheng Zhang, Matthew Lentz, Danyang Zhuo",
        "translated": "稀疏激活的混合专家(MoE)体系结构由于其计算开销的次线性扩展性，越来越多地被用于进一步扩展大型语言模型(LLM)。然而，频繁的失败仍然给培训规模带来了巨大的挑战。即使是单个故障的代价也是巨大的，因为所有的 GPU 都需要等待空闲，直到故障得到解决，这可能会失去相当大的训练进度，因为训练必须从检查点重新开始。现有的高效容错培训解决方案要么缺乏弹性，要么依赖于将弹性构建为流水线并行性，由于教育部体系结构采用的专家并行策略，这些解决方案不能应用于教育部模型。我们提出 Lazarus，一个系统的弹性和弹性训练的 MoE 模型。Lazarus 自适应地分配专家副本，以解决专家工作量固有的不平衡问题，加快训练速度，同时开发了一个可证明的最优专家配置算法，以最大限度地提高故障恢复的概率。通过自适应专家布局和灵活的令牌调度器，Lazarus 还可以在故障后充分利用所有可用的节点，不让 GPU 处于空闲状态。我们的评估表明，Lazarus 在频繁节点故障下的性能比现有的 MoE 培训系统高出5.7倍，在实际现场实例跟踪上的性能高出3.4倍。"
    },
    {
        "title": "Accelerating Communication in Deep Learning Recommendation Model\n  Training with Dual-Level Adaptive Lossy Compression",
        "url": "http://arxiv.org/abs/2407.04272v1",
        "pub_date": "2024-07-05",
        "summary": "DLRM is a state-of-the-art recommendation system model that has gained\nwidespread adoption across various industry applications. The large size of\nDLRM models, however, necessitates the use of multiple devices/GPUs for\nefficient training. A significant bottleneck in this process is the\ntime-consuming all-to-all communication required to collect embedding data from\nall devices. To mitigate this, we introduce a method that employs error-bounded\nlossy compression to reduce the communication data size and accelerate DLRM\ntraining. We develop a novel error-bounded lossy compression algorithm,\ninformed by an in-depth analysis of embedding data features, to achieve high\ncompression ratios. Moreover, we introduce a dual-level adaptive strategy for\nerror-bound adjustment, spanning both table-wise and iteration-wise aspects, to\nbalance the compression benefits with the potential impacts on accuracy. We\nfurther optimize our compressor for PyTorch tensors on GPUs, minimizing\ncompression overhead. Evaluation shows that our method achieves a 1.38$\\times$\ntraining speedup with a minimal accuracy impact.",
        "authors": "Hao Feng, Boyuan Zhang, Fanjiang Ye, Min Si, Ching-Hsiang Chu, Jiannan Tian, Chunxing Yin,  Zhaoxia,  Deng, Yuchen Hao, Pavan Balaji, Tong Geng, Dingwen Tao",
        "translated": "DLRM 是最先进的推荐系统模型，已经在各种行业应用程序中得到广泛采用。然而，DLRM 模型的大规模需要使用多个设备/GPU 进行有效的培训。这个过程中的一个重要瓶颈是从所有设备收集嵌入数据所需的耗时的全对全通信。为了缓解这种情况，我们引入了一种使用错误限制有损数据压缩的方法，以减少通信数据量并加速 DLRM 培训。我们开发了一个新的错误限制有损数据压缩算法，通过深入分析嵌入数据特征，以实现高压缩比。此外，我们还引入了一个跨越表和迭代两个方面的误差范围调整的双层自适应策略，以平衡压缩优势和对精度的潜在影响。我们在 GPU 上进一步优化我们的 PyTorch 张量压缩器，使压缩开销最小化。评估结果表明，该方法在精度影响最小的情况下，获得了1.38美元的训练加速比。"
    },
    {
        "title": "A High-Quality Workflow for Multi-Resolution Scientific Data Reduction\n  and Visualization",
        "url": "http://arxiv.org/abs/2407.04267v1",
        "pub_date": "2024-07-05",
        "summary": "Multi-resolution methods such as Adaptive Mesh Refinement (AMR) can enhance\nstorage efficiency for HPC applications generating vast volumes of data.\nHowever, their applicability is limited and cannot be universally deployed\nacross all applications. Furthermore, integrating lossy compression with\nmulti-resolution techniques to further boost storage efficiency encounters\nsignificant barriers. To this end, we introduce an innovative workflow that\nfacilitates high-quality multi-resolution data compression for both uniform and\nAMR simulations. Initially, to extend the usability of multi-resolution\ntechniques, our workflow employs a compression-oriented Region of Interest\n(ROI) extraction method, transforming uniform data into a multi-resolution\nformat. Subsequently, to bridge the gap between multi-resolution techniques and\nlossy compressors, we optimize three distinct compressors, ensuring their\noptimal performance on multi-resolution data. Lastly, we incorporate an\nadvanced uncertainty visualization method into our workflow to understand the\npotential impacts of lossy compression. Experimental evaluation demonstrates\nthat our workflow achieves significant compression quality improvements.",
        "authors": "Daoce Wang, Pascal Grosset, Jesus Pulido, Tushar M. Athawale, Jiannan Tian, Kai Zhao, Zarija Lukic, Axel Huebl, Zhe Wang, James Ahrens, Dingwen Tao",
        "translated": "自适应网格细化(AMR)等多分辨率方法可以提高 HPC 应用程序生成大量数据的存储效率。但是，它们的适用性是有限的，并且不能在所有应用程序中普遍部署。此外，将有损数据压缩与多分辨率技术相结合以进一步提高存储效率会遇到重大障碍。为此，我们引入了一个创新的工作流程，为统一模拟和 AMR 模拟提供高质量的多分辨率数据压缩。首先，为了扩展多分辨率技术的可用性，我们的工作流采用了面向压缩的感兴趣区域(ROI)提取方法，将统一的数据转换成多分辨率格式。随后，为了弥补多分辨率技术和有损压缩器之间的差距，我们对三种不同的压缩器进行了优化，以确保它们在多分辨率数据上的最优性能。最后，我们将一种先进的不确定性可视化方法融入到我们的工作流程中，以了解有损数据压缩的潜在影响。实验结果表明，我们的工作流程实现了显著的压缩质量改进。"
    },
    {
        "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
        "url": "http://arxiv.org/abs/2407.04053v1",
        "pub_date": "2024-07-04",
        "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyse data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. The goal of Edge AI is to optimize data processing efficiency\nand velocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research, spanning from 2014 to the present, it has\nshown significant and rapid development over the last five years. In this\narticle, we present a systematic literature review for Edge AI to discuss the\nexisting research, recent advancements, and future research directions. We\ncreated a collaborative edge AI learning system for cloud and edge computing\nanalysis, including an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while also examining its potential influence\nacross many fields through compassing infrastructure, cloud computing, fog\ncomputing, services, use cases, ML and deep learning, and resource management.\nThis study highlights the significance of Edge AI in processing real-time data\nat the edge of the network. Additionally, it emphasizes the research challenges\nencountered by Edge AI systems, including constraints on resources,\nvulnerabilities to security threats, and problems with scalability. Finally,\nthis study highlights the potential future research directions that aim to\naddress the current limitations of Edge AI by providing innovative solutions.",
        "authors": "Sukhpal Singh Gill, Muhammed Golec, Jianmin Hu, Minxian Xu, Junhui Du, Huaming Wu, Guneet Kaur Walia, Subramaniam Subramanian Murugesan, Babar Ali, Mohit Kumar, Kejiang Ye, Prabal Verma, Surendra Kumar, Felix Cuadrado, Steve Uhlig",
        "translated": "边缘人工智能(AI)集成了一个互联系统和设备的网络，这些设备接收、缓存、处理和分析数据，与用 AI 技术捕获数据的位置进行密切通信。人工智能效率的最新进展，物联网(IoT)设备的广泛使用，以及边缘计算的出现，已经开启了边缘人工智能的巨大范围。边缘人工智能的目标是优化数据处理效率和速度，同时确保数据的机密性和完整性。尽管从2014年到现在，它还是一个相对较新的研究领域，但在过去的五年里，它已经显示出重大而迅速的发展。在本文中，我们提出了一个系统的文献综述，边缘人工智能讨论现有的研究，最近的进展和未来的研究方向。我们创建了一个用于云和边缘计算分析的协作式边缘 AI 学习系统，包括对促进这种机制的体系结构的深入研究。边缘人工智能的分类促进了边缘人工智能系统的分类和配置，同时也通过包括基础设施、云计算、雾计算、服务、用例、机器学习和深度学习以及资源管理在许多领域的潜在影响。该研究突出了边缘人工智能在处理网络边缘实时数据中的重要性。此外，它强调了边缘人工智能系统所面临的研究挑战，包括资源约束、安全威胁的脆弱性和可伸缩性问题。最后，本研究强调了未来可能的研究方向，旨在通过提供创新的解决方案来解决边缘人工智能目前的局限性。"
    },
    {
        "title": "Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM\n  Inference on Heterogeneous Systems",
        "url": "http://arxiv.org/abs/2407.04014v1",
        "pub_date": "2024-07-04",
        "summary": "The rapid adoption of large language models (LLMs) has led to significant\nadvances in natural language processing and text generation. However, the\nenergy consumed through LLM model inference remains a major challenge for\nsustainable AI deployment. To address this problem, we model the\nworkload-dependent energy consumption and runtime of LLM inference tasks on\nheterogeneous GPU-CPU systems. By conducting an extensive characterization\nstudy of several state-of-the-art LLMs and analyzing their energy and runtime\nbehavior across different magnitudes of input prompts and output text, we\ndevelop accurate (R^2&gt;0.96) energy and runtime models for each LLM. We employ\nthese models to explore an offline, energy-optimal LLM workload scheduling\nframework. Through a case study, we demonstrate the advantages of energy and\naccuracy aware scheduling compared to existing best practices.",
        "authors": "Grant Wilkins, Srinivasan Keshav, Richard Mortier",
        "translated": "大型语言模型(LLM)的迅速应用导致了自然语言处理和文本生成方面的重大进展。然而，通过 LLM 模型推理所消耗的能量仍然是可持续人工智能部署的一个主要挑战。为了解决这个问题，我们在异构 GPU-CPU 系统上建立了与工作负载相关的 LLM 推理任务的能量消耗和运行时模型。通过对几种最先进的 LLM 进行广泛的角色塑造研究，并分析它们在不同大小的输入提示和输出文本中的能量和运行时行为，我们为每种 LLM 开发了精确的(R ^ 2 > 0.96)能量和运行时模型。我们使用这些模型来探索一个离线的，能量最优的 LLM 工作负载调度框架。通过一个案例研究，我们证明了与现有的最佳实践相比，能源和精确性意识调度的优势。"
    },
    {
        "title": "PaSE: Parallelization Strategies for Efficient DNN Training",
        "url": "http://arxiv.org/abs/2407.04001v1",
        "pub_date": "2024-07-04",
        "summary": "Training a deep neural network (DNN) requires substantial computational and\nmemory requirements. It is common to use multiple devices to train a DNN to\nreduce the overall training time. There are several choices to parallelize each\nlayer in a DNN. Exhaustively searching this list to find an optimal\nparallelization strategy is prohibitively time consuming and impractical. The\nstandard practice is to use data parallelism because of its simplicity.\nHowever, data parallelism is often sub-optimal, and suffers from poor\nperformance and high memory requirement. Expert-designed strategies have been\nproposed on a case-by-case basis using domain specific knowledge. These\nexpert-designed strategies do not generalize well to DNNs other than the ones\nfor which they were designed, and are not always necessarily the best choice.\n  In this paper, we propose an approach to automatically find efficient\nparallelization strategies for DNNs from their computation graphs. We present\nan efficient algorithm to compute these strategies within a reasonable time in\npractice. We evaluate the effectiveness of our approach on various DNNs. We\nalso compare the performance of the strategies identified by our approach\nagainst data parallelism, expert-designed strategies, and the state-of-the-art\napproaches. Our results show that the strategies found using our approach\noutperform the baseline data parallelism strategy in all the cases. In\naddition, our strategies achieve better performance than the expert-designed\nstrategies and the state-of-the-art approaches.",
        "authors": "Venmugil Elango",
        "translated": "训练一个深层神经网络(DNN)需要大量的计算和内存需求。通常使用多种设备来训练一个 DNN，以减少整体训练时间。有几种选择可以并行化 DNN 中的每一层。彻底搜索此列表以找到最佳并行策略是非常耗时和不切实际的。标准做法是使用资料平行，因为它很简单。然而，资料平行往往是次优的，并且存在性能差和内存需求高的问题。利用具体领域的知识，逐案提出了专家设计的战略。这些专家设计的策略并不能很好地推广到 DNN 以外的其他设计策略，并不一定总是最佳选择。本文提出了一种从 DNN 的计算图中自动寻找有效并行策略的方法。我们提出了一个有效的算法，在合理的时间内计算这些策略在实践中。我们评估我们的方法在各种 DNN 上的有效性。我们还将我们的方法与资料平行、专家设计的策略和最先进的方法进行了比较。我们的研究结果表明，在所有情况下，使用我们的方法发现的策略都优于基线资料平行策略。此外，我们的策略比专家设计的策略和最先进的方法取得了更好的效果。"
    },
    {
        "title": "DEVS/SOA: A Cross-Platform Framework for Net-centric Modeling and\n  Simulation in DEVS Unified Process",
        "url": "http://arxiv.org/abs/2407.03686v1",
        "pub_date": "2024-07-04",
        "summary": "Discrete EVent Specification (DEVS) environments are known to be implemented\nover middleware systems such as HLA, RMI, CORBA and others. DEVS exhibits\nconcepts of systems theory and modeling and supports capturing the system\nbehavior from the physical and behavioral perspectives. Further, they are\nimplemented using Object-oriented languages like Java and C++. This research\nwork uses the Java platform to implement DEVS over a Service Oriented\nArchitecture (SOA) framework. Called the DEVS/SOA, the framework supports a\ndevelopment and testing environment known as DEVS Unified Process that is built\non a model-continuity-based life cycle methodology. DEVS Unified Process allows\nDEVS-based Modeling and Simulation (M&amp;S) over net-centric platforms using\nDEVS/SOA. This framework also provides the crucial feature of run-time\ncomposability of coupled systems using SOA. We describe the architecture and\ndesigns of the both the server and the client. The client application\ncommunicates with multiple servers hosting DEVS simulation services. These\nSimulation services are developed using the proposed symmetrical services\narchitecture wherein the server can act as both a service provider and a\nservice consumer contrary to the unidirectional client-server paradigm. We also\ndiscuss how this Services based architecture provides solutions for\ncross-platform distributed M&amp;S. We demonstrate DEVS/SOA framework with a\nscenario of Joint Close Air Support specified in Business Process Modeling\nNotation (BPMN). We also provide a real-world application of Network health\nmonitoring using DEVS/SOA layered architectural framework.",
        "authors": "Saurabh Mittal, José L. Risco-Martín, Bernard P. Zeigler",
        "translated": "众所周知，离散事件规范(DevS)环境是在 HLA、 RMI、 CORBA 等中间件系统上实现的。DevS 展示了系统理论和建模的概念，并支持从物理和行为角度捕获系统行为。此外，它们是使用面向对象的语言(如 Java 和 C + +)实现的。这项研究工作使用 Java 平台在一个面向服务的体系结构(SOA)框架下实现 DEVS。该框架被称为 devS/SOA，它支持一个名为 devS 统一过程的开发和测试环境，该环境建立在基于模型连续性的生命周期方法之上。在以网络为中心的平台上使用 devS/SOA，devS 统一过程允许基于 devS 的建模与模拟(M & S)。此框架还提供了使用 SOA 的耦合系统的运行时可组合性的关键特性。我们描述了服务器和客户端的体系结构和设计。客户端应用程序与多个承载 DevS 模拟服务的服务器通信。这些仿真服务是使用所提出的对称服务体系结构开发的，其中服务器既可以充当服务提供者，也可以充当服务使用者，这与单向客户机-服务器模式相反。我们还讨论了这种基于服务的体系结构如何为跨平台分布式 M & S 提供解决方案。我们用业务流程建模标记法中指定的联合近距离空中支援(bPMN)场景来演示 devS/SOA 框架。我们还提供了一个使用 DevS/SOA 分层体系结构框架的实际网络健康监控应用程序。"
    },
    {
        "title": "Loki: A System for Serving ML Inference Pipelines with Hardware and\n  Accuracy Scaling",
        "url": "http://arxiv.org/abs/2407.03583v1",
        "pub_date": "2024-07-04",
        "summary": "The rapid adoption of machine learning (ML) has underscored the importance of\nserving ML models with high throughput and resource efficiency. Traditional\napproaches to managing increasing query demands have predominantly focused on\nhardware scaling, which involves increasing server count or computing power.\nHowever, this strategy can often be impractical due to limitations in the\navailable budget or compute resources. As an alternative, accuracy scaling\noffers a promising solution by adjusting the accuracy of ML models to\naccommodate fluctuating query demands. Yet, existing accuracy scaling\ntechniques target independent ML models and tend to underperform while managing\ninference pipelines. Furthermore, they lack integration with hardware scaling,\nleading to potential resource inefficiencies during low-demand periods. To\naddress the limitations, this paper introduces Loki, a system designed for\nserving inference pipelines effectively with both hardware and accuracy\nscaling. Loki incorporates an innovative theoretical framework for optimal\nresource allocation and an effective query routing algorithm, aimed at\nimproving system accuracy and minimizing latency deadline violations. Our\nempirical evaluation demonstrates that through accuracy scaling, the effective\ncapacity of a fixed-size cluster can be enhanced by more than $2.7\\times$\ncompared to relying solely on hardware scaling. When compared with\nstate-of-the-art inference-serving systems, Loki achieves up to a $10\\times$\nreduction in Service Level Objective (SLO) violations, with minimal compromises\non accuracy and while fulfilling throughput demands.",
        "authors": "Sohaib Ahmad, Hui Guan, Ramesh K. Sitaraman",
        "translated": "机器学习(ML)的快速应用突出了为机器学习模型提供高吞吐量和资源效率服务的重要性。管理日益增长的查询需求的传统方法主要集中在硬件扩展上，这涉及到增加服务器数量或计算能力。但是，由于可用预算或计算资源的限制，这种策略通常不切实际。作为一种替代方案，精度缩放提供了一个有希望的解决方案，通过调整机器学习模型的精度，以适应波动的查询需求。然而，现有的精度缩放技术针对的是独立的机器学习模型，在管理推理流水线时往往表现不佳。此外，它们缺乏与硬件扩展的集成，导致在低需求时期潜在的资源低效。针对这些局限性，本文介绍了 Loki 系统，该系统设计用于有效地为推理管道提供硬件和精度扩展。Loki 采用了一个创新的资源优化分配理论框架和一个有效的查询路由算法，旨在提高系统的准确性和最大限度地减少延迟最后期限违规。我们的实证评估表明，通过精度扩展，固定规模集群的有效容量可以比单纯依靠硬件扩展提高2.7倍以上。与最先进的推理服务系统相比，Loki 在满足吞吐量需求的同时，在准确性方面做出最小的妥协，从而在服务水平目标(SLO)违规方面实现了高达10倍的减少。"
    },
    {
        "title": "A multigrid reduction framework for domains with symmetries",
        "url": "http://arxiv.org/abs/2407.05930v1",
        "pub_date": "2024-07-08",
        "summary": "Divergence constraints are present in the governing equations of many\nphysical phenomena, and they usually lead to a Poisson equation whose solution\ntypically is the main bottleneck of many simulation codes. Algebraic Multigrid\n(AMG) is arguably the most powerful preconditioner for Poisson's equation, and\nits effectiveness results from the complementary roles played by the smoother,\nresponsible for damping high-frequency error components, and the coarse-grid\ncorrection, which in turn reduces low-frequency modes. This work presents\nseveral strategies to make AMG more compute-intensive by leveraging reflection,\ntranslational and rotational symmetries, often present in academic and\nindustrial configurations. The best-performing method, AMGR, is based on a\nmultigrid reduction framework that introduces an aggressive coarsening to the\nmultigrid hierarchy, reducing the memory footprint, setup and application costs\nof the top-level smoother. While preserving AMG's excellent convergence, AMGR\nallows replacing the standard sparse matrix-vector product with the more\ncompute-intensive sparse matrix-matrix product, yielding significant\naccelerations. Numerical experiments on industrial CFD applications\ndemonstrated up to 70% speed-ups when solving Poisson's equation with AMGR\ninstead of AMG. Additionally, strong and weak scalability analyses revealed no\nsignificant degradation.",
        "authors": "Àdel Alsalti-Baldellou, Carlo Janna, Xavier Álvarez-Farré, F. Xavier Trias",
        "translated": "发散约束存在于许多物理现象的控制方程中，它们通常导致一个泊松方程，而泊松方程的求解通常是许多仿真程序的主要瓶颈。代数多重网格(AMG)可以说是泊松方程最强大的预处理器，其有效性源于平滑器(负责阻尼高频误差分量)和粗网格校正(反过来减少低频模式)的互补作用。这项工作提出了几个策略，使 AMG 更加计算密集型的利用反射，平移和旋转对称性，往往存在于学术和工业配置。最好的方法，AMGR，是基于一个多网格缩减框架，引入了一个积极的粗化多网格层次结构，减少内存占用，设置和应用成本的顶级平滑。在保持 AMG 的优良收敛性的同时，AMGR 允许用计算量更大的稀疏矩阵向量积取代标准的稀疏矩阵向量积，从而产生显著的加速度。工业 CFD 应用的数值实验表明，用 AMGR 代替 AMG 求解泊松方程可以提高70% 的速度。此外，强可伸缩性和弱可伸缩性分析显示没有明显的降解。"
    },
    {
        "title": "DFedSat: Communication-Efficient and Robust Decentralized Federated\n  Learning for LEO Satellite Constellations",
        "url": "http://arxiv.org/abs/2407.05850v1",
        "pub_date": "2024-07-08",
        "summary": "Low Earth Orbit (LEO) satellites play a crucial role in the development of 6G\nmobile networks and space-air-ground integrated systems. Recent advancements in\nspace technology have empowered LEO satellites with the capability to run AI\napplications. However, centralized approaches, where ground stations (GSs) act\nas servers and satellites as clients, often encounter slow convergence and\ninefficiencies due to intermittent connectivity between satellites and GSs. In\ncontrast, decentralized federated learning (DFL) offers a promising alternative\nby facilitating direct communication between satellites (clients) via\ninter-satellite links (ISLs). However, inter-plane ISLs connecting satellites\nfrom different orbital planes are dynamic due to Doppler shifts and pointing\nlimitations. This could impact model propagation and lead to slower\nconvergence. To mitigate these issues, we propose DFedSat, a fully\ndecentralized federated learning framework tailored for LEO satellites. DFedSat\naccelerates the training process by employing two adaptive mechanisms for\nintra-plane and inter-plane model aggregation, respectively. Furthermore, a\nself-compensation mechanism is integrated to enhance the robustness of\ninter-plane ISLs against transmission failure. Additionally, we derive the\nsublinear convergence rate for the non-convex case of DFedSat. Extensive\nexperimental results demonstrate DFedSat's superiority over other DFL baselines\nregarding convergence rate, communication efficiency, and resilience to\nunreliable links.",
        "authors": "Minghao Yang, Jingjing Zhang, Shengyun Liu",
        "translated": "近地轨道(LEO)卫星在6G 移动网络和空地一体化系统的发展中起着至关重要的作用。空间技术的最新进展使低地轨道卫星具有运行人工智能应用的能力。然而，由于地面站(GS)作为服务器，卫星作为客户端，集中式方法经常遇到收敛缓慢和效率低下的问题，这是由于卫星和 GS 之间的间歇性连接。相比之下，分散联邦学习(DFL)通过促进卫星(客户)之间通过卫星间链路(ISL)的直接通信，提供了一个有前途的替代方案。然而，由于多普勒频移和指向限制，从不同轨道平面连接卫星的平面间 ISL 是动态的。这可能会影响模型的传播，导致收敛速度较慢。为了缓解这些问题，我们提出了 DFedSat，一个为低轨道卫星量身定制的完全分散的联邦学习框架。DFedSat 采用两种自适应机制分别对平面内和平面间模型进行聚合，从而加速了训练过程。此外，自我补偿机制的集成，以增强平面间 ISL 对传输故障的鲁棒性。此外，我们还推导了 DFedSat 非凸情形下的次线性收敛速度。大量的实验结果表明，DFedSat 在收敛速度、通信效率和对不可靠链路的恢复能力方面优于其他 DFL 基线。"
    },
    {
        "title": "Cyber Physical Games",
        "url": "http://arxiv.org/abs/2407.05817v1",
        "pub_date": "2024-07-08",
        "summary": "We describe a formulation of multi-agents operating within a Cyber-Physical\nSystem, resulting in collaborative or adversarial games. We show that the\nnon-determinism inherent in the communication medium between agents and the\nunderlying physical environment gives rise to environment evolution that is a\nprobabilistic function of agents' strategies. We name these emergent properties\nCyber Physical Games and study its properties. We present an algorithmic model\nthat determines the most likely system evolution, approximating Cyber Physical\nGames through Probabilistic Finite State Automata, and evaluate it on\ncollaborative and adversarial versions of the Iterated Boolean Game, comparing\ntheoretical results with simulated ones. Results support the validity of the\nproposed model, and suggest several required research directions to continue\nevolving our understanding of Cyber Physical System, as well as how to best\ndesign agents that must operate within such environments.",
        "authors": "Warisa Sritriratanarak, Paulo Garcia",
        "translated": "我们描述了在一个网宇实体系统中运行的多代理的公式，导致了合作或对抗性的游戏。我们指出，代理人与底层物理环境之间的交流媒介所固有的非决定性导致了环境的演化，而环境演化是代理人策略的一个概率函数。我们命名这些涌现性质赛博物理游戏和研究其性质。我们提出了一个算法模型，确定最有可能的系统演化，近似网络物理游戏通过概率有限状态自动机，并评估它的协作和对手版本的迭代布尔游戏，比较理论结果与模拟的。结果支持提出的模型的有效性，并建议几个必要的研究方向，以继续发展我们对网络物理系统的理解，以及如何最佳设计代理必须在这样的环境中操作。"
    },
    {
        "title": "FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep\n  Reinforcement Learning for Medical Imaging",
        "url": "http://arxiv.org/abs/2407.05800v1",
        "pub_date": "2024-07-08",
        "summary": "Despite recent advancements in federated learning (FL) for medical image\ndiagnosis, addressing data heterogeneity among clients remains a significant\nchallenge for practical implementation. A primary hurdle in FL arises from the\nnon-IID nature of data samples across clients, which typically results in a\ndecline in the performance of the aggregated global model. In this study, we\nintroduce FedMRL, a novel federated multi-agent deep reinforcement learning\nframework designed to address data heterogeneity. FedMRL incorporates a novel\nloss function to facilitate fairness among clients, preventing bias in the\nfinal global model. Additionally, it employs a multi-agent reinforcement\nlearning (MARL) approach to calculate the proximal term $(\\mu)$ for the\npersonalized local objective function, ensuring convergence to the global\noptimum. Furthermore, FedMRL integrates an adaptive weight adjustment method\nusing a Self-organizing map (SOM) on the server side to counteract distribution\nshifts among clients' local data distributions. We assess our approach using\ntwo publicly available real-world medical datasets, and the results demonstrate\nthat FedMRL significantly outperforms state-of-the-art techniques, showing its\nefficacy in addressing data heterogeneity in federated learning. The code can\nbe found here~{\\url{https://github.com/Pranabiitp/FedMRL}}.",
        "authors": "Pranab Sahoo, Ashutosh Tripathi, Sriparna Saha, Samrat Mondal",
        "translated": "尽管联邦学习(FL)在医学图像诊断方面取得了最新进展，但是解决客户端之间的数据异构性仍然是实际应用中的一个重大挑战。FL 中的一个主要障碍来自跨客户端的数据样本的非 IID 性质，这通常会导致聚合全局模型的性能下降。在这项研究中，我们介绍了 FedMRL，一个新的联邦多代理深度强化学习框架，旨在解决数据异构性问题。FedMRL 采用了一种新的损失函数，以促进客户之间的公平性，防止最终全球模型的偏见。此外，它还采用多智能体强化学习(MARL)方法计算个性化局部目标函数的近似项 $(mu) $，确保收敛到全局最优。此外，FedMRL 整合了一个自适应权重调整方法，在服务器端使用自组织映射(SOM)来抵消客户本地数据分布之间的分布变化。我们使用两个公开可用的真实世界医学数据集评估我们的方法，结果表明 FedMRL 显着优于最先进的技术，显示其在解决联邦学习中的数据异质性方面的功效。代码可以在这里找到 ~ { url { https://github.com/pranabiitp/fedmrl }}。"
    },
    {
        "title": "Ten Years of Teaching Empirical Software Engineering in the context of\n  Energy-efficient Software",
        "url": "http://arxiv.org/abs/2407.05689v1",
        "pub_date": "2024-07-08",
        "summary": "In this chapter we share our experience in running ten editions of the Green\nLab course at the Vrije Universiteit Amsterdam, the Netherlands. The course is\ngiven in the Software Engineering and Green IT track of the Computer Science\nMaster program of the VU. The course takes place every year over a 2-month\nperiod and teaches Computer Science students the fundamentals of Empirical\nSoftware Engineering in the context of energy-efficient software. The\npeculiarity of the course is its research orientation: at the beginning of the\ncourse the instructor presents a catalog of scientifically relevant goals, and\neach team of students signs up for one of them and works together for 2 months\non their own experiment for achieving the goal. Each team goes over the classic\nsteps of an empirical study, starting from a precise formulation of the goal\nand research questions to context definition, selection of experimental\nsubjects and objects, definition of experimental variables, experiment\nexecution, data analysis, and reporting. Over the years, the course became\nwell-known within the Software Engineering community since it led to several\nscientific studies that have been published at various scientific conferences\nand journals. Also, students execute their experiments using\n\\textit{open-source tools}, which are developed and maintained by researchers\nand other students within the program, thus creating a virtuous community of\nlearners where students exchange ideas, help each other, and learn how to\ncollaboratively contribute to open-source projects in a safe environment.",
        "authors": "Ivano Malavolta, Vincenzo Stoico, Patricia Lago",
        "translated": "在本章中，我们将分享我们在荷兰阿姆斯特丹阿姆斯特丹自由大学举办十个版本的绿色实验课程的经验。该课程是在软件工程和绿色信息技术轨道的计算机科学硕士课程的 VU。本课程每年举行，为期两个月，教授计算机科学专业的学生经验软件工程的基础知识。这门课程的特殊之处在于它的研究方向: 在课程开始的时候，讲师会提出一系列与科学相关的目标，每组学生报名参加其中的一个，然后一起工作2个月，进行他们自己的实验，以达到这个目标。每个研究小组都要复习经典的实证研究步骤，从目标和研究问题的精确表述开始，到背景定义、实验对象和对象的选择、实验变量的定义、实验执行、数据分析和报告。多年来，这门课程在软件工程社区内广为人知，因为它导致了一些已经在各种科学会议和期刊上发表的科学研究。此外，学生使用文本{开源工具}执行他们的实验，这是由研究人员和其他学生在项目中开发和维护的，从而创造了一个良性的学习者社区，在这里学生交流思想，互相帮助，并学习如何在安全的环境中协作贡献开源项目。"
    },
    {
        "title": "Computational Power of Mobile Robots in Synchronous Environment:\n  Discrete Version",
        "url": "http://arxiv.org/abs/2407.05678v1",
        "pub_date": "2024-07-08",
        "summary": "In distributed computing by mobile robots, robots are deployed over a region,\ncontinuous or discrete, operating through a sequence of\n\\textit{look-compute-move} cycles. An extensive study has been carried out to\nunderstand the computational powers of different robot models. The models vary\non the ability to 1)~remember constant size information and 2)~communicate\nconstant size message. Depending on the abilities the different models are\n1)~$\\mathcal{OBLOT}$ (robots are oblivious and silent), 2)~$\\mathcal{FSTA}$\n(robots have finite states but silent), 3)~$\\mathcal{FCOM}$ (robots are\noblivious but can communicate constant size information) and,\n4)~$\\mathcal{LUMI}$ (robots have finite states and can communicate constant\nsize information). Another factor that affects computational ability is the\nscheduler that decides the activation time of the robots. The main three\nschedulers are \\textit{fully-synchronous}, \\textit{semi-synchronous} and\n\\textit{asynchronous}. Combining the models ($M$) with schedulers ($K$), we\nhave twelve combinations $M^K$.\n  In the euclidean domain, the comparisons between these twelve variants have\nbeen done in different works for transparent robots, opaque robots, and robots\nwith limited visibility. There is a vacant space for similar works when robots\nare operating on discrete regions like networks. It demands separate research\nattention because there have been a series of works where robots operate on\ndifferent networks, and there is a fundamental difference when robots are\noperating on a continuous domain versus a discrete domain in terms of robots'\nmovement. This work contributes to filling the space by giving a full\ncomparison table for all models with two synchronous schedulers:\nfully-synchronous and semi-synchronous.",
        "authors": "Avisek Sharma, Pritam Goswami, Buddhadeb Sau",
        "translated": "在移动机器人的分布式计算中，机器人被部署在一个连续或离散的区域，按照一系列的文本{ look-computer-move }周期进行操作。为了理解不同机器人模型的计算能力，人们进行了广泛的研究。这些模型的不同之处在于能够1) ~ 记住固定大小的信息和2) ~ 传递固定大小的信息。根据不同的能力，不同的模型有: 1) ~ $数学{ OBLOT } $(机器人是无意识的和沉默的) ，2) ~ $数学{ FSTA } $(机器人有有限的状态但是沉默的) ，3) ~ $数学{ FCOM } $(机器人是无意识的但是可以传递常数大小的信息) ，4) ~ $数学{ LUMI } $(机器人有有限的状态并且可以传递常数大小的信息)。影响计算能力的另一个因素是决定机器人激活时间的调度器。主要的三个调度程序分别是 texttit {全同步}、 texttit {半同步}和 texttit {异步}。结合模型($m $)和调度器($k $) ，我们有十二种组合 $m ^ k $。在欧几里得整环中，这十二个变种的比较已经在透明机器人、不透明机器人和能见度有限的机器人的不同作品中完成。当机器人在离散的区域(如网络)上操作时，类似的工作有一个空位。它需要单独的研究注意力，因为已经有一系列的工作机器人操作在不同的网络，有一个根本的区别，当机器人操作在一个连续的领域与一个离散的领域就机器人的运动而言。这项工作有助于填补空间，给出了一个完整的比较表的所有模型与两个同步调度器: 完全同步和半同步。"
    },
    {
        "title": "A Blockchain Embedded Peer-to-Peer Access Control Framework for IoT\n  Systems",
        "url": "http://arxiv.org/abs/2407.05506v1",
        "pub_date": "2024-07-07",
        "summary": "We consider access control for IoT systems that involves shared accesses to\nthe IoT devices as well as their data. Since IoT devices are dispersed all over\nthe edge of the Internet, traditional centralized access control has problems.\nBlockchain based decentralized access control is thus the new solution trend.\nHowever, existing blockchain based access control methods do not focus on\nperformance issues and may incur a high communication overhead.\n  In this paper, we develop a Pruned Blockchain based Access Control (PBAC)\nprotocol to cutdown the unnecessary message rounds and achieve high efficiency\nin access validations and policy management. The protocol includes a shortcut\nand a Role and Device Hierarchy-Based Access Control (R&amp;D-BAC) approaches for\ndifferent environment settings. To realize the PBAC protocol, it is necessary\nto carefully engineer the system architecture, which is also discussed in the\npaper. Experiments demonstrate the efficacy of the PBAC protocol, specifically,\nthe shortcut mechanism reduces access time by approximately 43%, and R&amp;D-BAC\noutperforms traditional blockchain based RBAC by more than two folds.",
        "authors": "Yongtao Huang, I-Ling Yen, Farokh Bastani",
        "translated": "我们考虑物联网系统的访问控制，包括对物联网设备及其数据的共享访问。由于物联网设备分散在互联网的各个边缘，传统的集中式访问控制存在问题。基于区块链的分散访问控制是解决这一问题的新趋势。然而，现有的基于区块链的访问控制方法并不关注性能问题，可能会导致较高的通信开销。本文提出了一种基于剪枝区块链的访问控制(PBAC)协议，以减少不必要的消息轮次，实现高效的访问验证和策略管理。该协议包括一个快捷方式和一个用于不同环境设置的基于角色和设备层次结构的访问控制(R & D-BAC)方法。为了实现 PBAC 协议，需要精心设计系统的体系结构，本文还对此进行了讨论。实验证明了 PBAC 协议的有效性，特别是该快捷机制将访问时间减少了约43% ，并且 R & D-BAC 的性能比传统的基于区块链的 RBAC 提高了两倍以上。"
    },
    {
        "title": "The infrastructure powering IBM's Gen AI model development",
        "url": "http://arxiv.org/abs/2407.05467v1",
        "pub_date": "2024-07-07",
        "summary": "AI Infrastructure plays a key role in the speed and cost-competitiveness of\ndeveloping and deploying advanced AI models. The current demand for powerful AI\ninfrastructure for model training is driven by the emergence of generative AI\nand foundational models, where on occasion thousands of GPUs must cooperate on\na single training job for the model to be trained in a reasonable time.\nDelivering efficient and high-performing AI training requires an end-to-end\nsolution that combines hardware, software and holistic telemetry to cater for\nmultiple types of AI workloads. In this report, we describe IBM's hybrid cloud\ninfrastructure that powers our generative AI model development. This\ninfrastructure includes (1) Vela: an AI-optimized supercomputing capability\ndirectly integrated into the IBM Cloud, delivering scalable, dynamic,\nmulti-tenant and geographically distributed infrastructure for large-scale\nmodel training and other AI workflow steps and (2) Blue Vela: a large-scale,\npurpose-built, on-premises hosting environment that is optimized to support our\nlargest and most ambitious AI model training tasks. Vela provides IBM with the\ndual benefit of high performance for internal use along with the flexibility to\nadapt to an evolving commercial landscape. Blue Vela provides us with the\nbenefits of rapid development of our largest and most ambitious models, as well\nas future-proofing against the evolving model landscape in the industry. Taken\ntogether, they provide IBM with the ability to rapidly innovate in the\ndevelopment of both AI models and commercial offerings.",
        "authors": "Talia Gershon, Seetharami Seelam, Brian Belgodere, Milton Bonilla, Lan Hoang, Danny Barnett, I-Hsin Chung, Apoorve Mohan, Ming-Hung Chen, Lixiang Luo, Robert Walkup, Constantinos Evangelinos, Shweta Salaria, Marc Dombrowa, Yoonho Park, Apo Kayi, Liran Schour, Alim Alim, Ali Sydney, Pavlos Maniotis, Laurent Schares, Bernard Metzler, Bengi Karacali-Akyamac, Sophia Wen, Tatsuhiro Chiba, Sunyanan Choochotkaew, Takeshi Yoshimura, Claudia Misale, Tonia Elengikal, Kevin O Connor, Zhuoran Liu, Richard Molina, Lars Schneidenbach, James Caden, Christopher Laibinis, Carlos Fonseca, Vasily Tarasov, Swaminathan Sundararaman, Frank Schmuck, Scott Guthridge, Jeremy Cohn, Marc Eshel, Paul Muench, Runyu Liu, William Pointer, Drew Wyskida, Bob Krull, Ray Rose, Brent Wolfe, William Cornejo, John Walter, Colm Malone, Clifford Perucci, Frank Franco, Nigel Hinds, Bob Calio, Pavel Druyan, Robert Kilduff, John Kienle, Connor McStay, Andrew Figueroa, Matthew Connolly, Edie Fost, Gina Roma, Jake Fonseca, Ido Levy, Michele Payne, Ryan Schenkel, Amir Malki, Lion Schneider, Aniruddha Narkhede, Shekeba Moshref, Alexandra Kisin, Olga Dodin, Bill Rippon, Henry Wrieth, John Ganci, Johnny Colino, Donna Habeger-Rose, Rakesh Pandey, Aditya Gidh, Aditya Gaur, Dennis Patterson, Samsuddin Salmani, Rambilas Varma, Rumana Rumana, Shubham Sharma, Aditya Gaur, Mayank Mishra, Rameswar Panda, Aditya Prasad, Matt Stallone, Gaoyuan Zhang, Yikang Shen, David Cox, Ruchir Puri, Dakshi Agrawal, Drew Thorstensen, Joel Belog, Brent Tang, Saurabh Kumar Gupta, Amitabha Biswas, Anup Maheshwari, Eran Gampel, Jason Van Patten, Matthew Runion, Sai Kaki, Yigal Bogin, Brian Reitz, Steve Pritko, Shahan Najam, Surya Nambala, Radhika Chirra, Rick Welp, Frank DiMitri, Felipe Telles, Amilcar Arvelo, King Chu, Ed Seminaro, Andrew Schram, Felix Eickhoff, William Hanson, Eric Mckeever, Dinakaran Joseph, Piyush Chaudhary, Piyush Shivam, Puneet Chaudhary, Wesley Jones, Robert Guthrie, Chris Bostic, Rezaul Islam, Steve Duersch, Wayne Sawdon, John Lewars, Matthew Klos, Michael Spriggs, Bill McMillan, George Gao, Ashish Kamra, Gaurav Singh, Marc Curry, Tushar Katarki, Joe Talerico, Zenghui Shi, Sai Sindhur Malleni, Erwan Gallen",
        "translated": "人工智能基础设施在开发和部署先进人工智能模型的速度和成本竞争力方面发挥着关键作用。目前对强大的人工智能模型培训基础设施的需求是由生成性人工智能和基础模型的出现驱动的，有时数千个图形处理器必须在一个单一的培训工作上进行合作，以便在合理的时间内对模型进行培训。提供高效和高性能的人工智能培训需要一个端到端的解决方案，结合硬件、软件和整体遥测，以满足多种类型的人工智能工作负荷。在本报告中，我们描述了 IBM 的混合云基础设施，它为我们的生成式人工智能模型开发提供了动力。这些基础设施包括: (1) Vela: 一种人工智能优化的超级计算能力，直接集成到 IBM 云中，为大规模模型培训和其他人工智能工作流程步骤提供可扩展的、动态的、多租户和地理分布的基础设施; (2) Blue Vela: 一个大规模的、专门建造的、内部托管环境，经过优化以支持我们最大和最雄心勃勃的人工智能模型培训任务。Vela 为 IBM 提供了内部使用的高性能以及适应不断变化的商业环境的灵活性的双重优势。蓝色船帆为我们提供了我们的最大和最雄心勃勃的模型快速发展的好处，以及未来证明对不断发展的模型景观的行业。总之，它们为 IBM 提供了在人工智能模型和商业产品开发方面快速创新的能力。"
    },
    {
        "title": "Shared Randomness Helps with Local Distributed Problems",
        "url": "http://arxiv.org/abs/2407.05445v1",
        "pub_date": "2024-07-07",
        "summary": "By prior work, we have many results related to distributed graph algorithms\nfor problems that can be defined with local constraints; the formal framework\nused in prior work is locally checkable labeling problems (LCLs), introduced by\nNaor and Stockmeyer in the 1990s. It is known, for example, that if we have a\ndeterministic algorithm that solves an LCL in $o(\\log n)$ rounds, we can speed\nit up to $O(\\log^*n)$ rounds, and if we have a randomized $O(\\log^*n)$ rounds\nalgorithm, we can derandomize it for free.\n  It is also known that randomness helps with some LCL problems: there are LCL\nproblems with randomized complexity $\\Theta(\\log\\log n)$ and deterministic\ncomplexity $\\Theta(\\log n)$. However, so far there have not been any LCL\nproblems in which the use of shared randomness has been necessary; in all prior\nalgorithms it has been enough that the nodes have access to their own private\nsources of randomness.\n  Could it be the case that shared randomness never helps with LCLs? Could we\nhave a general technique that takes any distributed graph algorithm for any LCL\nthat uses shared randomness, and turns it into an equally fast algorithm where\nprivate randomness is enough?\n  In this work we show that the answer is no. We present an LCL problem $\\Pi$\nsuch that the round complexity of $\\Pi$ is $\\Omega(\\sqrt n)$ in the usual\nrandomized \\local model with private randomness, but if the nodes have access\nto a source of shared randomness, then the complexity drops to $O(\\log n)$.\n  As corollaries, we also resolve several other open questions related to the\nlandscape of distributed computing in the context of LCL problems. In\nparticular, problem $\\Pi$ demonstrates that distributed quantum algorithms for\nLCL problems strictly benefit from a shared quantum state. Problem $\\Pi$ also\ngives a separation between finitely dependent distributions and non-signaling\ndistributions.",
        "authors": "Alkida Balliu, Mohsen Ghaffari, Fabian Kuhn, Augusto Modanese, Dennis Olivetti, Mikaël Rabie, Jukka Suomela, Jara Uitto",
        "translated": "通过以前的工作，我们已经得到了许多与分布式图算法有关的结果，这些问题可以用局部约束来定义; 在以前的工作中使用的形式框架是局部可检查标记问题(LCLs) ，由 Naor 和 Stockmeyer 在20世纪90年代引入。例如，我们知道，如果我们有一个在 $o (log n) $round 中解决一个 LCL 的确定性算法，我们可以将它加速到 $O (log ^ * n) $round，如果我们有一个随机的 $O (log ^ * n) $round 算法，我们可以免费对它进行去随机化。众所周知，随机性有助于解决一些 LCL 问题: 存在具有随机复杂度的 LCL 问题 $Θ (log log n) $和确定性复杂度 $Θ (log n) $。然而，到目前为止还没有任何 LCL 问题需要使用共享的随机性; 在所有先前的算法中，节点已经足够访问它们自己的私有随机性来源。有没有可能共享的随机性从来不会对 LCLs 有帮助呢？我们能否有一个通用的技术，采取任何分布式图算法的任何 LCL 使用共享的随机性，并把它变成一个同样快速的算法，其中私有随机性是足够的？在这项工作中，我们表明，答案是否定的。我们提出了一个 LCL 问题 $Pi $，使得 $Pi $的整数复杂度在通常的具有私有随机性的随机局部模型中为 $Omega (sqrt n) $，但是如果节点能够访问共享随机性的来源，那么复杂度就下降到 $O (log n) $。作为推论，我们还解决了其他几个与拼箱问题背景下的分布式计算景观有关的悬而未决的问题。特别是，问题 $Pi $演示了用于 LCL 问题的分布式量子算法严格受益于共享量子状态。问题 $Pi $还给出了有限相关分布和非信令分布之间的分离。"
    },
    {
        "title": "A Fault Tolerance Mechanism for Hybrid Scientific Workflows",
        "url": "http://arxiv.org/abs/2407.05337v1",
        "pub_date": "2024-07-07",
        "summary": "In large distributed systems, failures are a daily event occurring\nfrequently, especially with growing numbers of computation tasks and locations\non which they are deployed. The advantage of representing an application with a\nworkflow is the possibility of exploiting Workflow Management System (WMS)\nfeatures such as portability. A relevant feature that some WMSs supply is\nreliability. Over recent years, the emergence of hybrid workflows has posed new\nand intriguing challenges by increasing the possibility of distributing\ncomputations involving heterogeneous and independent environments.\nConsequently, the number of possible points of failure in the execution\nincreased, creating different important challenges that are interesting to\nstudy. This paper presents the implementation of a fault tolerance mechanism\nfor hybrid workflows based on the recovery and rollback approach. A\nrepresentation of the hybrid workflows with the formal framework is provided,\ntogether with the experiments demonstrating the functionality of implementing\napproach.",
        "authors": "Alberto Mulone, Doriana Medić, Marco Aldinucci",
        "translated": "在大型分布式系统中，故障是经常发生的日常事件，特别是随着计算任务和部署它们的位置的增加。使用工作流表示应用程序的优势在于可以利用工作流管理系统(WMS)特性，比如可移植性。一些 WMS 提供的一个相关特性是可靠性。近年来，混合工作流的出现提出了新的和有趣的挑战，通过增加分布计算的可能性涉及异构和独立的环境。因此，执行过程中可能的失败点的数量增加了，产生了有趣的不同的重要挑战。提出了一种基于恢复和回滚的混合工作流容错机制的实现方法。提出了一种基于形式化框架的混合工作流表示方法，并通过实验验证了该方法的实现功能。"
    },
    {
        "title": "OpenDiLoCo: An Open-Source Framework for Globally Distributed\n  Low-Communication Training",
        "url": "http://arxiv.org/abs/2407.07852v1",
        "pub_date": "2024-07-10",
        "summary": "OpenDiLoCo is an open-source implementation and replication of the\nDistributed Low-Communication (DiLoCo) training method for large language\nmodels. We provide a reproducible implementation of the DiLoCo experiments,\noffering it within a scalable, decentralized training framework using the\nHivemind library. We demonstrate its effectiveness by training a model across\ntwo continents and three countries, while maintaining 90-95% compute\nutilization. Additionally, we conduct ablations studies focusing on the\nalgorithm's compute efficiency, scalability in the number of workers and show\nthat its gradients can be all-reduced using FP16 without any performance\ndegradation. Furthermore, we scale OpenDiLoCo to 3x the size of the original\nwork, demonstrating its effectiveness for billion parameter models.",
        "authors": "Sami Jaghouar, Jack Min Ong, Johannes Hagemann",
        "translated": "OpenDiLoCo 是用于大型语言模型的分布式低通信(DiLoCo)培训方法的开源实现和复制。我们提供了 DiLoCo 实验的可重复实现，使用 Hivemind 库在一个可伸缩的、分散的培训框架内提供它。我们通过在两个大陆和三个国家训练一个模型，同时保持90-95% 的计算利用率来证明它的有效性。此外，我们进行了消融研究，重点是该算法的计算效率，可扩展性的人数，并表明其梯度可以全部降低使用 FP16没有任何性能下降。此外，我们将 OpenDiLoCo 的规模扩展到原始作品的3倍，证明了其对数十亿参数模型的有效性。"
    },
    {
        "title": "Harnessing Integrated CPU-GPU System Memory for HPC: a first look into\n  Grace Hopper",
        "url": "http://arxiv.org/abs/2407.07850v1",
        "pub_date": "2024-07-10",
        "summary": "Memory management across discrete CPU and GPU physical memory is\ntraditionally achieved through explicit GPU allocations and data copy or\nunified virtual memory. The Grace Hopper Superchip, for the first time,\nsupports an integrated CPU-GPU system page table, hardware-level addressing of\nsystem allocated memory, and cache-coherent NVLink-C2C interconnect, bringing\nan alternative solution for enabling a Unified Memory system. In this work, we\nprovide the first in-depth study of the system memory management on the Grace\nHopper Superchip, in both in-memory and memory oversubscription scenarios. We\nprovide a suite of six representative applications, including the Qiskit\nquantum computing simulator, using system memory and managed memory. Using our\nmemory utilization profiler and hardware counters, we quantify and characterize\nthe impact of the integrated CPU-GPU system page table on GPU applications. Our\nstudy focuses on first-touch policy, page table entry initialization, page\nsizes, and page migration. We identify practical optimization strategies for\ndifferent access patterns. Our results show that as a new solution for unified\nmemory, the system-allocated memory can benefit most use cases with minimal\nporting efforts.",
        "authors": "Gabin Schieffer, Jacob Wahlgren, Jie Ren, Jennifer Faj, Ivy Peng",
        "translated": "跨离散 CPU 和 GPU 物理内存的内存管理传统上是通过显式的 GPU 分配和数据复制或统一的虚拟内存来实现的。Grace Hopper 超级芯片首次支持集成 CPU-GPU 系统页面表、系统分配内存的硬件级寻址以及缓存相关的 NVLink-C2C 互连，为支持统一内存系统带来了一种替代解决方案。在这项工作中，我们首次深入研究了 Grace Hopper 超级芯片的系统内存管理，包括内存和内存超订情况。我们提供了一套六个具有代表性的应用程序，包括 Qiskit 量子计算模拟器，使用系统内存和托管内存。使用我们的内存利用率分析器和硬件计数器，我们量化和描述了集成 CPU-GPU 系统页表对 GPU 应用程序的影响。我们的研究集中在第一触摸策略、页表条目初始化、页面大小和页面迁移。我们为不同的访问模式确定了实用的优化策略。我们的研究结果表明，作为一种新的统一内存解决方案，系统分配的内存可以在最小的移植工作量下使大多数用例受益。"
    },
    {
        "title": "Fine-Tuning Large Language Models with User-Level Differential Privacy",
        "url": "http://arxiv.org/abs/2407.07737v1",
        "pub_date": "2024-07-10",
        "summary": "We investigate practical and scalable algorithms for training large language\nmodels (LLMs) with user-level differential privacy (DP) in order to provably\nsafeguard all the examples contributed by each user. We study two variants of\nDP-SGD with: (1) example-level sampling (ELS) and per-example gradient\nclipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We\nderive a novel user-level DP accountant that allows us to compute provably\ntight privacy guarantees for ELS. Using this, we show that while ELS can\noutperform ULS in specific settings, ULS generally yields better results when\neach user has a diverse collection of examples. We validate our findings\nthrough experiments in synthetic mean estimation and LLM fine-tuning tasks\nunder fixed compute budgets. We find that ULS is significantly better in\nsettings where either (1) strong privacy guarantees are required, or (2) the\ncompute budget is large. Notably, our focus on LLM-compatible training\nalgorithms allows us to scale to models with hundreds of millions of parameters\nand datasets with hundreds of thousands of users.",
        "authors": "Zachary Charles, Arun Ganesh, Ryan McKenna, H. Brendan McMahan, Nicole Mitchell, Krishna Pillutla, Keith Rush",
        "translated": "我们研究实用和可扩展的算法，用于训练大型语言模型(LLM)和用户级差分隐私(DP) ，以便可证明地保护每个用户提供的所有示例。我们研究了 DP-SGD 的两种变体: (1)示例级采样(ELS)和每个示例的梯度裁剪，以及(2)用户级采样(ULS)和每个用户的梯度裁剪。我们推导出一个新颖的用户级 DP 会计，它允许我们计算可证明的紧密的 ELS 隐私保证。使用这个，我们表明，虽然 ELS 可以在特定的设置优于 ULS，ULS 通常产生更好的结果时，每个用户有一个不同的例子集合。在固定计算预算下，通过合成均值估计和 LLM 微调任务的实验验证了我们的研究结果。我们发现 ULS 在需要(1)强隐私保护或(2)计算预算较大的情况下明显更好。值得注意的是，我们对 LLM 兼容的训练算法的关注使我们能够扩展到具有数亿个参数和数据集的模型和数十万个用户。"
    },
    {
        "title": "High-Performance Sorting-Based k-mer Counting in Distributed Memory with\n  Flexible Hybrid Parallelism",
        "url": "http://arxiv.org/abs/2407.07718v1",
        "pub_date": "2024-07-10",
        "summary": "In generating large quantities of DNA data, high-throughput sequencing\ntechnologies require advanced bioinformatics infrastructures for efficient data\nanalysis. k-mer counting, the process of quantifying the frequency of\nfixed-length k DNA subsequences, is a fundamental step in various\nbioinformatics pipelines, including genome assembly and protein prediction. Due\nto the growing volume of data, the scaling of the counting process is critical.\nIn the literature, distributed memory software uses hash tables, which exhibit\npoor cache friendliness and consume excessive memory. They often also lack\nsupport for flexible parallelism, which makes integration into existing\nbioinformatics pipelines difficult. In this work, we propose HySortK, a highly\nefficient sorting-based distributed memory k-mer counter. HySortK reduces the\ncommunication volume through a carefully designed communication scheme and\ndomain-specific optimization strategies. Furthermore, we introduce an abstract\ntask layer for flexible hybrid parallelism to address load imbalances in\ndifferent scenarios. HySortK achieves a 2-10x speedup compared to the GPU\nbaseline on 4 and 8 nodes. Compared to state-of-the-art CPU software, HySortK\nachieves up to 2x speedup while reducing peak memory usage by 30% on 16 nodes.\nFinally, we integrated HySortK into an existing genome assembly pipeline and\nachieved up to 1.8x speedup, proving its flexibility and practicality in\nreal-world scenarios.",
        "authors": "Yifan Li, Giulia Guidi",
        "translated": "在生成大量 DNA 数据时，高通量测序技术需要先进的生物信息学基础设施来进行有效的数据分析。K-mer 计数是对固定长度 k DNA 子序列的频率进行量化的过程，是包括基因组组装和蛋白质预测在内的各种生物信息学管道中的基本步骤。由于数据量的增长，计数过程的缩放是至关重要的。在文献中，分布式存储软件使用哈希表，这表现出很差的缓存友好性和消耗过多的内存。它们还常常缺乏对灵活并行性的支持，这使得很难集成到现有的生物信息学管道中。在这项工作中，我们提出了 HySortK，一个高效的基于排序的分布式内存 k-mer 计数器。HySortK 通过精心设计的通信方案和特定于领域的优化策略来减少通信量。此外，我们还引入了一个抽象的任务层来实现灵活的混合并行性，以解决不同场景下的负载不平衡问题。HySortK 在4和8个节点上实现了2-10倍于 GPU 基线的加速。与最先进的 CPU 软件相比，HySortK 在16个节点上实现了高达2倍的加速，同时将峰值内存使用减少了30% 。最后，我们将 HySortK 集成到一个现有的基因组装配流水线中，实现了1.8倍的加速，证明了它在现实场景中的灵活性和实用性。"
    },
    {
        "title": "A Transverse-Read-assisted Valid-Bit Collection to Accelerate Stochastic\n  Conmputing MAC for Energy-Efficient in-RTM DNNs",
        "url": "http://arxiv.org/abs/2407.07476v1",
        "pub_date": "2024-07-10",
        "summary": "It looks very attractive to coordinate racetrack-memory(RM) and\nstochastic-computing (SC) jointly to build an ultra-low power\nneuron-architecture.However,the above combination has always been questioned in\na fatal weakness that the narrow bit-view of the RM-MTJ\nstructure,a.k.a.shift-and-access pattern,cannot physically match the great\nthroughput of direct-stored stochastic sequences.Fortunately,a recently\ndeveloped Transverse-Read(TR) provides a wider segment-view to RM via detecting\nthe resistance of domain-walls between a couple of MTJs on single\nnanowire,therefore RM can be enhanced with a faster access to the sequences\nwithout any substantial domain-shift.To utilize TR for a power-efficient\nSC-DNNs, in this work, we propose a segment-based compression to leverage\none-cycle TR to only read those kernel segments of stochastic\nsequences,meanwhile,remove a large number of redundant segments for ultra-high\nstorage density.In decompression stage,the low-discrepancy stochastic sequences\ncan be quickly reassembled by a select-and-output loop using kernel segments\nrather than slowly regenerated by costly SNGs.Since TR can provide an ideal\nin-memory acceleration in one-counting, counter-free SC-MACs are designed and\ndeployed near RMs to form a power-efficient neuron-architecture,in which,the\nbinary results of TR are activated straightforward without sluggish APCs.The\nresults show that under the TR aided RM model,the power efficiency,speed,and\nstochastic accuracy of Seed-based Fast Stochastic Computing significantly\nenhance the performance of DNNs.The speed of computation is 2.88x faster in\nLenet-5 and 4.40x faster in VGG-19 compared to the CORUSCANT model.The\nintegration of TR with RTM is deployed near the memory to create a\npower-efficient neuron architecture, eliminating the need for slow Accumulative\nParallel Counters (APCs) and improving access speed to stochastic sequences.",
        "authors": "Jihe Wang, Zhiying Zhang, Xingwu Dong, Danghui Wang",
        "translated": "将赛道存储器(RM)和随机计算(SC)结合起来构建超低功耗的神经元体系结构非常有吸引力。然而，上述组合一直受到质疑，致命的弱点是 RM-MTJ 结构的窄比特视图，即移位和访问模式，不能在物理上匹配直接存储的随机序列的巨大吞吐量。幸运的是，最近开发的 Transverse-Read (TR)通过检测单根纳米线上两个 MTJ 之间的域壁电阻为 RM 提供了更广泛的片段视图，因此 RM 可以通过更快地访问序列而不需要任何实质性的域移位来增强。为了利用 TR 实现高功耗的 SC-DNN，本文提出了一种基于分段的压缩方法，该方法利用单周期 TR 只读取随机序列的核心片段，同时为超高的存储密度去除大量冗余片段。在解压缩阶段，低差异随机序列可以通过使用核心片段的选择-输出循环快速重新组装，而不是由昂贵的 SNG 缓慢重新生成。由于 TR 可以在一次计数中提供理想的内存加速，因此在 RM 附近设计和部署无计数器的 SC-MAC 以形成功率效率高的神经元结构，其中 TR 的二进制结果直接激活而没有缓慢的 APC。结果表明，在 TR 辅助 RM 模型下，基于种子的快速随机计算的功率效率、速度和随机精度显著提高了 DNN 的性能。与 CORUSCANT 模型相比，Lenet-5的计算速度快2.88倍，VGG-19的计算速度快4.40倍。TR 与 RTM 的集成部署在内存附近，以创建一个高效的神经元架构，消除了对缓慢的累积并行计数器(APC)的需要，并提高了对随机序列的访问速度。"
    },
    {
        "title": "Federated PCA on Grassmann Manifold for IoT Anomaly Detection",
        "url": "http://arxiv.org/abs/2407.07421v1",
        "pub_date": "2024-07-10",
        "summary": "With the proliferation of the Internet of Things (IoT) and the rising\ninterconnectedness of devices, network security faces significant challenges,\nespecially from anomalous activities. While traditional machine learning-based\nintrusion detection systems (ML-IDS) effectively employ supervised learning\nmethods, they possess limitations such as the requirement for labeled data and\nchallenges with high dimensionality. Recent unsupervised ML-IDS approaches such\nas AutoEncoders and Generative Adversarial Networks (GAN) offer alternative\nsolutions but pose challenges in deployment onto resource-constrained IoT\ndevices and in interpretability. To address these concerns, this paper proposes\na novel federated unsupervised anomaly detection framework, FedPCA, that\nleverages Principal Component Analysis (PCA) and the Alternating Directions\nMethod Multipliers (ADMM) to learn common representations of distributed\nnon-i.i.d. datasets. Building on the FedPCA framework, we propose two\nalgorithms, FEDPE in Euclidean space and FEDPG on Grassmann manifolds. Our\napproach enables real-time threat detection and mitigation at the device level,\nenhancing network resilience while ensuring privacy. Moreover, the proposed\nalgorithms are accompanied by theoretical convergence rates even under a\nsubsampling scheme, a novel result. Experimental results on the UNSW-NB15 and\nTON-IoT datasets show that our proposed methods offer performance in anomaly\ndetection comparable to nonlinear baselines, while providing significant\nimprovements in communication and memory efficiency, underscoring their\npotential for securing IoT networks.",
        "authors": "Tung-Anh Nguyen, Long Tan Le, Tuan Dung Nguyen, Wei Bao, Suranga Seneviratne, Choong Seon Hong, Nguyen H. Tran",
        "translated": "随着物联网(IoT)的普及和设备互联性的提高，网络安全面临着严峻的挑战，尤其是来自异常活动的挑战。虽然传统的基于机器学习的入侵检测系统(ML-IDS)有效地采用了监督式学习检测方法，但它们也存在一些局限性，例如对标记数据的需求以及高维数的挑战。最近的无监督机器学习入侵检测方法，如自动编码器和生成对抗网络(GAN)提供了替代解决方案，但在部署到资源受限的物联网设备和可解释性方面提出了挑战。为了解决这些问题，本文提出了一种新型的联邦无监督异常检测框架——联邦主成分分析法(fedPCA) ，它利用主成分分析(PCA)和交替方向法乘法器(ADMM)来学习分布式非 i.d. 数据集的通用表示。在 FedPCA 框架的基础上，提出了两种算法: 欧氏空间 FEDPE 算法和 Grassmann 流形 FEDPG 算法。我们的方法能够在设备级实时检测和缓解威胁，增强网络弹性，同时确保隐私。此外，即使在次采样情况下，该算法也具有理论收敛速度，这是一个新的结果。在 UNSW-nb15和 TON-IoT 数据集上的实验结果表明，我们提出的方法在异常检测上提供了与非线性基线相当的性能，同时在通信和存储效率方面提供了显著的改进，强调了它们在保护物联网方面的潜力。"
    },
    {
        "title": "BoostCom: Towards Efficient Universal Fully Homomorphic Encryption by\n  Boosting the Word-wise Comparisons",
        "url": "http://arxiv.org/abs/2407.07308v1",
        "pub_date": "2024-07-10",
        "summary": "Fully Homomorphic Encryption (FHE) allows for the execution of computations\non encrypted data without the need to decrypt it first, offering significant\npotential for privacy-preserving computational operations. Emerging\narithmetic-based FHE schemes (ar-FHE), like BGV, demonstrate even better\nperformance in word-wise comparison operations over non-arithmetic FHE (na-FHE)\nschemes, such as TFHE, especially for basic tasks like comparing values,\nfinding maximums, and minimums. This shows the universality of ar-FHE in\neffectively handling both arithmetic and non-arithmetic operations without the\nexpensive conversion between arithmetic and non-arithmetic FHEs. We refer to\nuniversal arithmetic Fully Homomorphic Encryption as uFHE. The arithmetic\noperations in uFHE remain consistent with those in the original arithmetic FHE,\nwhich have seen significant acceleration. However, its non-arithmetic\ncomparison operations differ, are slow, and have not been as thoroughly studied\nor accelerated. In this paper, we introduce BoostCom, a scheme designed to\nspeed up word-wise comparison operations, enhancing the efficiency of uFHE\nsystems. BoostCom involves a multi-prong optimizations including infrastructure\nacceleration (Multi-level heterogeneous parallelization and GPU-related\nimprovements), and algorithm-aware optimizations (slot compaction, non-blocking\ncomparison semantic). Together, BoostCom achieves an end-to-end performance\nimprovement of more than an order of magnitude (11.1x faster) compared to the\nstate-of-the-art CPU-based uFHE systems, across various FHE parameters and\ntasks.",
        "authors": "Ardhi Wiratama Baskara Yudha, Jiaqi Xue, Qian Lou, Huiyang Zhou, Yan Solihin",
        "translated": "完全同态加密(fHE)允许对加密数据执行计算，而不需要首先解密，为保护隐私的计算操作提供了巨大的潜力。新兴的基于算术的 FHE (ar-FHE)方案，如 BGV，比非算术的 FHE (na-FHE)方案(如 TFHE)在字比较操作方面表现出更好的性能，特别是对于比较值、寻找最大值和最小值等基本任务。这表明了 ar-FHE 在有效地处理算术和非算术操作方面的普遍性，而不需要在算术和非算术 FHE 之间进行昂贵的转换。我们把通用算术完全称为 ufHE 同态加密。UFHE 中的算术运算与原始算术 FHE 中的算术运算保持一致，并有显著的加速度。然而，它的非算术比较操作不同，速度较慢，并且没有得到充分的研究或加速。在本文中，我们介绍了 BoostCom，这是一个设计来加速逐字比较操作，提高 uFHE 系统效率的方案。BoostCom 涉及多方面的优化，包括基础设施加速(多级异构并行化和与 GPU 相关的改进)和算法感知优化(插槽压缩、非阻塞比较语义)。与最先进的基于 CPU 的 uFHE 系统相比，BoostCom 在各种 FHE 参数和任务方面实现了端到端性能的提高，比数量级(11.1倍的速度)快得多。"
    },
    {
        "title": "Metron: Holistic Performance Evaluation Framework for LLM Inference\n  Systems",
        "url": "http://arxiv.org/abs/2407.07000v1",
        "pub_date": "2024-07-09",
        "summary": "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Metron, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Metron, discussing their strengths and weaknesses. Metron is\navailable at https://github.com/project-metron/metron.",
        "authors": "Amey Agrawal, Anmol Agarwal, Nitin Kedia, Jayashree Mohan, Souvik Kundu, Nipun Kwatra, Ramachandran Ramjee, Alexey Tumanov",
        "translated": "在生产环境中使用大型语言模型(LLM)可能会产生巨大的成本，这促进了推理系统优化方面的最新进展。目前，这些系统是根据传统的延迟和吞吐量指标进行评估的(例如。TTFT，TBT，归一化延迟和 TPOT)。然而，这些指标无法完全捕捉 LLM 推理的细微差别，导致对实时应用程序(如聊天和翻译)至关重要的用户面向性能的不完整评估。在本文中，我们首先确定当前性能指标在评估 LLM 推理系统中的缺陷。然后，我们提出了 Metron，一个包括流动性指数的综合性能评估框架——一个新的度量标准，旨在反映 LLM 推理过程的复杂性及其对实时用户体验的影响。最后，我们评估了使用 Metron 的各种现有开源平台和模型即服务(model-as-a-service)产品，讨论了它们的优缺点。麦特隆 https://github.com/project-Metron/Metron 有空。"
    },
    {
        "title": "SP-Chain: Boosting Intra-Shard and Cross-Shard Security and Performance\n  in Blockchain Sharding",
        "url": "http://arxiv.org/abs/2407.06953v1",
        "pub_date": "2024-07-09",
        "summary": "A promising way to overcome the scalability limitations of the current\nblockchain is to use sharding, which is to split the transaction processing\namong multiple, smaller groups of nodes. A well-performed blockchain sharding\nsystem requires both high performance and high security in both intra- and\ncross-shard perspectives. However, existing protocols either have issues on\nprotecting security or trade off great performance for security. In this paper,\nwe propose SP-Chain, a blockchain sharding system with enhanced Security and\nPerformance for both intra- and cross-shard perspectives. For intra-shard\naspect, we design a two-phase concurrent voting scheme to provide high system\nthroughput and low transaction confirmation latency. Moreover, we propose an\nefficient unbiased leader rotation scheme to ensure high performance under\nmalicious behavior. For cross-shard aspect, a proof-assisted efficient\ncross-shard transaction processing mechanism is proposed to guard the\ncross-shard transactions with low overhead. We implement SP-Chain based on\nHarmony, and evaluate its performance via large-scale deployment. Extensive\nevaluations suggest that SP-Chain can process more than 10,000 tx/sec under\nmalicious behaviors with a confirmation latency of 7.6s in a network of 4,000\nnodes.",
        "authors": "Mingzhe Li, You Lin, Wei Wang, Jin Zhang",
        "translated": "克服当前区块链可伸缩性限制的一个有前途的方法是使用分片，即在多个较小的节点组之间分割事务处理。一个良好表现的区块链分片系统需要高性能和高安全性都在内部和跨碎片的观点。然而，现有的协议要么存在保护安全性的问题，要么牺牲了很好的安全性能。在本文中，我们提出了 SP 链，一个区块链分片系统的增强安全性和性能的内部和跨碎片的观点。对于内部分片方面，我们设计了一个两阶段并发投票方案，以提供高系统吞吐量和低事务确认延迟。此外，我们提出了一个有效的无偏领导者轮换方案，以确保在恶意行为下的高性能。在跨碎片方面，提出了一种证明辅助的高效跨碎片事务处理机制，以较低的开销保护跨碎片事务。我们实现了基于 Harmony 的 SP 链，并通过大规模部署对其性能进行了评估。广泛的评估表明，SP-Chain 可以在4000个节点的网络中处理超过10,000 tx/sec 的恶意行为，确认延迟为7.6 s。"
    },
    {
        "title": "DL-Chain: Scalable and Stable Blockchain Sharding with High Concurrency\n  via Dual-Layer Consensus",
        "url": "http://arxiv.org/abs/2407.06882v1",
        "pub_date": "2024-07-09",
        "summary": "Sharding enhances blockchain scalability by partitioning nodes into multiple\ngroups for concurrent transaction processing. Configuring a large number of\n\\emph{small shards} helps improve the transaction concurrency of a sharding\nsystem. However, it increases the fraction of malicious nodes within each\nshard, easily leading to shard corruption and jeopardizing system security.\nSome existing works have attempted to improve concurrency by reducing the shard\nsize while maintaining security. However, they often require frequent and\ntime-consuming recovery of corrupted shards, leading to severe system\nstagnation. Also, they usually require network-wide consensus to guarantee\nsecurity, which limits scalability.\n  To address these issues, we propose DL-Chain, a blockchain sharding system\nthat can securely provide \\emph{high concurrency with stable and scalable\nperformance.} Our core idea is a \\underline{D}ual-\\underline{L}ayer\narchitecture and consensus, which consists of numerous smaller proposer shards\n(PSs) for transaction processing and multiple larger finalizer committees (FCs)\nfor transaction finalization. To avoid system stagnation and thus guarantee\nstable performance, we ensure PSs' liveness even if they are corrupted through\nthe cooperation of PSs and FCs, thus eliminating the recovery process of\ncorrupted PSs. To better trade-off security and scalability, we fine-tune the\nFCs to enable multiple FCs to coexist securely. As a result, DL-Chain allows a\nlarger fraction of malicious nodes in each PS ($&lt;1/2$) and thus can securely\nconfigure smaller shards for boosted stable and scalable concurrency.\nEvaluation results show that DL-Chain achieves up to 10 times improvement in\nthroughput compared to existing solutions and provides stable concurrency with\nup to 2,550 nodes.",
        "authors": "You Lin, Mingzhe Li, Qingsong Wei, Yong Liu, Siow Mong Rick Goh, Jin Zhang",
        "translated": "分片通过将节点划分为多个组进行并发事务处理来增强区块链的可伸缩性。配置大量 emph { small shards }有助于提高分片系统的事务并发性。但是，它增加了每个碎片中恶意节点的比例，很容易导致碎片损坏并危及系统安全。一些现有的工作已经尝试通过在维护安全性的同时减少碎片大小来改进并发性。然而，它们通常需要频繁且耗时地恢复损坏的碎片，从而导致严重的系统停滞。此外，它们通常需要网络范围内的一致同意来保证安全性，这限制了可伸缩性。为了解决这些问题，我们提出了 DL 链，一个区块链分片系统，可以安全地提供高并发性和稳定的可伸缩性能。}我们的核心思想是一个下划线{ D }双下划线{ L }层架构和共识，其由用于事务处理的许多较小的提议者碎片(PSS)和用于事务终结的多个较大的终结者委员会(FC)组成。为了避免系统停滞，从而保证稳定的性能，我们即使通过 PSs 和 FC 的协作，也可以保证 PSs 的活性，从而消除了被损坏的 PSs 的恢复过程。为了更好地兼顾安全性和可扩展性，我们对功能界面进行了微调，使多个功能界面能够安全共存。因此，DL-Chain 允许每个 PS 中有更大比例的恶意节点($< 1/2 $) ，因此可以安全地配置更小的碎片，以提高稳定性和可伸缩性并发性。评估结果表明，与现有解决方案相比，DL-Chain 在吞吐量方面实现了高达10倍的提高，并提供了多达2,550个节点的稳定并发性。"
    },
    {
        "title": "Flex-TPU: A Flexible TPU with Runtime Reconfigurable Dataflow\n  Architecture",
        "url": "http://arxiv.org/abs/2407.08700v1",
        "pub_date": "2024-07-11",
        "summary": "Tensor processing units (TPUs) are one of the most well-known machine\nlearning (ML) accelerators utilized at large scale in data centers as well as\nin tiny ML applications. TPUs offer several improvements and advantages over\nconventional ML accelerators, like graphical processing units (GPUs), being\ndesigned specifically to perform the multiply-accumulate (MAC) operations\nrequired in the matrix-matrix and matrix-vector multiplies extensively present\nthroughout the execution of deep neural networks (DNNs). Such improvements\ninclude maximizing data reuse and minimizing data transfer by leveraging the\ntemporal dataflow paradigms provided by the systolic array architecture. While\nthis design provides a significant performance benefit, the current\nimplementations are restricted to a single dataflow consisting of either input,\noutput, or weight stationary architectures. This can limit the achievable\nperformance of DNN inference and reduce the utilization of compute units.\nTherefore, the work herein consists of developing a reconfigurable dataflow\nTPU, called the Flex-TPU, which can dynamically change the dataflow per layer\nduring run-time. Our experiments thoroughly test the viability of the Flex-TPU\ncomparing it to conventional TPU designs across multiple well-known ML\nworkloads. The results show that our Flex-TPU design achieves a significant\nperformance increase of up to 2.75x compared to conventional TPU, with only\nminor area and power overheads.",
        "authors": "Mohammed Elbtity, Peyton Chandarana, Ramtin Zand",
        "translated": "张量处理单元(TPU)是最著名的机器学习加速器之一，在数据中心和微型机器学习应用中得到了广泛的应用。与传统的机器学习加速器相比，TPU 提供了一些改进和优势，比如图形处理单元(GPU) ，它被专门设计用于执行矩阵矩阵和矩阵向量乘所需的乘积(MAC)操作，这些操作在深度神经网络(DNN)的执行过程中广泛存在。这些改进包括通过利用收缩阵列体系结构提供的时间数据流范例，最大限度地提高数据重用和最小化数据传输。虽然这种设计提供了显著的性能优势，但当前的实现仅限于一个由输入、输出或权重固定架构组成的单一数据流。这会限制 DNN 推理的可实现性能，降低计算单元的利用率。因此，这里的工作包括开发一个可重构的数据流 TPU，称为 Flex-TPU，它可以在运行时动态地改变每一层的数据流。我们的实验彻底测试了 Flex-TPU 的可行性，并将其与传统的 TPU 设计在多个著名的 ML 工作负载中进行了比较。结果表明，我们的 Flex-TPU 设计与传统 TPU 相比，性能提高了2.75倍，只有很小的面积和功耗开销。"
    },
    {
        "title": "Cloud Atlas: Efficient Fault Localization for Cloud Systems using\n  Language Models and Causal Insight",
        "url": "http://arxiv.org/abs/2407.08694v1",
        "pub_date": "2024-07-11",
        "summary": "Runtime failure and performance degradation is commonplace in modern cloud\nsystems. For cloud providers, automatically determining the root cause of\nincidents is paramount to ensuring high reliability and availability as prompt\nfault localization can enable faster diagnosis and triage for timely\nresolution. A compelling solution explored in recent work is causal reasoning\nusing causal graphs to capture relationships between varied cloud system\nperformance metrics. To be effective, however, systems developers must\ncorrectly define the causal graph of their system, which is a time-consuming,\nbrittle, and challenging task that increases in difficulty for large and\ndynamic systems and requires domain expertise. Alternatively, automated\ndata-driven approaches have limited efficacy for cloud systems due to the\ninherent rarity of incidents. In this work, we present Atlas, a novel approach\nto automatically synthesizing causal graphs for cloud systems. Atlas leverages\nlarge language models (LLMs) to generate causal graphs using system\ndocumentation, telemetry, and deployment feedback. Atlas is complementary to\ndata-driven causal discovery techniques, and we further enhance Atlas with a\ndata-driven validation step. We evaluate Atlas across a range of fault\nlocalization scenarios and demonstrate that Atlas is capable of generating\ncausal graphs in a scalable and generalizable manner, with performance that far\nsurpasses that of data-driven algorithms and is commensurate to the\nground-truth baseline.",
        "authors": "Zhiqiang Xie, Yujia Zheng, Lizi Ottens, Kun Zhang, Christos Kozyrakis, Jonathan Mace",
        "translated": "运行时故障和性能下降在现代云系统中很常见。对于云供应商来说，自动确定事故的根本原因对于确保高可靠性和可用性至关重要，因为及时的故障定位可以实现更快的诊断和分类，以及时解决问题。在最近的工作中，一个引人注目的解决因果推理是使用因果图来捕捉不同云系统性能指标之间的关系。然而，为了有效，系统开发人员必须正确地定义他们系统的因果图，这是一项耗时、脆弱和具有挑战性的任务，对于大型动态系统而言，这项任务的难度会增加，并且需要领域专业知识。另外，由于事件固有的罕见性，自动化数据驱动方法对云系统的功效有限。在这项工作中，我们提出了阿特拉斯，一种新的方法来自动合成因果图云系统。Atlas 利用大型语言模型(LLM)使用系统文档、遥测和部署反馈生成因果图。Atlas 是数据驱动的因果发现技术的补充，我们进一步加强 Atlas 与数据驱动的验证步骤。我们在一系列故障定位场景中评估 Atlas，并证明 Atlas 能够以可伸缩和可推广的方式生成因果图，其性能远远超过数据驱动算法，并与地面真相基线相称。"
    },
    {
        "title": "DART: A Solution for Decentralized Federated Learning Model Robustness\n  Analysis",
        "url": "http://arxiv.org/abs/2407.08652v1",
        "pub_date": "2024-07-11",
        "summary": "Federated Learning (FL) has emerged as a promising approach to address\nprivacy concerns inherent in Machine Learning (ML) practices. However,\nconventional FL methods, particularly those following the Centralized FL (CFL)\nparadigm, utilize a central server for global aggregation, which exhibits\nlimitations such as bottleneck and single point of failure. To address these\nissues, the Decentralized FL (DFL) paradigm has been proposed, which removes\nthe client-server boundary and enables all participants to engage in model\ntraining and aggregation tasks. Nevertheless, as CFL, DFL remains vulnerable to\nadversarial attacks, notably poisoning attacks that undermine model\nperformance. While existing research on model robustness has predominantly\nfocused on CFL, there is a noteworthy gap in understanding the model robustness\nof the DFL paradigm. In this paper, a thorough review of poisoning attacks\ntargeting the model robustness in DFL systems, as well as their corresponding\ncountermeasures, are presented. Additionally, a solution called DART is\nproposed to evaluate the robustness of DFL models, which is implemented and\nintegrated into a DFL platform. Through extensive experiments, this paper\ncompares the behavior of CFL and DFL under diverse poisoning attacks,\npinpointing key factors affecting attack spread and effectiveness within the\nDFL. It also evaluates the performance of different defense mechanisms and\ninvestigates whether defense mechanisms designed for CFL are compatible with\nDFL. The empirical results provide insights into research challenges and\nsuggest ways to improve the robustness of DFL models for future research.",
        "authors": "Chao Feng, Alberto Huertas Celdrán, Jan von der Assen, Enrique Tomás Martínez Beltrán, Gérôme Bovet, Burkhard Stiller",
        "translated": "联邦学习(FL)已经成为解决机器学习(ML)实践中固有的隐私问题的一种有前途的方法。然而，传统的 FL 方法，特别是那些遵循集中式 FL (CFL)范式的方法，利用中央服务器进行全局聚合，这种方法存在瓶颈和单点故障等局限性。为了解决这些问题，提出了分散 FL (Decentralization FL)范式，该范式消除了客户机-服务器边界，使所有参与者都能参与模型培训和聚合任务。尽管如此，作为 CFL，DFL 仍然容易受到敌对攻击，特别是中毒攻击，破坏模型的性能。虽然现有的模型鲁棒性研究主要集中在 CFL，但是在理解 DFL 范式的模型鲁棒性方面还存在一个值得注意的差距。本文对 DFL 系统中针对模型鲁棒性的中毒攻击及其相应的对策进行了全面的综述。此外，本文还提出了一种 DART 解决方案来评估 DFL 模型的鲁棒性，并将其实现和集成到 DFL 平台中。通过大量的实验，比较了 CFL 和 DFL 在不同中毒攻击下的行为，找出了影响 DFL 攻击传播和有效性的关键因素。并对不同防御机制的性能进行了评估，研究了 CFL 设计的防御机制是否与 DFL 兼容。实证结果为研究挑战提供了见解，并为未来的研究提出了改善 DFL 模型稳健性的方法。"
    },
    {
        "title": "Data-Locality-Aware Task Assignment and Scheduling for Distributed Job\n  Executions",
        "url": "http://arxiv.org/abs/2407.08584v1",
        "pub_date": "2024-07-11",
        "summary": "This paper investigates a data-locality-aware task assignment and scheduling\nproblem aimed at minimizing job completion times for distributed job\nexecutions. Without prior knowledge of future job arrivals, we propose an\noptimal balanced task assignment algorithm (OBTA) that minimizes the completion\ntime of each arriving job. We significantly reduce OBTA's computational\noverhead by narrowing the search space of potential solutions. Additionally, we\nextend an approximate algorithm known as water-filling (WF) and nontrivially\nprove that its approximation factor equals the number of task groups in the job\nassignment. We also design a novel heuristic, replica-deletion (RD), which\noutperforms WF. To further reduce the completion time of each job, we expand\nthe problem to include job reordering, where we adjust the order of outstanding\njobs following the shortest-estimated-time-first policy. Extensive trace-driven\nevaluations validate the performance and efficiency of the proposed algorithms.",
        "authors": "Hailiang Zhao, Xueyan Tang, Peng Chen, Jianwei Yin, Shuiguang Deng",
        "translated": "针对分布式作业执行的任务完成时间最小化问题，研究了一种基于数据位置感知的任务分配与调度问题。在不知道未来任务到达时间的情况下，提出了一种最小化每个到达任务完成时间的最优平衡任务分配算法(OBTA)。通过缩小可能解的搜索空间，显著降低了 OBTA 的计算开销。此外，我们推广了一种近似算法，称为水填充(WF) ，并不平凡地证明了其近似因子等于任务分配中的任务组数。我们还设计了一种新的启发式复制删除算法(RD) ，其性能优于 WF。为了进一步缩短每个作业的完成时间，我们将问题扩展到包括作业重排序，其中我们按照最短估计时间优先策略调整未完成作业的顺序。大量的跟踪驱动评估验证了所提算法的性能和效率。"
    },
    {
        "title": "Distributed Edge Analytics in Edge-Fog-Cloud Continuum",
        "url": "http://arxiv.org/abs/2407.08543v1",
        "pub_date": "2024-07-11",
        "summary": "To address the increased latency, network load and compromised privacy issues\nassociated with the Cloud-centric IoT applications, fog computing has emerged.\nFog computing utilizes the proximal computational and storage devices, for\nsensor data analytics. The edge-fog-cloud continuum thus provides significant\nedge analytics capabilities for realizing interesting IoT applications. While\nedge analytics tasks are usually performed on a single node, distributed edge\nanalytics proposes utilizing multiple nodes from the continuum, concurrently.\nThis paper discusses and demonstrates distributed edge analytics from three\ndifferent perspectives; serverless data pipelines (SDP), distributed computing\nand edge analytics, and federated learning, with our frameworks, MQTT based\nSDP, CANTO and FIDEL, respectively. The results produced in the paper, through\ndifferent case studies, show the feasibility of performing distributed edge\nanalytics following the three approaches, across the continuum.",
        "authors": "Satish Narayana Srirama",
        "translated": "为了解决与以云为中心的物联网应用程序相关的延迟增加、网络负载和隐私泄露问题，雾计算应运而生。雾计算利用近端计算和存储设备，进行传感器数据分析。因此，边雾-云连续体为实现有趣的物联网应用提供了重要的边缘分析能力。尽管边缘分析任务通常在单个节点上执行，但分布式边缘分析提出利用连续统中的多个节点并发执行。这篇文章从三个不同的角度讨论和演示了分布式边缘分析: 无服务器数据管道(sDP) ，分布式计算和边缘分析，以及联邦学习，我们的框架分别是基于 MQTT 的 sDP，CANTO 和 FIDEL。通过不同的案例研究，本文得到的结果表明，在整个连续统中，按照这三种方法进行分布式边缘分析是可行的。"
    },
    {
        "title": "FedLog: Personalized Federated Classification with Less Communication\n  and More Flexibility",
        "url": "http://arxiv.org/abs/2407.08337v1",
        "pub_date": "2024-07-11",
        "summary": "In federated learning (FL), the common paradigm that FedAvg proposes and most\nalgorithms follow is that clients train local models with their private data,\nand the model parameters are shared for central aggregation, mostly averaging.\nIn this paradigm, the communication cost is often a challenge, as modern\nmassive neural networks can contain millions to billions parameters. We suggest\nthat clients do not share model parameters but local data summaries, to\ndecrease the cost of sharing. We develop a new algorithm FedLog with Bayesian\ninference, which shares only sufficient statistics of local data. FedLog\ntransmits messages as small as the last layer of the original model. We\nconducted comprehensive experiments to show we outperform other FL algorithms\nthat aim at decreasing the communication cost. To provide formal privacy\nguarantees, we further extend FedLog with differential privacy and show the\ntrade-off between privacy budget and accuracy.",
        "authors": "Haolin Yu, Guojun Zhang, Pascal Poupart",
        "translated": "在联邦学习(FL)中，FedAvg 提出的和大多数算法遵循的共同范式是: 客户端用他们的私有数据训练局部模型，模型参数共享用于中央聚合，主要是平均。在这个范例中，通信成本往往是一个挑战，因为现代大规模神经网络可以包含数百万到数十亿个参数。我们建议客户端不共享模型参数，而是共享本地数据汇总，以降低共享成本。我们开发了一个新的带有贝叶斯推断的算法 FedLog，它只共享本地数据的足够统计信息。FedLog 传输的消息小到原始模型的最后一层。我们进行了全面的实验，表明我们的性能优于其他旨在降低通信成本的 FL 算法。为了提供正式的隐私保障，我们进一步扩展了 FedLog 的差分隐私，并展示了隐私预算和准确性之间的权衡。"
    },
    {
        "title": "Performance Evaluation of Hashing Algorithms on Commodity Hardware",
        "url": "http://arxiv.org/abs/2407.08284v1",
        "pub_date": "2024-07-11",
        "summary": "Hashing functions, which are created to provide brief and erratic digests for\nthe message entered, are the primary cryptographic primitives used in\nblockchain networks. Hashing is employed in blockchain networks to create\nlinked block lists, which offer safe and secure distributed repository storage\nfor critical information. Due to the unique nature of the hash search problem\nin blockchain networks, the most parallelization of calculations is possible.\nThis technical report presents a performance evaluation of three popular\nhashing algorithms Blake3, SHA-256, and SHA-512. These hashing algorithms are\nwidely used in various applications, such as digital signatures, message\nauthentication, and password storage. It then discusses the performance metrics\nused to evaluate the algorithms, such as hash rate/throughput and memory usage.\nThe evaluation is conducted on a range of hardware platforms, including desktop\nand VMs. The evaluation includes synthetic benchmarks. The results of the\nevaluation show that Blake3 generally outperforms both SHA-256 and SHA-512 in\nterms of throughput and latency. However, the performance advantage of Blake3\nvaries depending on the specific hardware platform and the size of the input\ndata. The report concludes with recommendations for selecting the most suitable\nhashing algorithm for a given application, based on its performance\nrequirements and security needs. The evaluation results can also inform future\nresearch and development efforts to improve the performance and security of\nhashing algorithms.",
        "authors": "Marut Pandya",
        "translated": "散列函数是区块链网络中使用的主要加密原语，其创建目的是为输入的消息提供简短而不规则的摘要。在区块链网络中使用散列来创建链接的块列表，为关键信息提供安全可靠的分布式存储库存储。由于哈希搜索问题在区块链网络中的独特性质，最并行的计算是可能的。本技术报告介绍了三种流行的散列算法 Blake3、 SHA-256和 SHA-512的性能评估。这些散列算法广泛应用于各种应用，如数字签名、消息认证和密码存储。然后讨论用于评估算法的性能指标，例如散列速率/吞吐量和内存使用情况。评估是在一系列的硬件平台上进行的，包括桌面和虚拟机。评估包括综合基准。评估结果表明，Blake3在吞吐量和延迟方面通常优于 SHA-256和 SHA-512。然而，Blake3的性能优势取决于特定的硬件平台和输入数据的大小。报告最后根据性能要求和安全需求，提出了为给定应用程序选择最合适的哈希算法的建议。评估结果还可以为今后的研究和开发工作提供信息，以改善哈希算法的性能和安全性。"
    },
    {
        "title": "Extending DD-$α$AMG on heterogeneous machines",
        "url": "http://arxiv.org/abs/2407.08092v1",
        "pub_date": "2024-07-10",
        "summary": "Multigrid solvers are the standard in modern scientific computing\nsimulations. Domain Decomposition Aggregation-Based Algebraic Multigrid, also\nknown as the DD-$\\alpha$AMG solver, is a successful realization of an algebraic\nmultigrid solver for lattice quantum chromodynamics. Its CPU implementation has\nmade it possible to construct, for some particular discretizations, simulations\notherwise computationally unfeasible, and furthermore it has motivated the\ndevelopment and improvement of other algebraic multigrid solvers in the area.\nFrom an existing version of DD-$\\alpha$AMG already partially ported via CUDA to\nrun some finest-level operations of the multigrid solver on Nvidia GPUs, we\ntranslate the CUDA code here by using HIP to run on the ORISE supercomputer. We\nmoreover extend the smoothers available in DD-$\\alpha$AMG, paying particular\nattention to Richardson smoothing, which in our numerical experiments has led\nto a multigrid solver faster than smoothing with GCR and only 10% slower\ncompared to SAP smoothing. Then we port the odd-even-preconditioned versions of\nGMRES and Richardson via CUDA. Finally, we extend some computationally\nintensive coarse-grid operations via advanced vectorization.",
        "authors": "Lianhua He, Gustavo Ramirez-Hidalgo, Ke-Long Zhang",
        "translated": "多重网格求解器是现代科学计算模拟的标准。基于领域分解聚合的代数多重网格，也被称为 DD-$alpha $AMG 求解器，是一个成功实现的格子量子色动力学的代数多重网格求解器。它的 CPU 实现使得有可能构建，对于一些特殊的离散化，否则计算不可行的模拟，此外，它推动了发展和改进的其他代数多重网格解决方案在该领域。从现有版本的 DD-$alpha $AMG 已经部分地通过 CUDA 移植到运行一些最好的水平运算的多网格解决方案的 Nvidia 图形处理器，我们翻译 CUDA 代码在这里使用 HIP 运行在 ORISE 超级计算机。此外，我们还扩展了 DD-$alpha $AMG 中可用的平滑器，特别关注 Richardson 平滑，在我们的数值实验中，它导致多重网格求解器比 GCR 平滑更快，比 SAP 平滑只慢10% 。然后通过 CUDA 移植 GMRES 和 Richardson 的奇偶预处理版本。最后，通过高级矢量化方法扩展了一些计算量较大的粗网格运算。"
    },
    {
        "title": "Securing Confidential Data For Distributed Software Development Teams:\n  Encrypted Container File",
        "url": "http://arxiv.org/abs/2407.09142v1",
        "pub_date": "2024-07-12",
        "summary": "In the context of modern software engineering, there is a trend towards\nCloud-native software development involving international teams with members\nfrom all over the world. Cloud-based version management services like GitHub\nare commonly used for source code and other files. However, a challenge arises\nwhen developers from different companies or organizations share the platform,\nas sensitive data should be encrypted to restrict access to certain developers\nonly. This paper discusses existing tools addressing this issue, highlighting\ntheir shortcomings. The authors propose their own solution, Encrypted Container\nFiles, designed to overcome the deficiencies observed in other tools.",
        "authors": "Tobias J. Bauer, Andreas Aßmuth",
        "translated": "在现代软件工程的背景下，有一种云本地软件开发的趋势，涉及来自世界各地的成员组成的国际团队。像 GitHub 这样的基于云的版本管理服务通常用于源代码和其他文件。但是，当来自不同公司或组织的开发人员共享该平台时，就会出现一个挑战，因为应该对敏感数据进行加密，以限制对某些开发人员的访问。本文讨论了解决这一问题的现有工具，突出了它们的不足之处。作者提出了自己的解决方案，加密容器文件，旨在克服在其他工具中观察到的缺陷。"
    },
    {
        "title": "Mapping Large Memory-constrained Workflows onto Heterogeneous Platforms",
        "url": "http://arxiv.org/abs/2407.09077v1",
        "pub_date": "2024-07-12",
        "summary": "Scientific workflows are often represented as directed acyclic graphs (DAGs),\nwhere vertices correspond to tasks and edges represent the dependencies between\nthem. Since these graphs are often large in both the number of tasks and their\nresource requirements, it is important to schedule them efficiently on parallel\nor distributed compute systems. Typically, each task requires a certain amount\nof memory to be executed and needs to communicate data to its successor tasks.\nThe goal is thus to execute the workflow as fast as possible (i.e., to minimize\nits makespan) while satisfying the memory constraints. Hence, we investigate\nthe partitioning and mapping of DAG-shaped workflows onto heterogeneous\nplatforms where each processor can have a different speed and a different\nmemory size. We first propose a baseline algorithm in the absence of existing\nmemory-aware solutions. As our main contribution, we then present a four-step\nheuristic. Its first step is to partition the input DAG into smaller blocks\nwith an existing DAG partitioner. The next two steps adapt the resulting blocks\nof the DAG to fit the processor memories and optimize for the overall makespan\nby further splitting and merging these blocks. Finally, we use local search via\nblock swaps to further improve the makespan. Our experimental evaluation on\nreal-world and simulated workflows with up to 30,000 tasks shows that\nexploiting the heterogeneity with the four-step heuristic reduces the makespan\nby a factor of 2.44 on average (even more on large workflows), compared to the\nbaseline that ignores heterogeneity.",
        "authors": "Svetlana Kulagina, Henning Meyerhenke, Anne Benoit",
        "translated": "科学工作流通常表示为有向无环图(DAGs) ，其中顶点对应于任务，边表示任务之间的依赖关系。由于这些图表在任务数量和资源需求方面通常都很大，因此在并行或分布式计算系统上有效地调度任务非常重要。通常，每个任务都需要执行一定数量的内存，并且需要将数据传递给其后续任务。因此，目标是在满足内存约束的同时尽可能快地执行工作流(即最小化其完成时间)。因此，我们研究 DAG 形状的工作流在异构平台上的划分和映射，在这些平台上，每个处理器可以有不同的速度和不同的内存大小。我们首先提出一个基线算法，在缺乏现有的内存感知解决方案的情况下。作为我们的主要贡献，然后我们提出了四个步骤的启发。它的第一步是使用现有的 DAG 分区程序将输入 DAG 划分为更小的块。接下来的两个步骤调整 DAG 的结果块以适应处理器存储器，并通过进一步拆分和合并这些块来优化整个完成时间。最后，我们使用本地搜索通过块交换，以进一步提高完成时间。我们对多达30,000个任务的真实世界和模拟工作流的实验评估表明，与忽略异构性的基线相比，利用四步启发式的异构性平均减少了2.44倍的完成时间(在大型工作流上更多)。"
    },
    {
        "title": "Enabling Elastic Model Serving with MultiWorld",
        "url": "http://arxiv.org/abs/2407.08980v1",
        "pub_date": "2024-07-12",
        "summary": "Machine learning models have been exponentially growing in terms of their\nparameter size over the past few years. We are now seeing the rise of\ntrillion-parameter models. The large models cannot fit into a single GPU and\nthus require partitioned deployment across GPUs and even hosts. A\nhigh-performance collective communication library (CCL) such as NCCL is\nessential to fully utilize expensive GPU resources. However, CCL is not a great\nfit for inference. Unlike training for which a fixed amount of GPU resources is\nused for fixed workloads (e.g., input datasets), the inference workloads can\nchange dynamically over time. Failures at the serving time can also impact\nindividual user's experiences directly. In contrast, workers in a CCL process\ngroup share a single fault domain and the process group cannot grow as the\nworkloads increase. The gap between the unique characteristics of model serving\nand CCL's nature makes it hard to serve large models elastically. To bridge the\ngap, we propose MultiWorld that enables fault tolerance and online scaling at\nthe granularity of workers for model serving. Our evaluation showcases that\nenabling these new functionalities incurs small overheads (1.4-4.3% throughput\nloss) for most of the scenarios we tested.",
        "authors": "Myungjin Lee, Akshay Jajoo, Ramana Rao Kompella",
        "translated": "在过去的几年中，机器学习模型的参数大小呈指数级增长。我们现在看到万亿参数模型的兴起。大型模型不能适用于单个 GPU，因此需要跨 GPU 甚至主机进行分区部署。像 NCCL 这样的高性能集体通信库(CCL)对于充分利用昂贵的 GPU 资源至关重要。然而，CCL 并不是一个很好的推理工具。与固定工作负载(例如输入数据集)使用固定数量的 GPU 资源的训练不同，推断工作负载可以随时间动态变化。服务时间的失败也会直接影响个人用户的体验。相比之下，CCL 流程组中的工作者共享单个故障域，流程组不能随着工作负载的增加而增长。模特服务的独特性与 CCL 的本质之间的差距，使得大型模特服务难以弹性化。为了弥补这一差距，我们提出了 MultiWorld，它支持在模型服务的工作者粒度上实现容错和在线扩展。我们的评估表明，对于我们测试的大多数场景，启用这些新功能会带来小的开销(1.4-4.3% 的吞吐量损失)。"
    },
    {
        "title": "Error Bounds for the Network Scale-Up Method",
        "url": "http://arxiv.org/abs/2407.10640v1",
        "pub_date": "2024-07-15",
        "summary": "Epidemiologists and social scientists have used the Network Scale-Up Method\n(NSUM) for over thirty years to estimate the size of a hidden sub-population\nwithin a social network. This method involves querying a subset of network\nnodes about the number of their neighbours belonging to the hidden\nsub-population. In general, NSUM assumes that the social network topology and\nthe hidden sub-population distribution are well-behaved; hence, the NSUM\nestimate is close to the actual value. However, bounds on NSUM estimation\nerrors have not been analytically proven. This paper provides analytical bounds\non the error incurred by the two most popular NSUM estimators. These bounds\nassume that the queried nodes accurately provide their degree and the number of\nneighbors belonging to the hidden population. Our key findings are twofold.\nFirst, we show that when an adversary designs the network and places the hidden\nsub-population, then the estimate can be a factor of $\\Omega(\\sqrt{n})$ off\nfrom the real value (in a network with $n$ nodes). Second, we also prove error\nbounds when the underlying network is randomly generated, showing that a small\nconstant factor can be achieved with high probability using samples of\nlogarithmic size $O(\\log{n})$. We present improved analytical bounds for\nErdos-Renyi and Scale-Free networks. Our theoretical analysis is supported by\nan extensive set of numerical experiments designed to determine the effect of\nthe sample size on the accuracy of the estimates in both synthetic and real\nnetworks.",
        "authors": "Sergio Díaz-Aranda, Juan Marcos Ramírez, Mohit Daga, Jaya Prakash Champati, José Aguilar, Rosa Elvira Lillo, Antonio Fernández Anta",
        "translated": "30多年来，流行病学家和社会科学家一直使用网络放大法(NSUM)来估计社会网络中隐藏的子群体的规模。这种方法包括查询网络节点子集中属于隐藏子种群的邻居数。一般来说，NSUM 假设社会网络拓扑和隐藏子人口分布表现良好，因此，NSUM 估计值接近实际值。然而，NSUM 估计误差的界限尚未得到解析证明。本文给出了两个最常用的 NSUM 估计量误差的解析界。这些边界假设被查询的节点准确地提供了它们的度和属于隐藏种群的邻居的数量。我们的主要发现有两方面。首先，我们展示了当一个对手设计网络并放置隐藏的子种群时，那么估计值可以是与实际值相差 $Omega (sqrt { n }) $的一个因子(在一个有 $n $节点的网络中)。其次，我们证明了随机生成底层网络时的误差界，表明使用对数大小为 $O (log { n }) $的样本可以以很高的概率得到一个小的常数因子。给出了 Erdos-Renyi 网络和无标度网络的改进解析界。我们的理论分析得到了大量数值实验的支持，这些实验旨在确定样本量对合成网络和实际网络估计精度的影响。"
    },
    {
        "title": "Comprehensive Review of Performance Optimization Strategies for\n  Serverless Applications on AWS Lambda",
        "url": "http://arxiv.org/abs/2407.10397v1",
        "pub_date": "2024-07-15",
        "summary": "This review paper synthesizes the latest research on performance optimization\nstrategies for serverless applications deployed on AWS Lambda. By examining\nrecent studies, we highlight the challenges, solutions, and best practices for\nenhancing the performance, cost efficiency, and scalability of serverless\napplications. The review covers a range of optimization techniques including\nresource management, runtime selection, observability improvements, and\nworkload aware operations.",
        "authors": "Mohamed Lemine El Bechir, Cheikh Sad Bouh, Abobakr Shuwail",
        "translated": "本文综述了基于 AWS Lambda 的无服务器应用程序性能优化策略的最新研究进展。通过研究最近的研究，我们强调了提高无服务器应用程序的性能、成本效率和可伸缩性的挑战、解决方案和最佳实践。综述涵盖了一系列优化技术，包括资源管理、运行时选择、可观测性改进和工作负载感知操作。"
    },
    {
        "title": "StatuScale: Status-aware and Elastic Scaling Strategy for Microservice\n  Applications",
        "url": "http://arxiv.org/abs/2407.10173v1",
        "pub_date": "2024-07-14",
        "summary": "Microservice architecture has transformed traditional monolithic applications\ninto lightweight components. Scaling these lightweight microservices is more\nefficient than scaling servers. However, scaling microservices still faces the\nchallenges resulted from the unexpected spikes or bursts of requests, which are\ndifficult to detect and can degrade performance instantaneously. To address\nthis challenge and ensure the performance of microservice-based applications,\nwe propose a status-aware and elastic scaling framework called StatuScale,\nwhich is based on load status detector that can select appropriate elastic\nscaling strategies for differentiated resource scheduling in vertical scaling.\nAdditionally, StatuScale employs a horizontal scaling controller that utilizes\ncomprehensive evaluation and resource reduction to manage the number of\nreplicas for each microservice. We also present a novel metric named\ncorrelation factor to evaluate the resource usage efficiency. Finally, we use\nKubernetes, an open-source container orchestration and management platform, and\nrealistic traces from Alibaba to validate our approach. The experimental\nresults have demonstrated that the proposed framework can reduce the average\nresponse time in the Sock-Shop application by 8.59% to 12.34%, and in the\nHotel-Reservation application by 7.30% to 11.97%, decrease service level\nobjective violations, and offer better performance in resource usage compared\nto baselines.",
        "authors": "Linfeng Wen, Minxian Xu, Sukhpal Singh Gill, Muhammad Hafizhuddin Hilman, Satish Narayana Srirama, Kejiang Ye, Chengzhong Xu",
        "translated": "微服务体系结构已经将传统的单片机应用转变为轻量级组件。扩展这些轻量级微服务比扩展服务器更有效。然而，扩展微服务仍然面临着由于意外的尖峰或者突发的请求而带来的挑战，这些请求很难被检测到并且可以瞬间降低性能。为了解决这一问题并保证基于微服务的应用程序的性能，我们提出了一种基于负载状态检测器的状态感知和弹性伸缩框架 StatuScale，该框架可以选择合适的弹性伸缩策略来进行垂直伸缩的差异化资源调度。此外，StatuScale 还使用了一个水平缩放控制器，该控制器利用综合评估和资源减少来管理每个微服务的副本数量。我们还提出了一种新的度量方法——相关因子来评估资源的使用效率。最后，我们使用开源容器编排和管理平台 Kubernetes，以及来自阿里巴巴的现实痕迹来验证我们的方法。实验结果表明，该框架可以将 Sock-Shop 应用程序的平均响应时间减少8.59% 至12.34% ，酒店预订应用程序的平均响应时间减少7.30% 至11.97% ，减少服务水平客观违规行为，并提供比基线更好的资源使用性能。"
    },
    {
        "title": "DRPC: Distributed Reinforcement Learning Approach for Scalable Resource\n  Provisioning in Container-based Clusters",
        "url": "http://arxiv.org/abs/2407.10169v1",
        "pub_date": "2024-07-14",
        "summary": "Microservices have transformed monolithic applications into lightweight,\nself-contained, and isolated application components, establishing themselves as\na dominant paradigm for application development and deployment in public clouds\nsuch as Google and Alibaba. Autoscaling emerges as an efficient strategy for\nmanaging resources allocated to microservices' replicas. However, the dynamic\nand intricate dependencies within microservice chains present challenges to the\neffective management of scaled microservices. Additionally, the centralized\nautoscaling approach can encounter scalability issues, especially in the\nmanagement of large-scale microservice-based clusters. To address these\nchallenges and enhance scalability, we propose an innovative distributed\nresource provisioning approach for microservices based on the Twin Delayed Deep\nDeterministic Policy Gradient algorithm. This approach enables effective\nautoscaling decisions and decentralizes responsibilities from a central node to\ndistributed nodes. Comparative results with state-of-the-art approaches,\nobtained from a realistic testbed and traces, indicate that our approach\nreduces the average response time by 15% and the number of failed requests by\n24%, validating improved scalability as the number of requests increases.",
        "authors": "Haoyu Bai, Minxian Xu, Kejiang Ye, Rajkumar Buyya, Chengzhong Xu",
        "translated": "微服务已经将单一应用程序转变成轻量级、自包含、独立的应用程序组件，使其成为在谷歌(Google)和阿里巴巴等公共云中进行应用程序开发和部署的主要范例。自动伸缩是管理分配给微服务副本的资源的一种有效策略。然而，微服务链中动态和复杂的依赖关系对规模化微服务的有效管理提出了挑战。此外，集中式自动伸缩方法可能会遇到可伸缩性问题，特别是在基于微服务的大型集群的管理中。为了应对这些挑战并提高可扩展性，我们提出了一种基于双时延深度确定性策略梯度算法的微服务分布式资源供应方法。这种方法支持有效的自动缩放决策，并将责任从中央节点分散到分布式节点。从现实的测试平台和跟踪中获得的与最先进的方法的比较结果表明，我们的方法将平均响应时间减少了15% ，失败请求数量减少了24% ，验证了随着请求数量的增加而改进的可伸缩性。"
    },
    {
        "title": "Accelerator-as-a-Service in Public Clouds: An Intra-Host Traffic\n  Management View for Performance Isolation in the Wild",
        "url": "http://arxiv.org/abs/2407.10098v1",
        "pub_date": "2024-07-14",
        "summary": "I/O devices in public clouds have integrated increasing numbers of hardware\naccelerators, e.g., AWS Nitro, Azure FPGA and Nvidia BlueField. However, such\nspecialized compute (1) is not explicitly accessible to cloud users with\nperformance guarantee, (2) cannot be leveraged simultaneously by both providers\nand users, unlike general-purpose compute (e.g., CPUs). Through ten\nobservations, we present that the fundamental difficulty of democratizing\naccelerators is insufficient performance isolation support. The key obstacles\nto enforcing accelerator isolation are (1) too many unknown traffic patterns in\npublic clouds and (2) too many possible contention sources in the datapath. In\nthis work, instead of scheduling such complex traffic on-the-fly and augmenting\nisolation support on each system component, we propose to model traffic as\nnetwork flows and proactively re-shape the traffic to avoid unpredictable\ncontention. We discuss the implications of our findings on the design of future\nI/O management stacks and device interfaces.",
        "authors": "Jiechen Zhao, Ran Shu, Katie Lim, Zewen Fan, Thomas Anderson, Mingyu Gao, Natalie Enright Jerger",
        "translated": "公共云中的 I/O 设备集成了越来越多的硬件加速器，例如 AWS Nitro、 Azure FPGA 和 Nvidia BlueField。然而，与通用计算(例如 CPU)不同，这样的专用计算(1)不能被云用户明确访问，并且性能保证(2)不能被提供者和用户同时利用。通过10次观察，我们发现加速器民主化的根本困难在于性能隔离支持不足。实施加速器隔离的主要障碍是: (1)公共云中有太多未知的流量模式; (2)数据路径中有太多可能的争用源。在这项工作中，我们提出将流量建模为网络流量，并主动重新塑造流量，以避免不可预测的争用，而不是在每个系统组件上动态调度这种复杂的流量和增加隔离支持。我们讨论我们的发现对未来 I/O 管理栈和设备接口设计的影响。"
    },
    {
        "title": "Distributed computing for physics-based data-driven reduced modeling at\n  scale: Application to a rotating detonation rocket engine",
        "url": "http://arxiv.org/abs/2407.09994v1",
        "pub_date": "2024-07-13",
        "summary": "High-performance computing (HPC) has revolutionized our ability to perform\ndetailed simulations of complex real-world processes. A prominent contemporary\nexample is from aerospace propulsion, where HPC is used for rotating detonation\nrocket engine (RDRE) simulations in support of the design of next-generation\nrocket engines; however, these simulations take millions of core hours even on\npowerful supercomputers, which makes them impractical for engineering tasks\nlike design exploration and risk assessment. Reduced-order models (ROMs)\naddress this limitation by constructing computationally cheap yet sufficiently\naccurate approximations that serve as surrogates for the high-fidelity model.\nThis paper contributes a new distributed algorithm that achieves fast and\nscalable construction of predictive physics-based ROMs trained from sparse\ndatasets of extremely large state dimension. The algorithm learns structured\nphysics-based ROMs that approximate the dynamical systems underlying those\ndatasets. This enables model reduction for problems at a scale and complexity\nthat exceeds the capabilities of existing approaches. We demonstrate our\nalgorithm's scalability using up to $2,048$ cores on the Frontera supercomputer\nat the Texas Advanced Computing Center. We focus on a real-world\nthree-dimensional RDRE for which one millisecond of simulated physical time\nrequires one million core hours on a supercomputer. Using a training dataset of\n$2,536$ snapshots each of state dimension $76$ million, our distributed\nalgorithm enables the construction of a predictive data-driven reduced model in\njust $13$ seconds on $2,048$ cores on Frontera.",
        "authors": "Ionut-Gabriel Farcas, Rayomand P. Gundevia, Ramakanth Munipalli, Karen E. Willcox",
        "translated": "高性能计算(HPC)彻底改变了我们对复杂现实世界过程进行详细模拟的能力。当代一个突出的例子来自航空推进，HPC 被用于旋转爆震火箭发动机(RDRE)模拟，以支持下一代火箭发动机的设计; 然而，这些模拟即使在功能强大的超级计算机上也需要数百万个核心小时，这使得它们在设计探索和风险评估等工程任务中不切实际。降阶模型(ROM)通过构造计算廉价但足够精确的近似值来解决这一局限性，这些近似值可以作为高保真模型的替代物。本文提供了一个新的分散式演算法，实现了基于预测物理的 ROM 的快速可扩展构建，这些预测物理的 ROM 由极大状态维度的稀疏数据集训练而成。该算法学习基于结构化物理的 ROM，这些 ROM 近似于这些数据集下的动力学系统。这使得问题的模型简化的规模和复杂性超出了现有方法的能力。我们在德克萨斯高级计算中心的 Frontera 超级计算机上展示了我们的算法的可扩展性，其核心价值高达2,048美元。我们关注的是一个真实世界的三维 RDRE，其中一毫秒的模拟物理时间需要超级计算机上的一百万个核心小时。通过使用每个状态维度 $7600万 $2536 $快照的训练数据集，我们的分散式演算法可以在 Frontera 上 $2048 $的核心上仅用 $13 $秒的时间构建一个预测性数据驱动的简化模型。"
    },
    {
        "title": "Memory Lower Bounds and Impossibility Results for Anonymous Dynamic\n  Broadcast",
        "url": "http://arxiv.org/abs/2407.09714v1",
        "pub_date": "2024-07-12",
        "summary": "Broadcast is a ubiquitous distributed computing problem that underpins many\nother system tasks. In static, connected networks, it was recently shown that\nbroadcast is solvable without any node memory and only constant-size messages\nin worst-case asymptotically optimal time (Hussak and Trehan,\nPODC'19/STACS'20/DC'23). In the dynamic setting of adversarial topology\nchanges, however, existing algorithms rely on identifiers, port labels, or\npolynomial memory to solve broadcast and compute functions over node inputs. We\ninvestigate space-efficient, terminating broadcast algorithms for anonymous,\nsynchronous, 1-interval connected dynamic networks and introduce the first\nmemory lower bounds in this setting. Specifically, we prove that broadcast with\ntermination detection is impossible for idle-start algorithms (where only the\nbroadcaster can initially send messages) and otherwise requires $\\Omega(\\log\nn)$ memory per node, where $n$ is the number of nodes in the network. Even if\nthe termination condition is relaxed to stabilizing termination (eventually no\nadditional messages are sent), we show that any idle-start algorithm must use\n$\\omega(1)$ memory per node, separating the static and dynamic settings for\nanonymous broadcast. This lower bound is not far from optimal, as we present an\nalgorithm that solves broadcast with stabilizing termination using\n$\\mathcal{O}(\\log n)$ memory per node in worst-case asymptotically optimal\ntime. In sum, these results reveal the necessity of non-constant memory for\nnontrivial terminating computation in anonymous dynamic networks.",
        "authors": "Garrett Parzych, Joshua J. Daymude",
        "translated": "广播是一个无处不在的分布式计算问题，是许多其他系统任务的基础。在静态的，连接的网络中，最近显示广播是可解的，没有任何节点存储器，只有在最坏情况下渐近最佳时间的常量消息(Hussak 和 Trehan，PODC’19/STACS’20/DC’23)。然而，在对抗性拓扑变化的动态设置中，现有算法依赖于标识符、端口标签或多项式存储器来解决节点输入上的广播和计算函数。我们研究了匿名、同步、1-间隔连接的动态网络的空间有效的终止广播算法，并在此设置中引入了第一个内存下界。具体来说，我们证明了带终止检测的广播对于空闲启动算法是不可能的(在这种情况下，只有广播机构最初可以发送消息) ，否则每个节点需要 $Omega (log n) $内存，其中 $n $是网络中的节点数。即使终止条件被放松到稳定终止(最终不再发送额外的消息) ，我们表明任何空闲启动算法必须使用每个节点 $omega (1) $memory，为匿名广播分离静态和动态设置。这个下界离最优不远，因为我们提出了一个算法，在最坏情况下渐近最优时间使用每个节点的 $mathcal { O }(log n) $memory 解决具有稳定终止的广播问题。总之，这些结果揭示了在匿名动态网络中非平凡终止计算中非常量存储器的必要性。"
    },
    {
        "title": "Hydra: Brokering Cloud and HPC Resources to Support the Execution of\n  Heterogeneous Workloads at Scale",
        "url": "http://arxiv.org/abs/2407.11967v1",
        "pub_date": "2024-07-16",
        "summary": "Scientific discovery increasingly depends on middleware that enables the\nexecution of heterogeneous workflows on heterogeneous platforms One of the main\nchallenges is to design software components that integrate within the existing\necosystem to enable scale and performance across cloud and high-performance\ncomputing HPC platforms Researchers are met with a varied computing landscape\nwhich includes services available on commercial cloud platforms data and\nnetwork capabilities specifically designed for scientific discovery on\ngovernment-sponsored cloud platforms and scale and performance on HPC platforms\nWe present Hydra an intra cross-cloud HPC brokering system capable of\nconcurrently acquiring resources from commercial private cloud and HPC\nplatforms and managing the execution of heterogeneous workflow applications on\nthose resources This paper offers four main contributions (1) the design of\nbrokering capabilities in the presence of task platform resource and middleware\nheterogeneity; (2) a reference implementation of that design with Hydra; (3) an\nexperimental characterization of Hydra s overheads and strong weak scaling with\nheterogeneous workloads and platforms and, (4) the implementation of a workflow\nthat models sea rise with Hydra and its scaling on cloud and HPC platforms",
        "authors": "Aymen Alsaadi, Shantenu Jha, Matteo Turilli",
        "translated": "科学发现越来越依赖于能够在异构平台上执行异构工作流的中间件。其中一个主要挑战是设计能够集成在现有生态系统中的软件组件，以实现跨云和高性能计算 HPC 平台的规模和性能。研究人员遇到了各种各样的计算环境，包括商业云平台上可用的服务数据和专门为在政府支持的云平台上进行科学发现而设计的网络能力，以及在HPC 平台我们提出了 Hydra 一个跨云 HPC 代理系统，它能够同时从商业私有云和 HPC 平台获取资源，并管理这些资源上异构工作流应用程序的执行。本文提出了四个主要贡献: (1)在任务平台资源和中间件异构的情况下设计代理能力;(2)该设计与 Hydra 的参考实现; (3) Hydra 开销的实验角色塑造，以及异构工作负载和平台的强弱伸缩性; (4)实现一个工作流程，用 Hydra 模拟海平面上升及其在云和 HPC 平台上的伸缩性"
    },
    {
        "title": "Personalized Conversational Travel Assistant powered by Generative AI",
        "url": "http://arxiv.org/abs/2407.11830v1",
        "pub_date": "2024-07-16",
        "summary": "The Tourism and Destination Management Organization (DMO) industry is rapidly\nevolving to adapt to new technologies and traveler expectations. Generative\nArtificial Intelligence (AI) offers an astonishing and innovative opportunity\nto enhance the tourism experience by providing personalized, interactive and\nengaging assistance. In this article, we propose a generative AI-based chatbot\nfor tourism assistance. The chatbot leverages AI ability to generate realistic\nand creative texts, adopting the friendly persona of the well-known Italian\nall-knowledgeable aunties, to provide tourists with personalized information,\ntailored and dynamic pre, during and post recommendations and trip plans and\npersonalized itineraries, using both text and voice commands, and supporting\ndifferent languages to satisfy Italian and foreign tourists expectations. This\nwork is under development in the Molise CTE research project, funded by the\nItalian Minister of the Economic Growth (MIMIT), with the aim to leverage the\nbest emerging technologies available, such as Cloud and AI to produce state of\nthe art solutions in the Smart City environment.",
        "authors": "Alexio Cassani, Michele Ruberl, Antonio Salis, Giacomo Giannese, Gianluca Boanelli",
        "translated": "旅游和目的地管理组织(DMO)行业正在迅速发展，以适应新技术和旅行者的期望。生成性人工智能(AI)提供了一个惊人的和创新的机会，通过提供个性化的，互动的和参与性的协助，以提高旅游体验。在本文中，我们提出了一个基于生成式人工智能的旅游协助聊天机器人。聊天机器人利用人工智能生成现实和创造性文本的能力，采用著名的意大利全知阿姨的友好角色，为游客提供个性化的信息，定制和动态的前期、中期和后期推荐和旅行计划以及个性化的行程，使用文本和语音命令，并支持不同的语言，以满足意大利和外国游客的期望。这项工作正在由意大利经济增长部(mIMIT)资助的莫利塞创新与创新教育研究项目中进行，目的是利用现有的最佳新兴技术，如云和人工智能，在智能城市环境中生产最先进的解决方案。"
    },
    {
        "title": "Scalable and Reliable Over-the-Air Federated Edge Learning",
        "url": "http://arxiv.org/abs/2407.11807v1",
        "pub_date": "2024-07-16",
        "summary": "Federated edge learning (FEEL) has emerged as a core paradigm for large-scale\noptimization. However, FEEL still suffers from a communication bottleneck due\nto the transmission of high-dimensional model updates from the clients to the\nfederator. Over-the-air computation (AirComp) leverages the additive property\nof multiple-access channels by aggregating the clients' updates over the\nchannel to save communication resources. While analog uncoded transmission can\nbenefit from the increased signal-to-noise ratio (SNR) due to the simultaneous\ntransmission of many clients, potential errors may severely harm the learning\nprocess for small SNRs. To alleviate this problem, channel coding approaches\nwere recently proposed for AirComp in FEEL. However, their error-correction\ncapability degrades with an increasing number of clients. We propose a digital\nlattice-based code construction with constant error-correction capabilities in\nthe number of clients, and compare to nested-lattice codes, well-known for\ntheir optimal rate and power efficiency in the point-to-point AWGN channel.",
        "authors": "Maximilian Egger, Christoph Hofmeister, Cem Kaya, Rawad Bitar, Antonia Wachter-Zeh",
        "translated": "联邦边缘学习(FEEL)已经成为大规模优化的核心范式。然而，由于将高维模型更新从客户机传输到联邦者，FEEL 仍然受到通信瓶颈的困扰。空中计算(AirComp)利用多访问通道的附加属性，通过聚合通道上客户端的更新来节省通信资源。虽然模拟非编码传输可以受益于增加的信噪比(由于同时传输许多客户端) ，潜在的错误可能会严重损害学习过程中的小信噪比。为了解决这个问题，最近提出了一种适用于自由电子阵列中 AirComp 的信道编码方法。然而，它们的纠错能力随着客户端数量的增加而降低。我们提出了一种具有恒定纠错能力的数字格子码结构，并与嵌套格子码进行了比较，嵌套格子码以其在点对点 AWGN 信道中的最佳速率和功率效率而著称。"
    },
    {
        "title": "PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined\n  Speculation",
        "url": "http://arxiv.org/abs/2407.11798v1",
        "pub_date": "2024-07-16",
        "summary": "Inference of Large Language Models (LLMs) across computer clusters has become\na focal point of research in recent times, with many acceleration techniques\ntaking inspiration from CPU speculative execution. These techniques reduce\nbottlenecks associated with memory bandwidth, but also increase end-to-end\nlatency per inference run, requiring high speculation acceptance rates to\nimprove performance. Combined with a variable rate of acceptance across tasks,\nspeculative inference techniques can result in reduced performance.\nAdditionally, pipeline-parallel designs require many user requests to maintain\nmaximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative\nacceleration technique to reduce inter-token latency and improve system\nutilization for single-request scenarios while also improving tolerance to low\nspeculation acceptance rates and low-bandwidth interconnects. PipeInfer\nexhibits up to a 2.15$\\times$ improvement in generation speed over standard\nspeculative inference. PipeInfer achieves its improvement through Continuous\nAsynchronous Speculation and Early Inference Cancellation, the former improving\nlatency and generation speed by running single-token inference simultaneously\nwith several speculative runs, while the latter improves speed and latency by\nskipping the computation of invalidated runs, even in the middle of inference.",
        "authors": "Branden Butler, Sixing Yu, Arya Mazaheri, Ali Jannesari",
        "translated": "跨计算机集群的大语言模型推理(LLMs)近年来已成为研究的焦点，许多加速技术都是从 CPU Speculative_execution 中获得灵感。这些技术减少了与内存带宽相关的瓶颈，但是也增加了每次推理运行的端到端延迟，因此需要较高的推测接受率来提高性能。与跨任务的可变接受率相结合，推测性推断技术可能导致性能下降。此外，流水线并行设计需要许多用户请求来保持最大利用率。作为补救措施，我们提出了 PipeInfer，一种流水线推测加速技术，以减少令牌间延迟，提高单请求场景的系统利用率，同时提高对低推测接受率和低带宽互连的容忍度。与标准的推测性推断相比，PipeInfer 的生成速度提高了2.15美元。PipeInfer 通过连续异步推理和早期推理抵消来实现改进，前者通过与多个推理运行同时运行单令牌推理来提高延迟和生成速度，而后者通过跳过无效运行的计算来提高速度和延迟，甚至在推理过程中也是如此。"
    },
    {
        "title": "Enhancing Split Computing and Early Exit Applications through Predefined\n  Sparsity",
        "url": "http://arxiv.org/abs/2407.11763v1",
        "pub_date": "2024-07-16",
        "summary": "In the past decade, Deep Neural Networks (DNNs) achieved state-of-the-art\nperformance in a broad range of problems, spanning from object classification\nand action recognition to smart building and healthcare. The flexibility that\nmakes DNNs such a pervasive technology comes at a price: the computational\nrequirements preclude their deployment on most of the resource-constrained edge\ndevices available today to solve real-time and real-world tasks. This paper\nintroduces a novel approach to address this challenge by combining the concept\nof predefined sparsity with Split Computing (SC) and Early Exit (EE). In\nparticular, SC aims at splitting a DNN with a part of it deployed on an edge\ndevice and the rest on a remote server. Instead, EE allows the system to stop\nusing the remote server and rely solely on the edge device's computation if the\nanswer is already good enough. Specifically, how to apply such a predefined\nsparsity to a SC and EE paradigm has never been studied. This paper studies\nthis problem and shows how predefined sparsity significantly reduces the\ncomputational, storage, and energy burdens during the training and inference\nphases, regardless of the hardware platform. This makes it a valuable approach\nfor enhancing the performance of SC and EE applications. Experimental results\nshowcase reductions exceeding 4x in storage and computational complexity\nwithout compromising performance. The source code is available at\nhttps://github.com/intelligolabs/sparsity_sc_ee.",
        "authors": "Luigi Capogrosso, Enrico Fraccaroli, Giulio Petrozziello, Francesco Setti, Samarjit Chakraborty, Franco Fummi, Marco Cristani",
        "translated": "在过去的十年中，深度神经网络(DNN)在广泛的问题中取得了最先进的性能，从对象分类和动作识别到智能建筑和医疗保健。使 DNN 成为如此普遍的技术的灵活性是有代价的: 计算需求阻碍了它们在大多数资源受限的边缘设备上的部署，这些设备目前可用于解决实时和实际任务。本文介绍了一种新的方法来解决这一挑战，结合预定义的稀疏概念与分裂计算(SC)和早期退出(EE)。特别是，SC 的目标是将一个 DNN 与部分部署在边缘设备上的 DNN 和部分部署在远程服务器上的 DNN 进行分离。相反，EE 允许系统停止使用远程服务器，并且只依赖边缘设备的计算，如果答案已经足够好的话。具体而言，如何将这种预定义的稀疏性应用到 SC 和 EE 范例中从未被研究过。本文研究了这个问题，并展示了预定义的稀疏性如何显著降低训练和推理阶段的计算、存储和能量负担，而不管硬件平台如何。这使得它成为提高 SC 和 EE 应用程序性能的一种有价值的方法。实验结果表明，在不影响性能的情况下，存储和计算复杂度降低超过4倍。源代码可在 https://github.com/intelligolabs/sparsity_sc_ee 下载。"
    },
    {
        "title": "Self-Duplicating Random Walks for Resilient Decentralized Learning on\n  Graphs",
        "url": "http://arxiv.org/abs/2407.11762v1",
        "pub_date": "2024-07-16",
        "summary": "Consider the setting of multiple random walks (RWs) on a graph executing a\ncertain computational task. For instance, in decentralized learning via RWs, a\nmodel is updated at each iteration based on the local data of the visited node\nand then passed to a randomly chosen neighbor. RWs can fail due to node or link\nfailures. The goal is to maintain a desired number of RWs to ensure failure\nresilience. Achieving this is challenging due to the lack of a central entity\nto track which RWs have failed to replace them with new ones by forking\n(duplicating) surviving ones. Without duplications, the number of RWs will\neventually go to zero, causing a catastrophic failure of the system. We propose\na decentralized algorithm called DECAFORK that can maintain the number of RWs\nin the graph around a desired value even in the presence of arbitrary RW\nfailures. Nodes continuously estimate the number of surviving RWs by estimating\ntheir return time distribution and fork the RWs when failures are likely to\nhappen. We present extensive numerical simulations that show the performance of\nDECAFORK regarding fast detection and reaction to failures. We further present\ntheoretical guarantees on the performance of this algorithm.",
        "authors": "Maximilian Egger, Ghadir Ayache, Rawad Bitar, Antonia Wachter-Zeh, Salim El Rouayheb",
        "translated": "考虑在执行某个计算任务的图上设置多个随机游动(RW)。例如，在基于 RW 的分散学习中，每次迭代都根据被访问节点的本地数据更新模型，然后将模型传递给随机选择的邻居。RW 可能由于节点或链路故障而失败。目标是维护所需数量的 RW，以确保故障恢复能力。实现这一点是具有挑战性的，因为缺乏一个中心实体来跟踪哪些 RW 未能通过分叉(复制)存活的 RW 来用新的 RW 替换它们。如果没有重复，RW 的数量最终将变为零，从而导致系统的灾难性故障。我们提出了一种称为 DECAFORK 的分散算法，即使在存在任意 RW 故障的情况下，该算法仍然可以保持图中 RW 的数目围绕一个期望值。节点通过估计返回时间分布不断估计存活 RW 的数量，并在可能发生故障时分叉 RW。我们提出了广泛的数值模拟，显示了性能的 DECAFORK 快速检测和反应的故障。进一步给出了该算法性能的理论保证。"
    },
    {
        "title": "Revolutionizing MRI Data Processing Using FSL: Preliminary Findings with\n  the Fugaku Supercomputer",
        "url": "http://arxiv.org/abs/2407.11742v1",
        "pub_date": "2024-07-16",
        "summary": "The amount of Magnetic resonance imaging data has grown tremendously\nrecently, creating an urgent need to accelerate data processing, which requires\nsubstantial computational resources and time. In this preliminary study, we\napplied FMRIB Software Library commands on T1-weighted and diffusion-weighted\nimages of a single young adult using the Fugaku supercomputer. The tensor-based\nmeasurements and subcortical structure segmentations performed on Fugaku\nsupercomputer were highly consistent with those from conventional systems,\ndemonstrating its reliability and significantly reduced processing time.",
        "authors": "Tianxiang Lyu, Wataru Uchida, Zhe Sun, Christina Andica, Keita Tokuda, Rui Zou, Jie Mao, Keigo Shimoji, Koji Kamagata, Mitsuhisa Sato, Ryutaro Himeno, Shigeki Aoki",
        "translated": "近年来，磁共振成像数据的数量急剧增长，因此迫切需要加快数据处理速度，这需要大量的计算资源和时间。在这个初步的研究中，我们使用 FMRIB 软件库命令对一个年轻人的 T1加权图像和扩散加权图像使用 Fugaku 超级计算机。在 Fugaku 超级计算机上进行的基于张量的测量和皮层下结构分割与传统系统的测量结果高度一致，证明了其可靠性，并显著缩短了处理时间。"
    },
    {
        "title": "Reducing Tail Latencies Through Environment- and Neighbour-aware Thread\n  Management",
        "url": "http://arxiv.org/abs/2407.11582v1",
        "pub_date": "2024-07-16",
        "summary": "Application tail latency is a key metric for many services, with high\nlatencies being linked directly to loss of revenue. Modern deeply-nested\nmicro-service architectures exacerbate tail latencies, increasing the\nlikelihood of users experiencing them. In this work, we show how CPU\novercommitment by OS threads leads to high tail latencies when applications are\nunder heavy load. CPU overcommitment can arise from two operational factors:\nincorrectly determining the number of CPUs available when under a CPU quota,\nand the ignorance of neighbour applications and their CPU usage. We discuss\ndifferent languages' solutions to obtaining the CPUs available, evaluating the\nimpact, and discuss opportunities for a more unified language-independent\ninterface to obtain the number of CPUs available. We then evaluate the impact\nof neighbour usage on tail latency and introduce a new neighbour-aware\nthreadpool, the friendlypool, that dynamically avoids overcommitment. In our\nevaluation, the friendlypool reduces maximum worker latency by up to\n$6.7\\times$ at the cost of decreasing throughput by up to $1.4\\times$.",
        "authors": "Andrew Jeffery, Chris Jensen, Richard Mortier",
        "translated": "应用程序尾延迟是许多服务的关键指标，高延迟与收入损失直接相关。现代深度嵌套的微服务架构加剧了尾部延迟，增加了用户体验尾部延迟的可能性。在这项工作中，我们展示了操作系统线程的 CPU 超量使用如何导致应用程序在重负载下的高尾延迟。CPU 超量使用可能源于两个操作因素: 在 CPU 配额下错误地确定可用 CPU 的数量，以及忽视相邻应用程序及其 CPU 使用情况。我们讨论了获取可用 CPU 的不同语言解决方案，评估影响，并讨论了使用更统一的独立于语言的接口来获取可用 CPU 数量的机会。然后，我们评估邻居使用对尾延迟的影响，并引入一个新的邻居感知线程池，友好池，动态避免过度承诺。在我们的评估中，Friendlypool 将最大工作延迟减少了6.7倍，代价是吞吐量减少了1.4倍。"
    },
    {
        "title": "Finite State Machines-Based Path-Following Collaborative Computing\n  Strategy for Emergency UAV Swarms",
        "url": "http://arxiv.org/abs/2407.11531v1",
        "pub_date": "2024-07-16",
        "summary": "Offloading services to UAV swarms for delay-sensitive tasks in Emergency UAV\nNetworks (EUN) can greatly enhance rescue efficiency. Most task-offloading\nstrategies assumed that UAVs were location-fixed and capable of handling all\ntasks. However, in complex disaster environments, UAV locations often change\ndynamically, and the heterogeneity of on-board resources presents a significant\nchallenge in optimizing task scheduling in EUN to minimize latency. To address\nthese problems, a Finite state machines-based Path-following Collaborative\ncomputation strategy (FPC) for emergency UAV swarms is proposed. First, an\nExtended Finite State Machine Space-time Graph (EFSMSG) model is constructed to\naccurately characterize on-board resources and state transitions while\nshielding the EUN dynamic characteristic. Based on the EFSMSG, a mathematical\nmodel is formulated for the FPC strategy to minimize task processing delay\nwhile facilitating computation during transmission. Finally, the Constraint\nSelection Adaptive Binary Particle Swarm Optimization (CSABPSO) algorithm is\nproposed for the solution. Simulation results demonstrate that the proposed FPC\nstrategy effectively reduces task processing delay, meeting the requirements of\ndelay-sensitive tasks in emergency situations.",
        "authors": "Jialin Hu, Zhiyuan Ren, Wenchi Cheng",
        "translated": "在应急无人机网络(EUN)中，为无人机群卸载服务以完成延迟敏感任务可以大大提高救援效率。大多数任务卸载策略假设无人机是位置固定的，能够处理所有任务。然而，在复杂的灾害环境中，无人机的位置经常会发生动态变化，机载资源的异构性对 EUN 中优化任务调度以减少延迟提出了严峻的挑战。针对这些问题，提出了一种基于有限状态机的应急无人机群路径跟踪协同计算策略(FPC)。首先，建立扩展有限状态机时空图(EFSMSG)模型，在屏蔽 EUN 动态特性的同时，准确地描述星载资源和状态转移。在 EFSMSG 的基础上，建立了 FPC 策略的数学模型，使任务处理延迟最小，同时便于传输过程中的计算。最后，提出了约束选择自适应二进制粒子群优化(cSAbPSO)算法来解决这个问题。仿真结果表明，所提出的 FPC 策略有效地降低了任务处理延迟，满足了紧急情况下时延敏感任务的要求。"
    },
    {
        "title": "Bringing Auto-tuning to HIP: Analysis of Tuning Impact and Difficulty on\n  AMD and Nvidia GPUs",
        "url": "http://arxiv.org/abs/2407.11488v1",
        "pub_date": "2024-07-16",
        "summary": "Many studies have focused on developing and improving auto-tuning algorithms\nfor Nvidia Graphics Processing Units (GPUs), but the effectiveness and\nefficiency of these approaches on AMD devices have hardly been studied. This\npaper aims to address this gap by introducing an auto-tuner for AMD's HIP. We\ndo so by extending Kernel Tuner, an open-source Python library for auto-tuning\nGPU programs. We analyze the performance impact and tuning difficulty for four\nhighly-tunable benchmark kernels on four different GPUs: two from Nvidia and\ntwo from AMD. Our results demonstrate that auto-tuning has a significantly\nhigher impact on performance on AMD compared to Nvidia (10x vs 2x).\nAdditionally, we show that applications tuned for Nvidia do not perform\noptimally on AMD, underscoring the importance of auto-tuning specifically for\nAMD to achieve high performance on these GPUs.",
        "authors": "Milo Lurati, Stijn Heldens, Alessio Sclocco, Ben van Werkhoven",
        "translated": "许多研究集中在开发和改进 Nvidia 图形处理器(GPU)的自动调整算法，但这些方法在 AMD 设备上的有效性和效率很少被研究。本文旨在通过引入 AMD 的 HIP 自动调谐器来弥补这一差距。为此，我们扩展了 Kernel Tuner，这是一个用于自动调优 gpU 程序的开源 Python 库。我们分析了在四个不同的 GPU 上四个高度可调基准内核的性能影响和调优难度: 两个来自 Nvidia，两个来自 AMD。我们的研究结果表明，自动调优对 AMD 性能的影响显著高于 Nvidia (10倍 vs 2倍)。此外，我们还展示了为 Nvidia 调优的应用程序在 AMD 上的性能并不是最佳的，这强调了专门为 AMD 进行自动调优以在这些 GPU 上实现高性能的重要性。"
    },
    {
        "title": "FlexFL: Heterogeneous Federated Learning via APoZ-Guided Flexible\n  Pruning in Uncertain Scenarios",
        "url": "http://arxiv.org/abs/2407.12729v1",
        "pub_date": "2024-07-17",
        "summary": "Along with the increasing popularity of Deep Learning (DL) techniques, more\nand more Artificial Intelligence of Things (AIoT) systems are adopting\nfederated learning (FL) to enable privacy-aware collaborative learning among\nAIoT devices. However, due to the inherent data and device heterogeneity\nissues, existing FL-based AIoT systems suffer from the model selection problem.\nAlthough various heterogeneous FL methods have been investigated to enable\ncollaborative training among heterogeneous models, there is still a lack of i)\nwise heterogeneous model generation methods for devices, ii) consideration of\nuncertain factors, and iii) performance guarantee for large models, thus\nstrongly limiting the overall FL performance. To address the above issues, this\npaper introduces a novel heterogeneous FL framework named FlexFL. By adopting\nour Average Percentage of Zeros (APoZ)-guided flexible pruning strategy, FlexFL\ncan effectively derive best-fit models for heterogeneous devices to explore\ntheir greatest potential. Meanwhile, our proposed adaptive local pruning\nstrategy allows AIoT devices to prune their received models according to their\nvarying resources within uncertain scenarios. Moreover, based on self-knowledge\ndistillation, FlexFL can enhance the inference performance of large models by\nlearning knowledge from small models. Comprehensive experimental results show\nthat, compared to state-of-the-art heterogeneous FL methods, FlexFL can\nsignificantly improve the overall inference accuracy by up to 14.24%.",
        "authors": "Zekai Chen, Chentao Jia, Ming Hu, Xiaofei Xie, Anran Li, Mingsong Chen",
        "translated": "随着深度学习(Deep Learning，DL)技术的日益普及，越来越多的人工智能(AI)系统正在采用联邦学习(federal Learning，FL)来实现 AIoT 设备之间的隐私合作学习。然而，由于固有的数据和设备异构性问题，现有的基于 FL 的 AIoT 系统存在模型选择问题。尽管已经研究了各种异构 FL 方法以使异构模型之间能够进行协同训练，但是仍然缺乏 i)明智的异构设备模型生成方法，ii)考虑不确定因素，以及 iii)大型模型的性能保证，从而强烈限制了整体 FL 性能。为了解决上述问题，本文提出了一种新的异构 FL 框架 FlexFL。通过采用平均零点百分比(APOZ)引导的灵活修剪策略，FlexFL 可以有效地为异构设备推导出最适合的模型，以发掘它们的最大潜力。同时，我们提出的自适应局部修剪策略允许 AIoT 设备在不确定的场景下根据不同的资源对接收到的模型进行修剪。此外，FlexFL 基于自知识精馏，通过从小模型中学习知识，提高了大模型的推理性能。综合实验结果表明，与现有的异构 FL 方法相比，FlexFL 方法可以显著提高整体推理准确率达14.24% 。"
    },
    {
        "title": "LSKV: A Confidential Distributed Datastore to Protect Critical Data in\n  the Cloud",
        "url": "http://arxiv.org/abs/2407.12623v1",
        "pub_date": "2024-07-17",
        "summary": "Software services are increasingly migrating to the cloud, requiring trust in\nactors with direct access to the hardware, software and data comprising the\nservice. A distributed datastore storing critical data sits at the core of many\nservices; a prime example being etcd in Kubernetes. Trusted execution\nenvironments can secure this data from cloud providers during execution, but it\nis complex to build trustworthy data storage systems using such mechanisms. We\npresent the design and evaluation of the Ledger-backed Secure Key-Value\ndatastore (LSKV), a distributed datastore that provides an etcd-like API but\ncan use trusted execution mechanisms to keep cloud providers outside the trust\nboundary. LSKV provides a path to transition traditional systems towards\nconfidential execution, provides competitive performance compared to etcd, and\nhelps clients to gain trust in intermediary services. LSKV forms a foundational\ncore, lowering the barriers to building more trustworthy systems.",
        "authors": "Andrew Jeffery, Julien Maffre, Heidi Howard, Richard Mortier",
        "translated": "软件服务正在越来越多地迁移到云端，需要对直接访问构成服务的硬件、软件和数据的参与者的信任。存储关键数据的分布式数据存储是许多服务的核心，Kubernetes 的 etcd 就是一个很好的例子。可信的执行环境可以在执行期间从云提供商保护这些数据，但是使用这些机制构建可信的数据存储系统是复杂的。我们介绍了 Ledger 支持的安全键值数据存储(Secure Key-Value datastore，LSKV)的设计和评估，LSKV 是一个分布式数据存储，它提供了一个类似 etcd 的 API，但是可以使用可信的执行机制将云提供者保持在信任边界之外。LSKV 为传统系统向保密执行过渡提供了一条途径，提供了相对于等级服务具有竞争力的性能，并帮助客户获得对中介服务的信任。LSKV 形成了一个基础核心，降低了构建更可靠系统的障碍。"
    },
    {
        "title": "Continuous reasoning for adaptive container image distribution in the\n  cloud-edge continuum",
        "url": "http://arxiv.org/abs/2407.12605v1",
        "pub_date": "2024-07-17",
        "summary": "Cloud-edge computing requires applications to operate across diverse\ninfrastructures, often triggered by cyber-physical events. Containers offer a\nlightweight deployment option but pulling images from central repositories can\ncause delays. This article presents a novel declarative approach and\nopen-source prototype for replicating container images across the cloud-edge\ncontinuum. Considering resource availability, network QoS, and storage costs,\nwe leverage logic programming to (i) determine optimal initial placements via\nAnswer Set Programming (ASP) and (ii) adapt placements using Prolog-based\ncontinuous reasoning. We evaluate our solution through simulations, showcasing\nhow combining ASP and Prolog continuous reasoning can balance cost optimisation\nand prompt decision-making in placement adaptation at increasing infrastructure\nsizes.",
        "authors": "Damiano Azzolini, Stefano Forti, Antonio Ielo",
        "translated": "云计算需要应用程序跨不同的基础设施运行，通常由网络物理事件触发。容器提供了一个轻量级的部署选项，但是从中央存储库提取映像会导致延迟。本文提出了一种新的声明性方法和开源原型，用于跨云边界连续体复制容器图像。考虑到资源可用性、网络服务质量和存储成本，我们利用逻辑编程(i)通过答案集编程(ASP)确定最佳初始位置(ii)使用基于 Prolog 的连续推理调整位置。我们通过仿真评估我们的解决方案，展示了如何结合 ASP 和 Prolog 连续推理可以平衡成本优化和迅速决策的位置适应在不断增加的基础设施规模。"
    },
    {
        "title": "Computing: Looking Back and Moving Forward",
        "url": "http://arxiv.org/abs/2407.12558v1",
        "pub_date": "2024-07-17",
        "summary": "The Internet and computer commercialization have transformed the computing\nsystems area over the past sixty years, affecting society. Computer systems\nhave evolved to meet diverse social needs thanks to technological advances. The\nInternet of Things (IoT), cloud computing, fog computing, edge computing, and\nother emerging paradigms provide new economic and creative potential.\nTherefore, this article explores and evaluates the elements impacting the\nadvancement of computing platforms, including both long standing systems and\nframeworks and more recent innovations like cloud computing, quantum\ntechnology, and edge AI. In this article, we examine computing paradigms,\ndomains, and next generation computing systems to better understand the past,\npresent, and future of computing technologies. This paper provides readers with\na comprehensive overview of developments in computing technologies and\nhighlights promising research gaps for the advancement of future computing\nsystems.",
        "authors": "Muhammed Golec, Sukhpal Singh Gill",
        "translated": "互联网和计算机商业化在过去的六十年里改变了计算机系统领域，影响了社会。由于技术的进步，计算机系统已经发展到能够满足不同的社会需求。物联网(IoT)、云计算、雾计算、边缘计算和其他新兴范式提供了新的经济和创造潜力。因此，本文将探索和评估影响计算平台进步的因素，包括长期存在的系统和框架以及最近的创新，如云计算、量子技术和边缘 AI。在本文中，我们将研究计算范例、领域和下一代计算系统，以更好地理解计算技术的过去、现在和未来。本文为读者提供了一个全面的概述计算技术的发展，并突出未来的计算系统的进步有希望的研究差距。"
    },
    {
        "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
        "url": "http://arxiv.org/abs/2407.12391v1",
        "pub_date": "2024-07-17",
        "summary": "This survey offers a comprehensive overview of recent advancements in Large\nLanguage Model (LLM) serving systems, focusing on research since the year 2023.\nWe specifically examine system-level enhancements that improve performance and\nefficiency without altering the core LLM decoding mechanisms. By selecting and\nreviewing high-quality papers from prestigious ML and system venues, we\nhighlight key innovations and practical considerations for deploying and\nscaling LLMs in real-world production environments. This survey serves as a\nvaluable resource for LLM practitioners seeking to stay abreast of the latest\ndevelopments in this rapidly evolving field.",
        "authors": "Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari",
        "translated": "这项调查提供了一个大型语言模型(LLM)服务系统的最新进展的综合概述，重点是自2023年以来的研究。我们特别研究了在不改变核心 LLM 解码机制的情况下提高性能和效率的系统级增强。通过选择和审查来自著名的机器学习和系统场所的高质量论文，我们强调在现实生产环境中部署和扩展机器学习的关键创新和实际考虑。这项调查是一个宝贵的资源，法律硕士从业人员寻求保持最新的发展，在这个迅速发展的领域。"
    },
    {
        "title": "Mitigating Interference of Microservices with a Scoring Mechanism in\n  Large-scale Clusters",
        "url": "http://arxiv.org/abs/2407.12248v1",
        "pub_date": "2024-07-17",
        "summary": "Co-locating latency-critical services (LCSs) and best-effort jobs (BEJs)\nconstitute the principal approach for enhancing resource utilization in\nproduction. Nevertheless, the co-location practice hurts the performance of\nLCSs due to resource competition, even when employing isolation technology.\nThrough an extensive analysis of voluminous real trace data derived from two\nproduction clusters, we observe that BEJs typically exhibit periodic execution\npatterns and serve as the primary sources of interference to LCSs. Furthermore,\ndespite occupying the same level of resource consumption, the diverse\ncompositions of BEJs can result in varying degrees of interference on LCSs.\nSubsequently, we propose PISM, a proactive Performance Interference Scoring and\nMitigating framework for LCSs through the optimization of BEJ scheduling.\nFirstly, PISM adopts a data-driven approach to establish a characterization and\nclassification methodology for BEJs. Secondly, PISM models the relationship\nbetween the composition of BEJs on servers and the response time (RT) of LCSs.\nThirdly, PISM establishes an interference scoring mechanism in terms of RT,\nwhich serves as the foundation for BEJ scheduling. We assess the effectiveness\nof PISM on a small-scale cluster and through extensive data-driven simulations.\nThe experiment results demonstrate that PISM can reduce cluster interference by\nup to 41.5%, and improve the throughput of long-tail LCSs by 76.4%.",
        "authors": "Dingyu Yang, Kangpeng Zheng, Shiyou Qian, Jian Cao, Guangtao Xue",
        "translated": "共同定位延迟关键服务(LCS)和尽力工作(BEJ)构成了提高生产中资源利用率的主要方法。然而，由于资源竞争，即使在使用隔离技术时，同位实践也会损害 LCS 的性能。通过对来自两个生产集群的大量实际跟踪数据的广泛分析，我们观察到 BEJ 通常表现出周期性的执行模式，并作为对 LCS 干扰的主要来源。此外，尽管 BEJ 占用了相同的资源消耗水平，但是不同的 BEJ 组成会对 LCS 产生不同程度的干扰。随后，我们提出了 PISM，一个积极的性能干扰评分和减轻框架的 LCS 通过优化的 BEJ 调度。首先，PISM 采用数据驱动的方法为 BEJ 建立角色塑造和分类方法。其次，PISM 建立了服务器上 BEJ 组成与 LCS 响应时间(RT)之间的关系模型。第三，PISM 建立了基于 RT 的干扰评分机制，为 BEJ 调度打下了基础。我们通过大量的数据驱动模拟来评估 PISM 在小规模集群上的有效性。实验结果表明，PISM 可以减少41.5% 的簇干扰，提高长尾 LCS 的吞吐量76.4% 。"
    },
    {
        "title": "The Latency Price of Threshold Cryptosystem in Blockchains",
        "url": "http://arxiv.org/abs/2407.12172v1",
        "pub_date": "2024-07-16",
        "summary": "Threshold cryptography is essential for many blockchain protocols. For\nexample, many protocols rely on threshold common coin to implement asynchronous\nconsensus, leader elections, and provide support for randomized applications.\nSimilarly, threshold signature schemes are frequently used for protocol\nefficiency and state certification, and threshold decryption and threshold\ntime-lock puzzles are often necessary for privacy.\n  In this paper, we study the interplay between threshold cryptography and a\nclass of blockchains that use Byzantine-fault tolerant (BFT) consensus\nprotocols with a focus on latency. More specifically, we focus on\nblockchain-native threshold cryptosystem, where the blockchain validators seek\nto run a threshold cryptographic protocol once for every block with the block\ncontents as an input to the threshold cryptographic protocol. All existing\napproaches for blockchain-native threshold cryptosystems introduce a latency\noverhead of at least one message delay for running the threshold cryptographic\nprotocol. In this paper, we first propose a mechanism to eliminate this\noverhead for blockchain-native threshold cryptosystems with tight thresholds,\ni.e., in threshold cryptographic protocols where the secrecy and reconstruction\nthresholds are the same. However, many real-world proof-of-stake-based\nblockchain-native threshold cryptosystems rely on ramp thresholds, where\nreconstruction thresholds are strictly greater than secrecy thresholds. For\nthese blockchains, we formally demonstrate that the additional delay is\nunavoidable. We then introduce a mechanism to minimize this delay in the\noptimistic case. We implement our optimistic protocol for the proof-of-stake\ndistributed randomness scheme on the Aptos blockchain. Our measurements from\nthe Aptos mainnet show that the optimistic approach reduces latency overhead by\n71%.",
        "authors": "Zhuolun Xiang, Sourav Das, Zekun Li, Zhoujun Ma, Alexander Spiegelman",
        "translated": "阈值加密对于许多区块链协议是必不可少的。例如，许多协议依赖于阈值公共硬币来实现异步共识、领导者选举，并为随机应用提供支持。类似地，门限签名方案经常用于协议效率和状态认证，而门限解密和门限时间锁难题经常是保护隐私所必需的。在本文中，我们研究了阈值加密技术和一类使用拜占庭容错(BFT)协商一致协议的区块链之间的相互作用。更具体地说，我们专注于区块链原生阈值加密系统，其中区块链验证器寻求对每个块运行一次阈值安全协议，将块内容作为阈值安全协议的输入。所有现有的区块链本地门限密码系统方法都会为运行门限安全协议带来至少一个消息延迟的延迟开销。在本文中，我们首先提出了一种机制来消除这种开销的块链-本机门限密码体制与严格的阈值，即在门限密码协议中，其中的秘密和重建阈值相同。然而，许多现实世界中基于木桩证明的区块链本地门限密码体制依赖于斜坡门限，其中重建门限严格大于保密门限。对于这些区块链，我们正式证明了额外的延迟是不可避免的。然后，我们引入一种机制，以最小化这种延迟在乐观的情况下。我们在 Aptos 区块链上实现了我们的乐观协议-证明木桩分布式随机方案。我们从阿普托斯大陆的测量显示，乐观的方法减少了71% 的延迟开销。"
    },
    {
        "title": "Building AI Agents for Autonomous Clouds: Challenges and Design\n  Principles",
        "url": "http://arxiv.org/abs/2407.12165v1",
        "pub_date": "2024-07-16",
        "summary": "The rapid growth in the use of Large Language Models (LLMs) and AI Agents as\npart of software development and deployment is revolutionizing the information\ntechnology landscape. While code generation receives significant attention, a\nhigher-impact application lies in using AI agents for operational resilience of\ncloud services, which currently require significant human effort and domain\nknowledge. There is a growing interest in AI for IT Operations (AIOps) which\naims to automate complex operational tasks, like fault localization and root\ncause analysis, thereby reducing human intervention and customer impact.\nHowever, achieving the vision of autonomous and self-healing clouds though\nAIOps is hampered by the lack of standardized frameworks for building,\nevaluating, and improving AIOps agents. This vision paper lays the groundwork\nfor such a framework by first framing the requirements and then discussing\ndesign decisions that satisfy them. We also propose AIOpsLab, a prototype\nimplementation leveraging agent-cloud-interface that orchestrates an\napplication, injects real-time faults using chaos engineering, and interfaces\nwith an agent to localize and resolve the faults. We report promising results\nand lay the groundwork to build a modular and robust framework for building,\nevaluating, and improving agents for autonomous clouds.",
        "authors": "Manish Shetty, Yinfang Chen, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Xuchao Zhang, Jonathan Mace, Dax Vandevoorde, Pedro Las-Casas, Shachee Mishra Gupta, Suman Nath, Chetan Bansal, Saravan Rajmohan",
        "translated": "作为软件开发和部署的一部分，大语言模型(LLM)和人工智能代理(AI Agent)的使用迅速增长，正在彻底改变信息技术的面貌。虽然代码生成受到重视，但影响更大的应用程序在于使用 AI 代理实现云服务的业务弹性，目前这需要大量人力和领域知识。人们对人工智能的兴趣与日俱增。人工智能的目的是使复杂的操作任务自动化，如故障定位和根本原因分析，从而减少人为干预和对客户的影响。然而，由于缺乏构建、评估和改进 AIOps 代理的标准化框架，通过 AIOps 实现自治和自愈云的愿景受到了阻碍。本远景文件通过首先构建需求，然后讨论满足需求的设计决策，为这样一个框架奠定了基础。我们还提出了 AIOpsLab，这是一个利用代理-云接口编排应用程序的原型实现，使用混沌工程注入实时故障，并与代理接口定位和解决故障。我们报告了有希望的结果，并为建立一个模块化的健壮框架奠定了基础，该框架用于构建、评估和改进自主云代理。"
    },
    {
        "title": "Gaming and Blockchain: Hype and Reality",
        "url": "http://arxiv.org/abs/2407.12134v1",
        "pub_date": "2024-07-16",
        "summary": "This paper explores the adoption of blockchain technology in the gaming\nindustry. While supporters affirm that distributed ledger technology has\npotential to revolutionize gaming economies and provide players with control\nover their virtual assets, there are practical challenges such as energy\nconsumption and user adoption to be addressed, and detractors question whether\nblockchain integration is even necessary. This report characterises popular\nblockchain-based gaming projects like Enjin and Axie Infinity, then compares\nmetrics such as transaction cost and player feedback to evaluate the longevity\nof blockchain-integrated gaming as a whole.",
        "authors": "Max McGuinness",
        "translated": "本文探讨了区块链技术在游戏产业中的应用。虽然支持者认为，分布式分类账技术有可能彻底改变游戏经济，并为玩家提供对其虚拟资产的控制权，但还有一些实际挑战需要解决，如能源消耗和用户采用，而批评者质疑区块链集成是否有必要。这份报告描述了受欢迎的基于区块链的游戏项目，如 Enjin 和 Axie Infinity，然后比较了交易成本和玩家反馈等指标，以评估整个区块链集成游戏的寿命。"
    },
    {
        "title": "Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs",
        "url": "http://arxiv.org/abs/2407.12117v1",
        "pub_date": "2024-07-16",
        "summary": "Nowadays, Large Language Models (LLMs) have been trained using extended\ncontext lengths to foster more creative applications. However, long context\ntraining poses great challenges considering the constraint of GPU memory. It\nnot only leads to substantial activation memory consumption during training,\nbut also incurs considerable memory fragmentation. To facilitate long context\ntraining, existing frameworks have adopted strategies such as recomputation and\nvarious forms of parallelisms. Nevertheless, these techniques rely on redundant\ncomputation or extensive communication, resulting in low Model FLOPS\nUtilization (MFU). In this paper, we propose MEMO, a novel LLM training\nframework designed for fine-grained activation memory management. Given the\nquadratic scaling of computation and linear scaling of memory with sequence\nlengths when using FlashAttention, we offload memory-consuming activations to\nCPU memory after each layer's forward pass and fetch them during the backward\npass. To maximize the swapping of activations without hindering computation,\nand to avoid exhausting limited CPU memory, we implement a token-wise\nactivation recomputation and swapping mechanism. Furthermore, we tackle the\nmemory fragmentation issue by employing a bi-level Mixed Integer Programming\n(MIP) approach, optimizing the reuse of memory across transformer layers.\nEmpirical results demonstrate that MEMO achieves an average of 2.42x and 2.26x\nMFU compared to Megatron-LM and DeepSpeed, respectively. This improvement is\nattributed to MEMO's ability to minimize memory fragmentation, reduce\nrecomputation and intensive communication, and circumvent the delays associated\nwith the memory reorganization process due to fragmentation. By leveraging\nfine-grained activation memory management, MEMO facilitates efficient training\nof 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU\nof 52.30%.",
        "authors": "Pinxue Zhao, Hailin Zhang, Fangcheng Fu, Xiaonan Nie, Qibin Liu, Fang Yang, Yuanbo Peng, Dian Jiao, Shuaipeng Li, Jinbao Xue, Yangyu Tao, Bin Cui",
        "translated": "如今，大型语言模型(LLM)已经通过使用扩展上下文长度来培养更具创造性的应用程序。然而，考虑到 GPU 内存的限制，长上下文训练提出了很大的挑战。它不仅导致训练期间大量的激活内存消耗，而且还引起相当大的内存碎片。为了方便长时间的上下文训练，现有的框架采用了重新计算和各种形式的并行处理等策略。然而，这些技术依赖于冗余计算或广泛的通信，导致低模型 FLOPS 利用率(MFU)。在本文中，我们提出了 MEMO，一个新的 LLM 训练框架设计的细粒度激活存储器管理。考虑到在使用 Flash 注意时计算的二次缩放和内存的线性缩放与序列长度，我们卸载内存消耗的激活到 CPU 内存后，每层的正向传递和获取它们在反向传递。为了在不妨碍计算的情况下最大化激活的交换，并避免耗尽有限的 CPU 内存，我们实现了一种基于令牌的激活重新计算和交换机制。此外，我们采用双层混合整数规划(MIP)方法解决内存碎片问题，优化跨变压器层的内存重用。实验结果表明，与威震天 LM 和 DeepSpeed 相比，MEMO 平均分别达到2.42 x 和2.26 x MFU。这种改进归功于 MEMO 最小化内存碎片、减少重新计算和密集通信以及规避由于碎片而导致的与内存重组过程相关的延迟的能力。通过利用细粒度激活内存管理，MEMO 促进了在8个 A800 GPU 上以100万序列长度有效地训练7B LLM，实现了52.30% 的 MFU。"
    },
    {
        "title": "DPDPU: Data Processing with DPUs",
        "url": "http://arxiv.org/abs/2407.13658v1",
        "pub_date": "2024-07-18",
        "summary": "Improving the performance and reducing the cost of cloud data systems is\nincreasingly challenging. Data processing units (DPUs) are a promising\nsolution, but utilizing them for data processing needs characterizing the new\nhardware and recognizing their capabilities and constraints. We hence propose\nDPDPU, a platform for holistically exploiting DPUs to optimize data processing\ntasks that are critical to performance and cost. It seeks to fill the semantic\ngap between DPUs and data processing systems and handle DPU heterogeneity with\nthree engines dedicated to compute, networking, and storage. This paper\ndescribes our vision, DPDPU's key components, their associated utilization\nchallenges, as well as the current progress and future plans.",
        "authors": "Jiasheng Hu, Philip A. Bernstein, Qizhen Zhang",
        "translated": "提高云数据系统的性能和降低成本越来越具有挑战性。数据处理单元(DPU)是一个有前途的解决方案，但利用它们的数据处理需要的特点，新的硬件和识别其能力和约束。因此，我们提出了 DPDPU，一个全面开发 DPU 的平台，以优化对性能和成本至关重要的数据处理任务。它寻求填补 DPU 和数据处理系统之间的语义鸿沟，并使用三个专门用于计算、网络和存储的引擎来处理 DPU 异构性。本文描述了我们的愿景，DPDPU 的关键组件，它们相关的利用挑战，以及当前的进展和未来的计划。"
    },
    {
        "title": "DDS: DPU-optimized Disaggregated Storage",
        "url": "http://arxiv.org/abs/2407.13618v1",
        "pub_date": "2024-07-18",
        "summary": "This extended report presents DDS, a novel disaggregated storage architecture\nenabled by emerging networking hardware, namely DPUs (Data Processing Units).\nDPUs can optimize the latency and CPU consumption of disaggregated storage\nservers. However, utilizing DPUs for DBMSs requires careful design of the\nnetwork and storage paths and the interface exposed to the DBMS. To fully\nbenefit from DPUs, DDS heavily uses DMA, zero-copy, and userspace I/O to\nminimize overhead when improving throughput. It also introduces an offload\nengine that eliminates host CPUs by executing client requests directly on the\nDPU. Adopting DDS' API requires minimal DBMS modification. Our experimental\nstudy and production system integration show promising results -- DDS achieves\nhigher disaggregated storage throughput with an order of magnitude lower\nlatency, and saves up to tens of CPU cores per storage server.",
        "authors": "Qizhen Zhang, Philip Bernstein, Badrish Chandramouli, Jason Hu, Yiming Zheng",
        "translated": "这个扩展报告介绍了 DDS，一种新型的分类存储架构，由新兴的网络硬件，即 DPU (数据处理单元)支持。DPU 可以优化分解存储服务器的延迟和 CPU 消耗。然而，将 DPU 用于 DBMS 需要仔细设计网络和存储路径以及公开给 DBMS 的接口。为了充分利用 DPU，DDS 大量使用 DMA、零拷贝和用户空间 I/O，以便在提高吞吐量时最小化开销。它还引入了一个卸载引擎，通过直接在 DPU 上执行客户端请求来消除主机 CPU。采用 DDS 的 API 只需对 DBMS 进行最小限度的修改。我们的实验研究和生产系统集成显示了令人鼓舞的结果—— DDS 实现了更高的分解存储吞吐量和更低的数量级延迟，并为每个存储服务器节省了多达数十个 CPU 核。"
    },
    {
        "title": "Integrated Hardware Architecture and Device Placement Search",
        "url": "http://arxiv.org/abs/2407.13143v1",
        "pub_date": "2024-07-18",
        "summary": "Distributed execution of deep learning training involves a dynamic interplay\nbetween hardware accelerator architecture and device placement strategy. This\nis the first work to explore the co-optimization of determining the optimal\narchitecture and device placement strategy through novel algorithms, improving\nthe balance of computational resources, memory usage, and data distribution.\nOur architecture search leverages tensor and vector units, determining their\nquantity and dimensionality, and on-chip and off-chip memory configurations. It\nalso determines the microbatch size and decides whether to recompute or stash\nactivations, balancing the memory footprint of training and storage size. For\neach explored architecture configuration, we use an Integer Linear Program\n(ILP) to find the optimal schedule for executing operators on the accelerator.\nThe ILP results then integrate with a dynamic programming solution to identify\nthe most effective device placement strategy, combining data, pipeline, and\ntensor model parallelism across multiple accelerators. Our approach achieves\nhigher throughput on large language models compared to the state-of-the-art\nTPUv4 and the Spotlight accelerator search framework. The entire source code of\nPHAZE is available at https://github.com/msr-fiddle/phaze.",
        "authors": "Irene Wang, Jakub Tarnawski, Amar Phanishayee, Divya Mahajan",
        "translated": "深度学习训练的分布式执行涉及硬件加速器体系结构和设备布局策略之间的动态相互作用。这是首次通过新的算法探索确定最优架构和设备布局策略的协同优化，改善计算资源、内存使用和数据分布的平衡。我们的架构搜索利用张量和矢量单元，确定它们的数量和维度，以及片内和片外存储配置。它还确定微批量大小，并决定是否重新计算或隐藏激活，平衡训练和存储大小的内存占用。对于每个探索的体系结构配置，我们使用一个整数线性规划(ILP)来找到在加速器上执行运算符的最佳时间表。然后，ILP 结果与动态编程解决方案相结合，结合数据、流水线和张量模型在多个加速器之间的并行性，确定最有效的设备布局策略。与最先进的 TPUv4和 Spotlight 加速器搜索框架相比，我们的方法在大型语言模型上实现了更高的吞吐量。整个 PHAZE 的源代码可在 https://github.com/msr-fiddle/PHAZE 下载。"
    },
    {
        "title": "Improving GPU Multi-Tenancy Through Dynamic Multi-Instance GPU\n  Reconfiguration",
        "url": "http://arxiv.org/abs/2407.13126v1",
        "pub_date": "2024-07-18",
        "summary": "Continuous learning (CL) has emerged as one of the most popular deep learning\nparadigms deployed in modern cloud GPUs. Specifically, CL has the capability to\ncontinuously update the model parameters (through model retraining) and use the\nupdated model (if available) to serve overtime arriving inference requests. It\nis generally beneficial to co-locate the retraining and inference together to\nenable timely model updates and avoid model transfer overheads. This brings the\nneed for GPU sharing among retraining and inferences. Meanwhile, multiple CL\nworkloads can share the modern GPUs in the cloud, leading to multi-tenancy\nexecution. In this paper, we observe that prior GPU-sharing techniques are not\noptimized for multi-tenancy CL workloads. Specifically, they do not coherently\nconsider the accuracy of the retraining model and the inference service level\nobjective (SLO) attainment. Moreover, they cannot accommodate the overtime\ndynamics (e.g., inference arrival intensity) in CL execution. In this paper, we\npropose MIGRator, a novel GPU reconfiguration runtime that dynamically performs\nGPU reconfiguration for multi-tenancy CL workloads. MIGRator is based on the\nrecent NVIDIA multi-instance GPU (MIG) to mitigate resource contention and\nformulates the reconfiguration optimization into Integer Linear Programming\n(ILP) to dynamically identify, reconfigure, and allocate the GPU instances.\nMIGRator leverages the \"Goodput\" metric in the ILP objective function to\nconsider both inference SLO attainment and model accuracy in the\nreconfiguration exploration. We evaluate MIGRator using representative\nmulti-tenancy CL workloads. The results show our approach outperforms the\nstate-of-the-art GPU sharing techniques (i.e., Ekya, Astraea, and PARIS) by\n17\\%, 21\\%, and 20\\%, respectively.",
        "authors": "Tianyu Wang, Sheng Li, Bingyao Li, Yue Dai, Ao Li, Geng Yuan, Yufei Ding, Youtao Zhang, Xulong Tang",
        "translated": "持续学习(CL)已经成为现代云 GPU 中最流行的深度学习范例之一。具体来说，CL 具有不断更新模型参数的能力(通过模型再训练) ，并使用更新后的模型(如果可用的话)来服务超时到达的推理请求。一般来说，将再培训和推理放在一起是有益的，以便能够及时更新模型，避免模型传输开销。这就需要在再训练和推理之间共享 GPU。同时，多个 CL 工作负载可以在云中共享现代 GPU，从而实现多租户执行。在本文中，我们观察到现有的 GPU 共享技术没有针对多租户 CL 工作负载进行优化。具体而言，他们没有一致地考虑再培训模型的准确性和推理服务水平目标(SLO)的达成。此外，它们不能适应 CL 执行中的超时动态(例如，推断到达强度)。在本文中，我们提出了一种新的 GPU 重构运行时 MIGRator，它可以动态地对多租户 CL 工作负载进行 GPU 重构。MIGrator 基于最新的 NVIDIA 多实例图形处理器(mIG) ，以减少资源争用，并将重新配置优化制定为整数线性规划(ILP) ，以动态识别、重新配置和分配图形处理器实例。MIGRator 利用 ILP 目标函数中的“ Goodput”度量，在重构探索中同时考虑推断 SLO 获得和模型精度。我们使用代表性的多租户 CL 工作负载来评估 MIGRator。结果显示，我们的方法比最先进的图形处理器共享技术(即 Ekya、 Astraea 和 PARIS)分别高出17% 、21% 和20% 。"
    },
    {
        "title": "Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint\n  Phase-shift Optimization and Multi-User Power Allocation",
        "url": "http://arxiv.org/abs/2407.13123v1",
        "pub_date": "2024-07-18",
        "summary": "Vehicular edge computing (VEC) is an emerging technology with significant\npotential in the field of internet of vehicles (IoV), enabling vehicles to\nperform intensive computational tasks locally or offload them to nearby edge\ndevices. However, the quality of communication links may be severely\ndeteriorated due to obstacles such as buildings, impeding the offloading\nprocess. To address this challenge, we introduce the use of Reconfigurable\nIntelligent Surfaces (RIS), which provide alternative communication pathways to\nassist vehicular communication. By dynamically adjusting the phase-shift of the\nRIS, the performance of VEC systems can be substantially improved. In this\nwork, we consider a RIS-assisted VEC system, and design an optimal scheme for\nlocal execution power, offloading power, and RIS phase-shift, where random task\narrivals and channel variations are taken into account. To address the scheme,\nwe propose an innovative deep reinforcement learning (DRL) framework that\ncombines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing\nRIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy\nGradient (MADDPG) algorithm for optimizing the power allocation of vehicle user\n(VU). Simulation results show that our proposed scheme outperforms the\ntraditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient\n(TD3) and some typical stochastic schemes.",
        "authors": "Kangwei Qi, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Khaled B. Letaief",
        "translated": "车辆边缘计算(VEC)是车辆互联网(IoV)领域中一项具有巨大潜力的新兴技术，它使车辆能够在本地执行密集的计算任务，或者将计算任务卸载到附近的边缘设备上。然而，由于建筑物等障碍物的存在，通信链路的质量可能会严重恶化，阻碍卸载过程。为了应对这一挑战，我们介绍了可重构智能表面(RIS)的使用，它提供了可替代的通信路径来协助车辆通信。通过动态调整 RIS 的相移，VEC 系统的性能可以得到很大的改善。在本文中，我们考虑了一个 RIS 辅助的 VEC 系统，并设计了一个本地执行能力、卸载能力和 RIS 相移的最佳方案，其中考虑了随机任务到达和信道变化。为了解决这个问题，我们提出了一个创新的深度强化学习(DRL)框架，它结合了深度确定性策略梯度(DDPG)算法来优化 RIS 移相系数和多 Agent 深度确定性策略梯度(MADdPG)算法来优化车辆用户(VU)的功率分配。仿真结果表明，该方案优于传统的集中式 DDPG、双延迟深度确定性策略梯度(TD3)和一些典型的随机方案。"
    },
    {
        "title": "Scheduling Deep Learning Jobs in Multi-Tenant GPU Clusters via Wise\n  Resource Sharing",
        "url": "http://arxiv.org/abs/2407.13088v1",
        "pub_date": "2024-07-18",
        "summary": "Deep learning (DL) has demonstrated significant success across diverse\nfields, leading to the construction of dedicated GPU accelerators within GPU\nclusters for high-quality training services. Efficient scheduler designs for\nsuch clusters are vital to reduce operational costs and enhance resource\nutilization. While recent schedulers have shown impressive performance in\noptimizing DL job performance and cluster utilization through periodic\nreallocation or selection of GPU resources, they also encounter challenges such\nas preemption and migration overhead, along with potential DL accuracy\ndegradation. Nonetheless, few explore the potential benefits of GPU sharing to\nimprove resource utilization and reduce job queuing times. Motivated by these\ninsights, we present a job scheduling model allowing multiple jobs to share the\nsame set of GPUs without altering job training settings. We introduce SJF-BSBF\n(shortest job first with best sharing benefit first), a straightforward yet\neffective heuristic scheduling algorithm. SJF-BSBF intelligently selects job\npairs for GPU resource sharing and runtime settings (sub-batch size and\nscheduling time point) to optimize overall performance while ensuring DL\nconvergence accuracy through gradient accumulation. In experiments with both\nphysical DL workloads and trace-driven simulations, even as a preemption-free\npolicy, SJF-BSBF reduces the average job completion time by 27-33\\% relative to\nthe state-of-the-art preemptive DL schedulers. Moreover, SJF-BSBF can wisely\ndetermine the optimal resource sharing settings, such as the sharing time point\nand sub-batch size for gradient accumulation, outperforming the aggressive GPU\nsharing approach (baseline SJF-FFS policy) by up to 17\\% in large-scale traces.",
        "authors": "Yizhou Luo, Qiang Wang, Shaohuai Shi, Jiaxin Lai, Shuhan Qi, Jiajia Zhang, Xuan Wang",
        "translated": "深度学习(DL)在不同领域取得了显著的成功，导致在 GPU 集群内建立专用的 GPU 加速器以提供高质量的培训服务。为此类集群设计高效的调度器对于降低运营成本和提高资源利用率至关重要。尽管最近的调度器通过定期重新分配或选择 GPU 资源在优化 DL 作业性能和集群利用率方面表现出了令人印象深刻的性能，但它们也遇到了诸如抢占和迁移开销等挑战，以及潜在的 DL 准确性下降。尽管如此，很少有人探索 GPU 共享在提高资源利用率和减少作业排队时间方面的潜在好处。受到这些见解的启发，我们提出了一个作业调度模型，允许多个作业共享同一组 GPU，而不改变作业培训设置。本文介绍了一种简单有效的启发式调度算法 SJF-BSBF (最短作业优先，利益最大共享优先)。SJF-BSBF 智能地为 GPU 资源共享和运行时设置(子批量和调度时间点)选择作业对，以优化整体性能，同时通过梯度积累确保 DL 收敛的准确性。在物理 DL 工作负载和跟踪驱动模拟的实验中，SJF-BSBF 甚至作为一种无抢占策略，相对于最先进的抢占式 DL 调度器，它将平均作业完成时间减少了27-33% 。此外，SJF-BSBF 可以明智地确定最佳资源共享设置，如梯度累积的共享时间点和子批量大小，在大规模跟踪中优于积极的 GPU 共享方法(基线 SJF-FFS 策略)高达17% 。"
    },
    {
        "title": "Proof-of-Collaborative-Learning: A Multi-winner Federated Learning\n  Consensus Algorithm",
        "url": "http://arxiv.org/abs/2407.13018v1",
        "pub_date": "2024-07-17",
        "summary": "Regardless of their variations, blockchains require a consensus mechanism to\nvalidate transactions, supervise added blocks, maintain network security,\nsynchronize the network state, and distribute incentives. Proof-of-Work (PoW),\none of the most influential implementations of consensus mechanisms, consumes\nan extraordinary amount of energy for a task that lacks direct productive\noutput. In this paper, we propose Proof-of-Collaborative-Learning (PoCL), a\nmulti-winner federated learning validated consensus mechanism that redirects\nthe computation power of blockchains to train federated learning models. In\naddition, we present a novel evaluation mechanism to ensure the efficiency of\nthe locally trained models of miners. We evaluated the security of our\nevaluation mechanism by introducing and conducting probable attacks. Moreover,\nwe present a novel reward distribution mechanism to incentivize winning miners\nfairly, and demonstrate that our reward system is fair both within and across\nall rounds.",
        "authors": "Amirreza Sokhankhosh, Sara Rouhani",
        "translated": "不管它们的变化如何，区块链都需要一个共识机制来验证事务、监督添加的区块、维护网络安全、同步网络状态和分配激励。工作证明(Proof-of-Work，PoW)是最有影响力的共识机制实现之一，它为一项缺乏直接生产性输出的任务消耗了大量的能量。本文提出了一种多赢家联邦学习验证共识机制——协作学习证明(Proof-of-Collaborative-Learning，PoCL) ，该机制将块链的计算能力重定向到训练联邦学习模型上。此外，我们提出了一个新的评估机制，以确保效率的局部训练的矿工模型。我们通过引入和实施可能的攻击来评估我们的评估机制的安全性。此外，我们提出了一个新颖的奖励分配机制，以公平地激励获胜矿工，并证明我们的奖励制度是公平的内部和跨所有轮次。"
    },
    {
        "title": "Automated Gateways: A Smart Contract-Powered Solution for\n  Interoperability Across Blockchains",
        "url": "http://arxiv.org/abs/2407.13001v1",
        "pub_date": "2024-07-17",
        "summary": "Interoperability is a significant challenge in blockchain technology,\nhindering seamless data and service sharing across diverse blockchain networks.\nThis study introduces \\textit {Automated Gateways} as a novel framework\nleveraging smart contracts to facilitate interoperability. Unlike existing\nsolutions, which often require adopting new technologies or relying on external\nservices, Automated Gateways framework is integrated directly with a\nblockchain's core infrastructure to enhance systems with built-in\ninteroperability features. By implementing fine-grained access control\nmechanisms, smart contracts within this framework manage accessibility and\nauthorization for cross-chain interactions and facilitate streamlining the\nselective sharing of services between blockchains. Our evaluation demonstrates\nthe framework's capability to handle cross-chain interactions efficiently,\nsignificantly reduce operational complexities, and uphold transactional\nintegrity and security across different blockchain networks. With its focus on\nuser-friendliness, self-managed permissions, and independence from external\nplatforms, this framework is designed to achieve broader adoption within the\nblockchain community.",
        "authors": "Koosha Esmaeilzadeh Khorasani, Sara Rouhani, Rui Pan, Vahid Pourheidari",
        "translated": "互操作性是区块链技术的一个重大挑战，阻碍了不同区块链网络之间的无缝数据和服务共享。本研究引入 textit { Automated Gateways }作为一个利用智能契约促进互操作性的新框架。与通常需要采用新技术或依赖外部服务的现有解决方案不同，Automated Gateways 框架直接与区块链的核心基础设施集成，以增强具有内置互操作性功能的系统。通过实施细粒度的访问控制机制，该框架内的智能合同管理跨链交互的可访问性和授权，并促进区块链之间有选择地共享服务。我们的评估展示了该框架有效处理跨链交互的能力，显著降低操作复杂性，并在不同的区块链网络中维护事务完整性和安全性。该框架侧重于用户友好性、自我管理权限和独立于外部平台，旨在实现区块链社区内更广泛的采用。"
    },
    {
        "title": "A Framework for testing Federated Learning algorithms using an edge-like\n  environment",
        "url": "http://arxiv.org/abs/2407.12980v1",
        "pub_date": "2024-07-17",
        "summary": "Federated Learning (FL) is a machine learning paradigm in which many clients\ncooperatively train a single centralized model while keeping their data private\nand decentralized. FL is commonly used in edge computing, which involves\nplacing computer workloads (both hardware and software) as close as possible to\nthe edge, where the data is being created and where actions are occurring,\nenabling faster response times, greater data privacy, and reduced data transfer\ncosts. However, due to the heterogeneous data distributions/contents of\nclients, it is non-trivial to accurately evaluate the contributions of local\nmodels in global centralized model aggregation. This is an example of a major\nchallenge in FL, commonly known as data imbalance or class imbalance. In\ngeneral, testing and assessing FL algorithms can be a very difficult and\ncomplex task due to the distributed nature of the systems. In this work, a\nframework is proposed and implemented to assess FL algorithms in a more easy\nand scalable way. This framework is evaluated over a distributed edge-like\nenvironment managed by a container orchestration platform (i.e. Kubernetes).",
        "authors": "Felipe Machado Schwanck, Marcos Tomazzoli Leipnitz, Joel Luís Carbonera, Juliano Araujo Wickboldt",
        "translated": "联邦学习(FL)是一种机器学习范式，其中许多客户协同训练一个单一的集中模型，同时保持其数据的私有性和分散性。FL 通常用于边缘计算，包括将计算机工作负载(包括硬件和软件)尽可能靠近边缘，数据在哪里创建，动作在哪里发生，从而实现更快的响应时间，更大的数据隐私，以及降低数据传输成本。然而，由于客户端的数据分布/内容是异构的，准确评估局部模型在全局集中模型聚合中的贡献是非常重要的。这是 FL 中一个主要挑战的例子，通常被称为数据不平衡或类不平衡。一般来说，由于系统的分布式特性，测试和评估 FL 算法可能是一项非常困难和复杂的任务。在这项工作中，提出并实现了一个框架，以更容易和可扩展的方式评估 FL 算法。这个框架是在一个由容器编排平台(即 Kubernetes)管理的分布式边缘环境上进行评估的。"
    },
    {
        "title": "Mixture of Experts with Mixture of Precisions for Tuning Quality of\n  Service",
        "url": "http://arxiv.org/abs/2407.14417v1",
        "pub_date": "2024-07-19",
        "summary": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 2.62 to 2.80, 6.48 to 7.24, and 3.24 to 3.53\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important.",
        "authors": "HamidReza Imani, Abdolah Amirany, Tarek El-Ghazawi",
        "translated": "在资源受限的环境中部署大型专家混合(MoE)模型的需求日益增长，这就需要有效的方法来解决其高内存和计算需求的挑战。此外，考虑到任务具有不同的用户定义约束，而且在多租户环境中可用资源随着时间的推移而变化，因此有必要设计一种提供灵活位形空间的方法。本文利用专家的部分量化，提出了一种适应性服务方法，以有效地部署模型。通过动态确定量化专家的数量及其在 CPU 和 GPU 之间的分布，我们的方法探索了 Pareto 前沿，并为调优吞吐量和模型质量提供了细粒度范围的配置。我们对 NVIDIA A100图形处理器的评估使用了 Mistral 8x7B MoE 模型，用于三种语言建模基准，表明令牌生成的吞吐量可以从每秒0.63到13.00令牌进行调整。在最大量化下，WikiText2、 PTB 和 C4数据集的边际困惑分别增加了2.62到2.80、6.48到7.24和3.24到3.53。这些结果突出了我们的方法在动态和精度敏感的应用程序中的实际适用性，在这些应用程序中，内存使用和输出质量都很重要。"
    },
    {
        "title": "The Vision of Autonomic Computing: Can LLMs Make It a Reality?",
        "url": "http://arxiv.org/abs/2407.14402v1",
        "pub_date": "2024-07-19",
        "summary": "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM.",
        "authors": "Zhiyang Zhang, Fangkai Yang, Xiaoting Qin, Jue Zhang, Qingwei Lin, Gong Cheng, Dongmei Zhang, Saravan Rajmohan, Qi Zhang",
        "translated": "二十多年前提出的“自主计算远景”(Vision of Autonomic Computing，ACV)设想计算系统可以像生物有机体一样自我管理，无缝地适应不断变化的环境。尽管经过几十年的研究，由于现代计算系统的动态性和复杂性，实现 ACV 仍然具有挑战性。大型语言模型(LLM)的最新进展通过利用其广泛的知识、语言理解和任务自动化能力，为这些挑战提供了有希望的解决方案。本文探讨了通过一个基于 LLM 的微服务管理多代理框架实现 ACV 的可行性。我们为自主服务维护引入了一个五级分类法，并提出了一个基于 Sock Shop 微服务演示项目的在线评估基准，以评估我们的框架的性能。我们的研究结果表明，在实现3级自治方面取得了重大进展，突出了 LLM 在检测和解决微服务体系结构中的问题方面的有效性。这项研究有助于推进自主计算的开创性整合 LLM 到微服务管理框架，为更多的自适应和自我管理的计算系统铺平道路。代码将在 https://aka.ms/acv-llm 公布。"
    },
    {
        "title": "On the Impact of PRB Load Uncertainty Forecasting for Sustainable Open\n  RAN",
        "url": "http://arxiv.org/abs/2407.14400v1",
        "pub_date": "2024-07-19",
        "summary": "The transition to sustainable Open Radio Access Network (O-RAN) architectures\nbrings new challenges for resource management, especially in predicting the\nutilization of Physical Resource Block (PRB)s. In this paper, we propose a\nnovel approach to characterize the PRB load using probabilistic forecasting\ntechniques. First, we provide background information on the O-RAN architecture\nand components and emphasize the importance of energy/power consumption models\nfor sustainable implementations. The problem statement highlights the need for\naccurate PRB load prediction to optimize resource allocation and power\nefficiency. We then investigate probabilistic forecasting techniques, including\nSimple-Feed-Forward (SFF), DeepAR, and Transformers, and discuss their\nlikelihood model assumptions. The simulation results show that DeepAR\nestimators predict the PRBs with less uncertainty and effectively capture the\ntemporal dependencies in the dataset compared to SFF- and Transformer-based\nmodels, leading to power savings. Different percentile selections can also\nincrease power savings, but at the cost of over-/under provisioning. At the\nsame time, the performance of the Long-Short Term Memory (LSTM) is shown to be\ninferior to the probabilistic estimators with respect to all error metrics.\nFinally, we outline the importance of probabilistic, prediction-based\ncharacterization for sustainable O-RAN implementations and highlight avenues\nfor future research.",
        "authors": "Vaishnavi Kasuluru, Luis Blanco, Cristian J. Vaca-Rubio, Engin Zeydan",
        "translated": "可持续的开放无线接入网(O-RAN)体系结构的转换给资源管理带来了新的挑战，尤其是在预测物理资源块(PRB)的利用率方面。在本文中，我们提出了一种新的方法来表征 PRB 负荷使用概率预测技术。首先，我们提供关于 O-RAN 架构和组件的背景信息，并强调能源/电力消耗模型对于可持续实施的重要性。问题陈述强调了准确的 PRB 负荷预测以优化资源分配和电力效率的必要性。然后，我们研究概率预测技术，包括简单前馈(SFF) ，DeepAR 和变压器，并讨论他们的似然模型假设。仿真结果表明，DeepAR 估计器能够以较小的不确定性预测 PRBs，并且能够有效地捕获数据集中的时间依赖关系，从而节省功耗。不同百分比的选择也可以增加功耗节省，但代价是供应过剩/不足。同时，对于所有的误差指标，长短期存储器(LSTM)的性能都不如概率估计器。最后，我们概述了基于概率的预测角色塑造对于可持续的 O-rAN 实现的重要性，并强调了未来研究的途径。"
    },
    {
        "title": "Enhancing Cloud-Native Resource Allocation with Probabilistic\n  Forecasting Techniques in O-RAN",
        "url": "http://arxiv.org/abs/2407.14377v1",
        "pub_date": "2024-07-19",
        "summary": "The need for intelligent and efficient resource provisioning for the\nproductive management of resources in real-world scenarios is growing with the\nevolution of telecommunications towards the 6G era. Technologies such as Open\nRadio Access Network (O-RAN) can help to build interoperable solutions for the\nmanagement of complex systems. Probabilistic forecasting, in contrast to\ndeterministic single-point estimators, can offer a different approach to\nresource allocation by quantifying the uncertainty of the generated\npredictions. This paper examines the cloud-native aspects of O-RAN together\nwith the radio App (rApp) deployment options. The integration of probabilistic\nforecasting techniques as a rApp in O-RAN is also emphasized, along with case\nstudies of real-world applications. Through a comparative analysis of\nforecasting models using the error metric, we show the advantages of Deep\nAutoregressive Recurrent network (DeepAR) over other deterministic\nprobabilistic estimators. Furthermore, the simplicity of Simple-Feed-Forward\n(SFF) leads to a fast runtime but does not capture the temporal dependencies of\nthe input data. Finally, we present some aspects related to the practical\napplicability of cloud-native O-RAN with probabilistic forecasting.",
        "authors": "Vaishnavi Kasuluru, Luis Blanco, Engin Zeydan, Albert Bel, Angelos Antonopoulos",
        "translated": "随着电信向6G 时代的演变，在现实世界的情况下，为生产性管理资源提供智能和高效的资源的需求正在增长。开放无线接入网(O-RAN)等技术可以帮助建立复杂系统管理的互操作解决方案。概率预测与确定性单点估计相比，可以通过量化所生成的预测的不确定性来提供一种不同的资源分配方法。本文研究了 O-RAN 的云本地方面以及无线应用程序(rApp)部署选项。还强调了将概率预测技术作为一个 rApp 集成到 O-RAN 中，以及实际应用的案例研究。通过对使用误差度量的预测模型的比较分析，我们显示了深度自回归回归网络(DeepAR)相对于其他确定性概率估计器的优势。此外，Simple-Feed-Forward (SFF)的简单性导致运行速度很快，但是不能捕获输入数据的时间依赖关系。最后，我们提出了一些与云本地 O-RAN 与概率预测的实际适用性有关的问题。"
    },
    {
        "title": "On the use of Probabilistic Forecasting for Network Analysis in Open RAN",
        "url": "http://arxiv.org/abs/2407.14375v1",
        "pub_date": "2024-07-19",
        "summary": "Unlike other single-point Artificial Intelligence (AI)-based prediction\ntechniques, such as Long-Short Term Memory (LSTM), probabilistic forecasting\ntechniques (e.g., DeepAR and Transformer) provide a range of possible outcomes\nand associated probabilities that enable decision makers to make more informed\nand robust decisions. At the same time, the architecture of Open RAN has\nemerged as a revolutionary approach for mobile networks, aiming at openness,\ninteroperability and innovation in the ecosystem of RAN. In this paper, we\npropose the use of probabilistic forecasting techniques as a radio App (rApp)\nwithin the Open RAN architecture. We investigate and compare different\nprobabilistic and single-point forecasting methods and algorithms to estimate\nthe utilization and resource demands of Physical Resource Blocks (PRBs) of\ncellular base stations. Through our evaluations, we demonstrate the numerical\nadvantages of probabilistic forecasting techniques over traditional\nsingle-point forecasting methods and show that they are capable of providing\nmore accurate and reliable estimates. In particular, DeepAR clearly outperforms\nsingle-point forecasting techniques such as LSTM and Seasonal-Naive (SN)\nbaselines and other probabilistic forecasting techniques such as\nSimple-Feed-Forward (SFF) and Transformer neural networks.",
        "authors": "Vaishnavi Kasuluru, Luis Blanco, Engin Zeydan",
        "translated": "与其他基于单点人工智能(AI)的预测技术(如长短期记忆(LSTM))不同，概率预测技术(如 DeepAR 和 former)提供了一系列可能的结果和相关的概率，使决策者能够做出更明智和稳健的决策。与此同时，开放式 RAN 体系结构作为移动网络的一种革命性方式，旨在实现 RAN 生态系统的开放性、互操作性和创新性。在本文中，我们提出使用概率预测技术作为一个无线应用程序(rApp)内的开放 RAN 架构。我们调查和比较不同的概率和单点预测方法和算法来估计物理资源块(PRBs)蜂窝基站的利用率和资源需求。通过我们的评估，我们证明了概率预测技术相对于传统的单点预测方法的数值优势，并表明它们能够提供更准确和可靠的估计。特别是，DeepAR 明显优于单点预测技术，如 LSTM 和季节天真(SN)基线和其他概率预测技术，如简单前馈(SFF)和变压器神经网络。"
    },
    {
        "title": "FaaS Is Not Enough: Serverless Handling of Burst-Parallel Jobs",
        "url": "http://arxiv.org/abs/2407.14331v1",
        "pub_date": "2024-07-19",
        "summary": "Function-as-a-Service (FaaS) struggles with burst-parallel jobs due to\nneeding multiple independent invocations to start a job. The lack of a group\ninvocation primitive complicates application development and overlooks crucial\naspects like locality and worker communication.\n  We introduce a new serverless solution designed specifically for\nburst-parallel jobs. Unlike FaaS, our solution ensures job-level isolation\nusing a group invocation primitive, allowing large groups of workers to be\nlaunched simultaneously. This method optimizes resource allocation by\nconsolidating workers into fewer containers, speeding up their initialization\nand enhancing locality. Enhanced locality drastically reduces remote\ncommunication compared to FaaS, and combined with simultaneity, it enables\nworkers to communicate synchronously via message passing and group collectives.\nThis makes applications that are impractical with FaaS feasible. We implemented\nour solution on OpenWhisk, providing a communication middleware that\nefficiently uses locality with zero-copy messaging. Evaluations show that it\nreduces job invocation and communication latency, resulting in a 2$\\times$\nspeed-up for TeraSort and a 98.5% reduction in remote communication for\nPageRank (13$\\times$ speed-up) compared to traditional FaaS.",
        "authors": "Daniel Barcelona-Pons, Aitor Arjona, Pedro García-López, Enrique Molina-Giménez, Stepan Klymonchuk",
        "translated": "由于启动一个作业需要多个独立调用，函数即服务(function-as-a-Service，FaaS)难以处理突发并行作业。缺少组调用原语使应用程序开发复杂化，并且忽略了关键的方面，如局部性和工作者通信。本文介绍了一种专门针对突发并行作业的无服务器解决方案。与 FaaS 不同，我们的解决方案使用组调用原语确保作业级隔离，允许同时启动大量工作者。该方法通过将工作者合并到较少的容器中，加快其初始化速度和增强局部性来优化资源分配。与 FaaS 相比，增强的局部性极大地减少了远程通信，并与同时性相结合，使工作者能够通过消息传递和组集合同步通信。这使得 FaaS 不实用的应用程序变得可行。我们在 OpenWhisk 上实现了我们的解决方案，提供了一个通信中间件，有效地使用了零拷贝消息传递的本地化。评估表明，它减少了作业调用和通信延迟，结果是 TeraSort 的速度提高了2美元，PageRank 的远程通信比传统的 FaaS 减少了98.5% (13美元)。"
    },
    {
        "title": "Theoretical Analysis on Block Time Distributions in Byzantine\n  Fault-Tolerant Consensus Blockchains",
        "url": "http://arxiv.org/abs/2407.14299v1",
        "pub_date": "2024-07-19",
        "summary": "Some blockchain networks employ a distributed consensus algorithm featuring\nByzantine fault tolerance. Notably, certain public chains, such as Cosmos and\nTezos, which operate on a proof-of-stake mechanism, have adopted this\nalgorithm. While it is commonly assumed that these blockchains maintain a\nnearly constant block creation time, empirical analysis reveals fluctuations in\nthis interval; this phenomenon has received limited attention. In this paper,\nwe propose a mathematical model to account for the processes of block\npropagation and validation within Byzantine fault-tolerant consensus\nblockchains, aiming to theoretically analyze the probability distribution of\nblock time. First, we propose stochastic processes governing the broadcasting\ncommunications among validator nodes. Consequently, we theoretically\ndemonstrate that the probability distribution of broadcast time among validator\nnodes adheres to the Gumbel distribution. This finding indicates that the\ndistribution of block time typically arises from convolving multiple Gumbel\ndistributions. Additionally, we derive an approximate formula for the block\ntime distribution suitable for data analysis purposes. By fitting this\napproximation to real-world block time data, we demonstrate the consistent\nestimation of block time distribution parameters.",
        "authors": "Akihiro Fujihara",
        "translated": "一些区块链网络采用分布式共识算法，以拜占庭将军问题为特征。值得注意的是，某些公共连锁企业，如 Cosmos 和 Tezos，采用了一种证明股权的机制，已经采用了这种算法。虽然通常假设这些区块链保持一个几乎恒定的块创建时间，经验分析显示在这个时间间隔波动; 这种现象已经得到有限的注意。在这篇文章中，我们提出了一个数学模型来解释拜占庭容错共有块链中的块传播和验证过程，旨在从理论上分析块时间的概率分布。首先，我们提出了控制验证节点间广播通信的随机过程。因此，我们在理论上证明了验证器节点之间的广播时间概率分布符合 Gumbel 分布。这一发现表明，块时间分布通常起源于卷积多个 Gumbel 分布。此外，我们还推导出适合于数据分析目的的块时间分布的近似公式。通过对实际块时间数据的拟合，证明了块时间分布参数的一致性估计。"
    },
    {
        "title": "On the Complexity of Reachability Properties in Serverless Function\n  Scheduling",
        "url": "http://arxiv.org/abs/2407.14159v1",
        "pub_date": "2024-07-19",
        "summary": "Functions-as-a-Service (FaaS) is a Serverless Cloud paradigm where a platform\nmanages the execution scheduling (e.g., resource allocation, runtime\nenvironments) of stateless functions. Recent developments demonstrate the\nbenefits of using domain-specific languages to express per-function scheduling\npolicies, e.g., enforcing the allocation of functions on nodes that enjoy low\ndata-access latencies thanks to proximity and connection pooling. We present\naAPP, an affinity-aware extension of a platform-agnostic function scheduling\nlanguage. We formalise its scheduling semantics and then study the complexity\nof statically checking reachability properties, e.g., useful to verify that\ntrusted and untrusted functions cannot be co-located. Analysing different\nfragments of aAPP, we show that checking reachability of policies without\naffinity has linear complexity, while affinity makes the problem PSpace.",
        "authors": "Giuseppe De Palma, Saverio Giallorenzo, Jacopo Mauro, Matteo Trentin, Gianluigi Zavattaro",
        "translated": "函数即服务(function-as-a-Service，FaaS)是一种无服务云模式，其中平台管理无状态函数的执行调度(例如，资源分配，运行时环境)。最近的发展表明，使用领域特定语言来表达每个功能的调度政策的好处，例如，由于邻近性和连接池，强制在数据访问延迟较低的节点上分配功能。我们提出了一个具有亲和力感知的平台无关功能调度语言的扩展 aAPP。我们将其调度语义形式化，然后研究静态检查可达性属性的复杂性，例如，有助于验证可信和不可信函数不能同时定位。通过分析 aAPP 的不同片段，我们发现无关联策略的检查可达性具有线性复杂性，而关联性使问题 PSpace。"
    },
    {
        "title": "Where is the Testbed for my Federated Learning Research?",
        "url": "http://arxiv.org/abs/2407.14154v1",
        "pub_date": "2024-07-19",
        "summary": "Progressing beyond centralized AI is of paramount importance, yet,\ndistributed AI solutions, in particular various federated learning (FL)\nalgorithms, are often not comprehensively assessed, which prevents the research\ncommunity from identifying the most promising approaches and practitioners from\nbeing convinced that a certain solution is deployment-ready. The largest hurdle\ntowards FL algorithm evaluation is the difficulty of conducting real-world\nexperiments over a variety of FL client devices and different platforms, with\ndifferent datasets and data distribution, all while assessing various\ndimensions of algorithm performance, such as inference accuracy, energy\nconsumption, and time to convergence, to name a few. In this paper, we present\nCoLExT, a real-world testbed for FL research. CoLExT is designed to streamline\nexperimentation with custom FL algorithms in a rich testbed configuration\nspace, with a large number of heterogeneous edge devices, ranging from\nsingle-board computers to smartphones, and provides real-time collection and\nvisualization of a variety of metrics through automatic instrumentation.\nAccording to our evaluation, porting FL algorithms to CoLExT requires minimal\ninvolvement from the developer, and the instrumentation introduces minimal\nresource usage overhead. Furthermore, through an initial investigation\ninvolving popular FL algorithms running on CoLExT, we reveal previously unknown\ntrade-offs, inefficiencies, and programming bugs.",
        "authors": "Janez Božič, Amândio R. Faustino, Boris Radovič, Marco Canini, Veljko Pejović",
        "translated": "超越集中式人工智能的发展是至关重要的，然而，分布式人工智能解决方案，特别是各种联邦学习(FL)算法，往往没有得到全面评估，这阻碍了研究界确定最有希望的方法和从业人员相信某个解决方案已经部署就绪。FL 算法评估的最大障碍是难以在各种 FL 客户端设备和不同平台上进行真实世界的实验，具有不同的数据集和数据分布，同时评估算法性能的各个维度，如推断精度，能源消耗和收敛时间等。在本文中，我们介绍了 CoLExT，一个真实世界的外语研究试验平台。CoLexT 的设计目的是在一个丰富的测试位形空间中简化自定义 FL 算法的实验，使用大量不同的边缘设备，从单板计算机到智能手机，并通过自动仪器提供各种指标的实时收集和可视化。根据我们的评估，将 FL 算法移植到 CoLExT 需要开发人员的最小参与，并且检测引入了最小的资源使用开销。此外，通过对运行在 CoLExT 上的流行 FL 算法的初步调查，我们揭示了以前未知的权衡、效率低下和编程错误。"
    },
    {
        "title": "TorchGT: A Holistic System for Large-scale Graph Transformer Training",
        "url": "http://arxiv.org/abs/2407.14106v1",
        "pub_date": "2024-07-19",
        "summary": "Graph Transformer is a new architecture that surpasses GNNs in graph\nlearning. While there emerge inspiring algorithm advancements, their practical\nadoption is still limited, particularly on real-world graphs involving up to\nmillions of nodes. We observe existing graph transformers fail on large-scale\ngraphs mainly due to heavy computation, limited scalability and inferior model\nquality. Motivated by these observations, we propose TorchGT, the first\nefficient, scalable, and accurate graph transformer training system. TorchGT\noptimizes training at different levels. At algorithm level, by harnessing the\ngraph sparsity, TorchGT introduces a Dual-interleaved Attention which is\ncomputation-efficient and accuracy-maintained. At runtime level, TorchGT scales\ntraining across workers with a communication-light Cluster-aware Graph\nParallelism. At kernel level, an Elastic Computation Reformation further\noptimizes the computation by reducing memory access latency in a dynamic way.\nExtensive experiments demonstrate that TorchGT boosts training by up to 62.7x\nand supports graph sequence lengths of up to 1M.",
        "authors": "Meng Zhang, Jie Sun, Qinghao Hu, Peng Sun, Zeke Wang, Yonggang Wen, Tianwei Zhang",
        "translated": "图形变换器是一种在图形学习方面超越 GNN 的新体系结构。虽然出现了令人鼓舞的算法进步，但它们的实际应用仍然有限，特别是在涉及数百万个节点的现实世界图表上。我们观察到现有的图变换器在大规模图上的失效主要是由于计算量大、可扩展性有限和模型质量差。基于这些观察，我们提出了 TorchGT，这是第一个高效、可扩展和准确的图形变换器培训系统。TorchGT 优化了不同层次的培训。在算法层面，TorchGT 通过利用图的稀疏性，引入了一种计算效率高、保持精度的双交错注意。在运行时级别，TorchGT 使用通信光集群感知的图形并行性(GraphParallelism)来跨工人进行培训。在内核级，弹性计算改革通过动态地减少内存访问延迟进一步优化了计算。大量的实验表明，TorchGT 可以将训练提高62.7倍，并支持图序列长度达到1M。"
    },
    {
        "title": "A simple and fast C++ thread pool implementation capable of running task\n  graphs",
        "url": "http://arxiv.org/abs/2407.15805v1",
        "pub_date": "2024-07-22",
        "summary": "In this paper, the author presents a simple and fast C++ thread pool\nimplementation capable of running task graphs. The implementation is publicly\navailable on GitHub, see https://github.com/dpuyda/scheduling.",
        "authors": "Dmytro Puyda",
        "translated": "在本文中，作者给出了一个简单快速的能够运行任务图的 C + + 线程池实现。该实现可以在 GitHub 上公开获得，见 https://GitHub.com/dpuyda/scheduling。"
    },
    {
        "title": "Parallel Split Learning with Global Sampling",
        "url": "http://arxiv.org/abs/2407.15738v1",
        "pub_date": "2024-07-22",
        "summary": "The expansion of IoT devices and the demands of Deep Learning have\nhighlighted significant challenges in Distributed Deep Learning (DDL) systems.\nParallel Split Learning (PSL) has emerged as a promising derivative of Split\nLearning that is well suited for distributed learning on resource-constrained\ndevices. However, PSL faces several obstacles, such as large effective batch\nsizes, non-IID data distributions, and the straggler effect. We view these\nissues as a sampling dilemma and propose to address them by orchestrating the\nmini-batch sampling process on the server side. We introduce the Uniform Global\nSampling (UGS) method to decouple the effective batch size from the number of\nclients and reduce mini-batch deviation in non-IID settings. To address the\nstraggler effect, we introduce the Latent Dirichlet Sampling (LDS) method,\nwhich generalizes UGS to balance the trade-off between batch deviation and\ntraining time. Our simulations reveal that our proposed methods enhance model\naccuracy by up to 34.1% in non-IID settings and reduce the training time in the\npresence of stragglers by up to 62%. In particular, LDS effectively mitigates\nthe straggler effect without compromising model accuracy or adding significant\ncomputational overhead compared to UGS. Our results demonstrate the potential\nof our methods as a promising solution for DDL in real applications.",
        "authors": "Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink",
        "translated": "物联网设备的扩展和深度学习的需求凸显了分布式深度学习(DDL)系统面临的重大挑战。并行分裂学习(PSL)已经成为分裂学习的一个有前途的衍生物，非常适合在资源受限的设备上进行分布式学习。然而，PSL 面临着一些障碍，如大的有效批量、非 IID 数据分布和滞后效应。我们将这些问题视为抽样困境，并建议通过在服务器端协调小批量抽样过程来解决这些问题。我们引入统一全局抽样(UGS)方法解耦的有效批量大小从客户的数量和减少小批量偏差的非 IID 设置。为了解决掉队效应，我们引入了潜在 Dirichlet 抽样(LDS)方法，该方法通过 UGS 来平衡批量偏差和训练时间之间的权衡。我们的仿真结果表明，我们提出的方法在非 IID 环境下提高了34.1% 的模型准确性，并且在有掉队的情况下减少了62% 的训练时间。特别是，与 UGS 相比，LDS 在不影响模型精度或增加大量计算开销的情况下，有效地减轻了滞后效应。我们的结果表明，我们的方法作为一个有前途的解决方案的 DDL 在实际应用中的潜力。"
    },
    {
        "title": "CrashEventLLM: Predicting System Crashes with Large Language Models",
        "url": "http://arxiv.org/abs/2407.15716v1",
        "pub_date": "2024-07-22",
        "summary": "As the dependence on computer systems expands across various domains,\nfocusing on personal, industrial, and large-scale applications, there arises a\ncompelling need to enhance their reliability to sustain business operations\nseamlessly and ensure optimal user satisfaction. System logs generated by these\ndevices serve as valuable repositories of historical trends and past failures.\nThe use of machine learning techniques for failure prediction has become\ncommonplace, enabling the extraction of insights from past data to anticipate\nfuture behavior patterns. Recently, large language models have demonstrated\nremarkable capabilities in tasks including summarization, reasoning, and event\nprediction. Therefore, in this paper, we endeavor to investigate the potential\nof large language models in predicting system failures, leveraging insights\nlearned from past failure behavior to inform reasoning and decision-making\nprocesses effectively. Our approach involves leveraging data from the Intel\nComputing Improvement Program (ICIP) system crash logs to identify significant\nevents and develop CrashEventLLM. This model, built upon a large language model\nframework, serves as our foundation for crash event prediction. Specifically,\nour model utilizes historical data to forecast future crash events, informed by\nexpert annotations. Additionally, it goes beyond mere prediction, offering\ninsights into potential causes for each crash event. This work provides the\npreliminary insights into prompt-based large language models for the log-based\nevent prediction task.",
        "authors": "Priyanka Mudgal, Bijan Arbab, Swaathi Sampath Kumar",
        "translated": "随着对计算机系统的依赖在各个领域的扩展，集中在个人、工业和大规模应用上，迫切需要提高其可靠性，以无缝地维持业务运营，并确保最佳用户满意度。这些设备生成的系统日志可以作为历史趋势和过去失败的宝贵存储库。使用机器学习技术进行故障预测已经变得很普遍，这使得从过去的数据中提取洞察力来预测未来的行为模式成为可能。近年来，大型语言模型在总结、推理和事件预测等方面表现出了显著的能力。因此，在本文中，我们致力于研究大型语言模型在预测系统故障方面的潜力，利用从过去的故障行为中学到的知识，有效地为推理和决策过程提供信息。我们的方法涉及利用来自 Intel 计算改进计划(ICIP)系统崩溃日志的数据来识别重大事件并开发 CrashEventLLM。该模型建立在一个大型语言模型框架之上，是我们进行事故预测的基础。具体来说，我们的模型利用历史数据来预测未来的崩溃事件，通过专家注释。此外，它超越了单纯的预测，提供了对每个坠机事件的潜在原因的洞察力。这项工作为基于日志的事件预测任务提供了基于提示的大型语言模型的初步见解。"
    },
    {
        "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
        "url": "http://arxiv.org/abs/2407.15309v1",
        "pub_date": "2024-07-22",
        "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
        "authors": "Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu Feng, Shixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi Guo, Jingwen Leng",
        "translated": "大型语言模型(LLM)广泛应用于各种领域，每天处理数百万个请求。这种需求的激增在优化吞吐量和延迟同时保持成本可控方面提出了重大挑战。键值(KV)缓存是保留以前计算的标准方法，它使 LLM 推理高度受内存限制。虽然批处理策略可以提高性能，但它们经常导致严重的内存碎片。尽管像 vLLM 这样的尖端系统使用分页注意机制来缓解 KV 缓存碎片，但由于紧密耦合的页面管理和计算内核，它们仍然存在内存和计算操作效率低下的问题。介绍了一种基于 GPU 虚拟内存管理(VMM)的 LLM 推理新型张量结构 vTensor。VTensor 解除了计算与内存碎片整理的耦合，并提供了动态扩展性，从而解决了现有的局限性。我们的框架采用 CPU-GPU 异构方法，确保高效、无碎片的内存管理，同时跨不同 LLM 架构容纳各种计算内核。实验结果表明，在不同的模型下，vTensor 的平均加速比为1.86 x，在多回合聊天场景下加速比为2.42 x。此外，vTensor 在内核评估中提供了2.12 x 和3.15 x 的平均加速，与 SGLang Triton 前缀填充内核和 vLLM 分页注意力内核相比，分别达到了3.92 x 和3.27 x。此外，与 vLLM 相比，它在 NVIDIA A100 GPU 上释放了大约71.25% (57 GB)的内存，从而支持更多的内存密集型工作负载。"
    },
    {
        "title": "A New Theoretical Perspective on Data Heterogeneity in Federated\n  Optimization",
        "url": "http://arxiv.org/abs/2407.15567v1",
        "pub_date": "2024-07-22",
        "summary": "In federated learning (FL), data heterogeneity is the main reason that\nexisting theoretical analyses are pessimistic about the convergence rate. In\nparticular, for many FL algorithms, the convergence rate grows dramatically\nwhen the number of local updates becomes large, especially when the product of\nthe gradient divergence and local Lipschitz constant is large. However,\nempirical studies can show that more local updates can improve the convergence\nrate even when these two parameters are large, which is inconsistent with the\ntheoretical findings. This paper aims to bridge this gap between theoretical\nunderstanding and practical performance by providing a theoretical analysis\nfrom a new perspective on data heterogeneity. In particular, we propose a new\nand weaker assumption compared to the local Lipschitz gradient assumption,\nnamed the heterogeneity-driven pseudo-Lipschitz assumption. We show that this\nand the gradient divergence assumptions can jointly characterize the effect of\ndata heterogeneity. By deriving a convergence upper bound for FedAvg and its\nextensions, we show that, compared to the existing works, local Lipschitz\nconstant is replaced by the much smaller heterogeneity-driven pseudo-Lipschitz\nconstant and the corresponding convergence upper bound can be significantly\nreduced for the same number of local updates, although its order stays the\nsame. In addition, when the local objective function is quadratic, more\ninsights on the impact of data heterogeneity can be obtained using the\nheterogeneity-driven pseudo-Lipschitz constant. For example, we can identify a\nregion where FedAvg can outperform mini-batch SGD even when the gradient\ndivergence can be arbitrarily large. Our findings are validated using\nexperiments.",
        "authors": "Jiayi Wang, Shiqiang Wang, Rong-Rong Chen, Mingyue Ji",
        "translated": "在联邦学习中，数据的异构性是现有理论分析对收敛速度持悲观态度的主要原因。特别是对于许多 FL 算法来说，当局部更新的数量变大时，特别是当梯度散度和局部 Lipschitz 常数的乘积变大时，算法的收敛速度显著增加。然而，实证研究表明，更多的局部更新可以提高收敛速度，即使这两个参数很大，这与理论结果不一致。本文旨在通过从数据异构性的新视角进行理论分析，弥合理论认识与实践表现之间的差距。特别地，与局部 Lipschitz 梯度假设相比，我们提出了一个新的较弱的假设，即异质性驱动的伪 Lipschitz 假设。我们表明，这种假设和梯度散度假设可以共同表征数据异质性的影响。通过推导 FedAvg 及其扩展的收敛上界，我们发现，与已有的工作相比，局部 Lipschitz 常数被更小的异质性驱动的伪 Lipschitz 常数所取代，并且对于相同数量的局部更新，相应的收敛上界可以显著降低，尽管其次序保持不变。此外，当局部目标函数为二次函数时，利用异质性驱动的伪 Lipschitz 常数可以更深入地了解数据异质性的影响。例如，我们可以确定一个区域，其中 FedAvg 可以胜过小批量 SGD，即使梯度散度可以任意大。我们的发现通过实验得到了验证。"
    },
    {
        "title": "The Diversity Bonus: Learning from Dissimilar Distributed Clients in\n  Personalized Federated Learning",
        "url": "http://arxiv.org/abs/2407.15464v1",
        "pub_date": "2024-07-22",
        "summary": "Personalized Federated Learning (PFL) is a commonly used framework that\nallows clients to collaboratively train their personalized models. PFL is\nparticularly useful for handling situations where data from different clients\nare not independent and identically distributed (non-IID). Previous research in\nPFL implicitly assumes that clients can gain more benefits from those with\nsimilar data distributions. Correspondingly, methods such as personalized\nweight aggregation are developed to assign higher weights to similar clients\nduring training. We pose a question: can a client benefit from other clients\nwith dissimilar data distributions and if so, how? This question is\nparticularly relevant in scenarios with a high degree of non-IID, where clients\nhave widely different data distributions, and learning from only similar\nclients will lose knowledge from many other clients. We note that when dealing\nwith clients with similar data distributions, methods such as personalized\nweight aggregation tend to enforce their models to be close in the parameter\nspace. It is reasonable to conjecture that a client can benefit from dissimilar\nclients if we allow their models to depart from each other. Based on this idea,\nwe propose DiversiFed which allows each client to learn from clients with\ndiversified data distribution in personalized federated learning. DiversiFed\npushes personalized models of clients with dissimilar data distributions apart\nin the parameter space while pulling together those with similar distributions.\nIn addition, to achieve the above effect without using prior knowledge of data\ndistribution, we design a loss function that leverages the model similarity to\ndetermine the degree of attraction and repulsion between any two models.\nExperiments on several datasets show that DiversiFed can benefit from\ndissimilar clients and thus outperform the state-of-the-art methods.",
        "authors": "Xinghao Wu, Xuefeng Liu, Jianwei Niu, Guogang Zhu, Shaojie Tang, Xiaotian Li, Jiannong Cao",
        "translated": "个性化联邦学习(PFL)是一个常用的框架，它允许客户协同训练他们的个性化模型。PFL 对于处理来自不同客户端的数据不独立且分布相同(非 IID)的情况特别有用。以前在 PFL 中的研究隐含地假设客户可以从那些具有相似数据分布的客户那里获得更多的好处。相应地，采用个性化权重聚合等方法，在训练过程中为相似的客户分配更高的权重。我们提出一个问题: 一个客户机能够从其他具有不同数据分布的客户机中获益吗? 如果能够，如何获益？这个问题在具有高度非 IID 的场景中尤其相关，在这些场景中，客户机具有差异很大的数据分布，仅从类似的客户机学习将失去许多其他客户机的知识。我们注意到当处理具有相似数据分布的客户端时，诸如个性化权重聚合的方法倾向于强制他们的模型在参数空间中接近。我们可以合理地推测，如果我们允许不同的客户的模型彼此背离，那么客户可以从不同的客户中获益。基于这种思想，我们提出了 DiversiFed，它允许每个客户在个性化联邦学习中向具有多样化数据分布的客户学习。DiversiFed 将具有不同数据分布的客户机的个性化模型分开放在参数空间中，同时将具有相似分布的客户机放在一起。此外，为了在不使用数据分布的先验知识的情况下达到上述效果，我们设计了一个损失函数，利用模型相似度来确定任意两个模型之间的吸引和排斥程度。在几个数据集上的实验表明，DiversiFed 可以从不同的客户端获益，因此优于最先进的方法。"
    },
    {
        "title": "GraphScale: A Framework to Enable Machine Learning over Billion-node\n  Graphs",
        "url": "http://arxiv.org/abs/2407.15452v1",
        "pub_date": "2024-07-22",
        "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for supervised\nmachine learning over graph-structured data, while sampling-based node\nrepresentation learning is widely utilized in unsupervised learning. However,\nscalability remains a major challenge in both supervised and unsupervised\nlearning for large graphs (e.g., those with over 1 billion nodes). The\nscalability bottleneck largely stems from the mini-batch sampling phase in GNNs\nand the random walk sampling phase in unsupervised methods. These processes\noften require storing features or embeddings in memory. In the context of\ndistributed training, they require frequent, inefficient random access to data\nstored across different workers. Such repeated inter-worker communication for\neach mini-batch leads to high communication overhead and computational\ninefficiency.\n  We propose GraphScale, a unified framework for both supervised and\nunsupervised learning to store and process large graph data distributedly. The\nkey insight in our design is the separation of workers who store data and those\nwho perform the training. This separation allows us to decouple computing and\nstorage in graph training, thus effectively building a pipeline where data\nfetching and data computation can overlap asynchronously. Our experiments show\nthat GraphScale outperforms state-of-the-art methods for distributed training\nof both GNNs and node embeddings. We evaluate GraphScale both on public and\nproprietary graph datasets and observe a reduction of at least 40% in\nend-to-end training times compared to popular distributed frameworks, without\nany loss in performance. While most existing methods don't support billion-node\ngraphs for training node embeddings, GraphScale is currently deployed in\nproduction at TikTok enabling efficient learning over such large graphs.",
        "authors": "Vipul Gupta, Xin Chen, Ruoyun Huang, Fanlong Meng, Jianjun Chen, Yujun Yan",
        "translated": "图形神经网络(GNN)已经成为处理图形结构化数据的有力工具，而基于抽样的节点表示学习在监督式学习非监督式学习中得到了广泛的应用。然而，可伸缩性仍然是大型图形(例如，超过10亿个节点)的监督和非监督式学习的主要挑战。可扩展性瓶颈主要来源于 GNN 中的小批量采样阶段和无监督方法中的随机游走采样阶段。这些进程通常需要在内存中存储特性或嵌入。在分布式培训的背景下，它们需要对存储在不同工人之间的数据进行频繁、低效的随机访问。对于每个小批处理，这种重复的工人间通信导致高通信开销和计算效率低下。我们提出了 GraphScale，这是一个统一的框架，用于监督和非监督式学习分布式存储和处理大型图形数据。我们设计中的关键洞察力是将存储数据的工作人员和执行培训的工作人员分离开来。这种分离允许我们在图训练中解耦计算和存储，从而有效地构建一个数据获取和数据计算可以异步重叠的管道。我们的实验表明，GraphScale 在 GNN 和节点嵌入的分布式训练方面优于最先进的方法。我们评估了 GraphScale 在公共和专有图形数据集上的表现，并观察到与流行的分布式框架相比，端到端培训时间至少减少了40% ，性能没有任何损失。虽然大多数现有的方法不支持十亿节点图来训练节点嵌入，GraphScale 目前已经部署在 TikTok 的生产环境中，可以在如此大的图上进行有效的学习。"
    },
    {
        "title": "Automated Road Safety: Enhancing Sign and Surface Damage Detection with\n  AI",
        "url": "http://arxiv.org/abs/2407.15406v1",
        "pub_date": "2024-07-22",
        "summary": "Public transportation plays a crucial role in our lives, and the road network\nis a vital component in the implementation of smart cities. Recent advancements\nin AI have enabled the development of advanced monitoring systems capable of\ndetecting anomalies in road surfaces and road signs, which, if unaddressed, can\nlead to serious road accidents. This paper presents an innovative approach to\nenhance road safety through the detection and classification of traffic signs\nand road surface damage using advanced deep learning techniques. This\nintegrated approach supports proactive maintenance strategies, improving road\nsafety and resource allocation for the Molise region and the city of\nCampobasso. The resulting system, developed as part of the Casa delle\nTecnologie Emergenti (House of Emergent Technologies) Molise (Molise CTE)\nresearch project funded by the Italian Minister of Economic Growth (MIMIT),\nleverages cutting-edge technologies such as Cloud Computing and High\nPerformance Computing with GPU utilization. It serves as a valuable tool for\nmunicipalities, enabling quick detection of anomalies and the prompt\norganization of maintenance operations",
        "authors": "Davide Merolla, Vittorio Latorre, Antonio Salis, Gianluca Boanelli",
        "translated": "公共交通在我们的生活中扮演着至关重要的角色，而道路网络是实施智慧城市的重要组成部分。人工智能的最新进展使得能够发展先进的监测系统，能够检测道路表面和道路标志的异常情况，如果不加以处理，这些异常情况可能导致严重的道路交通事故。本文提出了一种利用先进的深度学习技术，通过对交通标志和路面损伤的检测和分类来提高道路安全性的创新方法。这种综合方法支持积极主动的维修策略，改善莫利塞地区和坎波巴索的道路安全和资源分配。该系统是意大利经济增长部资助的 Casa delle Technology Emergenti (House of Emergent Technologies) Molise (Molise CTE)研究项目的一部分，利用云计算和高性能计算等尖端技术和 GPU 利用率。它是市政当局的一个宝贵工具，能够迅速发现异常情况并迅速组织维护行动"
    },
    {
        "title": "Tackling Selfish Clients in Federated Learning",
        "url": "http://arxiv.org/abs/2407.15402v1",
        "pub_date": "2024-07-22",
        "summary": "Federated Learning (FL) is a distributed machine learning paradigm\nfacilitating participants to collaboratively train a model without revealing\ntheir local data. However, when FL is deployed into the wild, some intelligent\nclients can deliberately deviate from the standard training process to make the\nglobal model inclined toward their local model, thereby prioritizing their\nlocal data distribution. We refer to this novel category of misbehaving clients\nas selfish. In this paper, we propose a Robust aggregation strategy for FL\nserver to mitigate the effect of Selfishness (in short RFL-Self). RFL-Self\nincorporates an innovative method to recover (or estimate) the true updates of\nselfish clients from the received ones, leveraging robust statistics (median of\nnorms) of the updates at every round. By including the recovered updates in\naggregation, our strategy offers strong robustness against selfishness. Our\nexperimental results, obtained on MNIST and CIFAR-10 datasets, demonstrate that\njust 2% of clients behaving selfishly can decrease the accuracy by up to 36%,\nand RFL-Self can mitigate that effect without degrading the global model\nperformance.",
        "authors": "Andrea Augello, Ashish Gupta, Giuseppe Lo Re, Sajal K. Das",
        "translated": "联邦学习(FL)是一种分布式的机器学习范式，它可以帮助参与者在不暴露本地数据的情况下协同训练一个模型。然而，当 FL 被部署到野外，一些智能客户可以故意偏离标准的训练过程，使全局模型倾向于他们的本地模型，从而优先考虑他们的本地数据分布。我们把这种行为不端的客户归类为自私。本文提出了一种针对 FL 服务器的鲁棒聚合策略，以减轻自私性(简称 RFL-Self)对 FL 服务器的影响。RFL-Self 采用了一种创新的方法来从收到的客户那里恢复(或估计)真实的自私客户的更新，利用每一轮更新的稳健统计(规范的中值)。通过在聚合中包含恢复的更新，我们的策略提供了对自私的强健性。我们在 MNIST 和 CIFAR-10数据集上获得的实验结果表明，只有2% 的客户行为自私可以降低36% 的准确性，而 RFL-Self 可以在不降低全局模型性能的情况下减轻这种影响。"
    },
    {
        "title": "Poisoning with A Pill: Circumventing Detection in Federated Learning",
        "url": "http://arxiv.org/abs/2407.15389v1",
        "pub_date": "2024-07-22",
        "summary": "Without direct access to the client's data, federated learning (FL) is\nwell-known for its unique strength in data privacy protection among existing\ndistributed machine learning techniques. However, its distributive and\niterative nature makes FL inherently vulnerable to various poisoning attacks.\nTo counteract these threats, extensive defenses have been proposed to filter\nout malicious clients, using various detection metrics. Based on our analysis\nof existing attacks and defenses, we find that there is a lack of attention to\nmodel redundancy. In neural networks, various model parameters contribute\ndifferently to the model's performance. However, existing attacks in FL\nmanipulate all the model update parameters with the same strategy, making them\neasily detectable by common defenses. Meanwhile, the defenses also tend to\nanalyze the overall statistical features of the entire model updates, leaving\nroom for sophisticated attacks. Based on these observations, this paper\nproposes a generic and attack-agnostic augmentation approach designed to\nenhance the effectiveness and stealthiness of existing FL poisoning attacks\nagainst detection in FL, pointing out the inherent flaws of existing defenses\nand exposing the necessity of fine-grained FL security. Specifically, we employ\na three-stage methodology that strategically constructs, generates, and injects\npoison (generated by existing attacks) into a pill (a tiny subnet with a novel\nstructure) during the FL training, named as pill construction, pill poisoning,\nand pill injection accordingly. Extensive experimental results show that FL\npoisoning attacks enhanced by our method can bypass all the popular defenses,\nand can gain an up to 7x error rate increase, as well as on average a more than\n2x error rate increase on both IID and non-IID data, in both cross-silo and\ncross-device FL systems.",
        "authors": "Hanxi Guo, Hao Wang, Tao Song, Tianhang Zheng, Yang Hua, Haibing Guan, Xiangyu Zhang",
        "translated": "由于不能直接访问客户端的数据，联邦学习(FL)在现有的分布式机器学习技术中以其独特的数据隐私保护能力而闻名。然而，它的分布性和迭代性使得 FL 本质上容易受到各种中毒攻击。为了应对这些威胁，已经提出了广泛的防御措施，以过滤掉恶意客户端，使用各种检测指标。基于我们对现有攻击和防御的分析，我们发现缺乏对模型冗余的关注。在神经网络中，各种模型参数对模型性能的影响是不同的。但是，FL 中现有的攻击使用相同的策略操纵所有模型更新参数，使得它们很容易被常见的防御系统检测到。同时，防御系统也倾向于分析整个模型更新的整体统计特征，为复杂的攻击留下空间。在此基础上，本文提出了一种通用的、攻击无关的增强方法，旨在提高现有 FL 中毒攻击对检测的有效性和隐蔽性，指出现有防御的固有缺陷，揭示细粒度 FL 安全的必要性。具体而言，我们采用了一种三阶段的方法，在 FL 培训期间策略性地构建，生成和注射毒药(由现有攻击产生)到药丸(具有新结构的微小子网)中，相应地命名为药丸构建，药丸中毒和药丸注射。大量的实验结果表明，该方法增强的 FL 中毒攻击可以绕过所有流行的防御，并且在跨竖井和跨设备 FL 系统中，IID 和非 IID 数据的错误率平均增加7倍以上。"
    }
]