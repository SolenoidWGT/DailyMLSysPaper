[
    {
        "title": "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs",
        "url": "http://arxiv.org/abs/2407.03234v1",
        "pub_date": "2024-07-03",
        "summary": "When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\nmade available at https://github.com/Linlt-leon/Adversarial-Alignments.",
        "translated": "当 LLM 部署在敏感的、面向人的设置中时，关键是不要输出不安全的、有偏见的或侵犯隐私的输出。出于这个原因，模特们都接受过训练，也接受过指导，拒绝回答诸如“告诉我如何制造炸弹”之类的不安全提示我们发现，尽管有这些保护措施，仅仅通过在模型输入的末尾添加一个空格，就有可能打破模型的防御。在一项对八个开源模型的研究中，我们证明了这种攻击足够强大，足以导致大多数模型产生有害的输出，成功率非常高。我们检查了这种行为的原因，发现在标记化的训练数据中出现单个空格的上下文鼓励模型在提示时生成列表，覆盖训练信号以拒绝回答不安全的请求。我们的研究结果强调了当前模型对齐的脆弱状态，并提出了开发更强大的对齐方法的重要性。代码和数据将在 https://github.com/linlt-leon/adversarial-alignments 公布。"
    },
    {
        "title": "Bridging Model Heterogeneity in Federated Learning via Uncertainty-based\n  Asymmetrical Reciprocity Learning",
        "url": "http://arxiv.org/abs/2407.03247v1",
        "pub_date": "2024-07-03",
        "summary": "This paper presents FedType, a simple yet pioneering framework designed to\nfill research gaps in heterogeneous model aggregation within federated learning\n(FL). FedType introduces small identical proxy models for clients, serving as\nagents for information exchange, ensuring model security, and achieving\nefficient communication simultaneously. To transfer knowledge between large\nprivate and small proxy models on clients, we propose a novel uncertainty-based\nasymmetrical reciprocity learning method, eliminating the need for any public\ndata. Comprehensive experiments conducted on benchmark datasets demonstrate the\nefficacy and generalization ability of FedType across diverse settings. Our\napproach redefines federated learning paradigms by bridging model\nheterogeneity, eliminating reliance on public data, prioritizing client\nprivacy, and reducing communication costs.",
        "authors": "Jiaqi Wang, Chenxu Zhao, Lingjuan Lyu, Quanzeng You, Mengdi Huai, Fenglong Ma",
        "translated": "本文介绍了 FedType，这是一个简单而具有开创性的框架，旨在填补联邦学习(FL)中异构模型聚合的研究空白。FedType 为客户端引入了小型相同的代理模型，作为信息交换的代理，保证了模型的安全性，同时实现了高效的通信。针对大型私有代理模型和小型代理模型之间的知识传递问题，提出了一种基于不确定性的非对称互惠学习方法，该方法不需要任何公开数据。在基准数据集上进行的综合实验证明了 FedType 在不同环境下的有效性和推广能力。我们的方法通过桥接模型异构性、消除对公共数据的依赖、优先考虑客户隐私和降低通信成本来重新定义联邦学习范例。"
    },
    {
        "title": "Streaming Large-Scale Electron Microscopy Data to a Supercomputing\n  Facility",
        "url": "http://arxiv.org/abs/2407.03215v1",
        "pub_date": "2024-07-03",
        "summary": "Data management is a critical component of modern experimental workflows. As\ndata generation rates increase, transferring data from acquisition servers to\nprocessing servers via conventional file-based methods is becoming increasingly\nimpractical. The 4D Camera at the National Center for Electron Microscopy\n(NCEM) generates data at a nominal rate of 480 Gbit/s (87,000 frames/s)\nproducing a 700 GB dataset in fifteen seconds. To address the challenges\nassociated with storing and processing such quantities of data, we developed a\nstreaming workflow that utilizes a high-speed network to connect the 4D\nCamera's data acquisition (DAQ) system to supercomputing nodes at the National\nEnergy Research Scientific Computing Center (NERSC), bypassing intermediate\nfile storage entirely. In this work, we demonstrate the effectiveness of our\nstreaming pipeline in a production setting through an hour-long experiment that\ngenerated over 10 TB of raw data, yielding high-quality datasets suitable for\nadvanced analyses. Additionally, we compare the efficacy of this streaming\nworkflow against the conventional file-transfer workflow by conducting a\npost-mortem analysis on historical data from experiments performed by real\nusers. Our findings show that the streaming workflow significantly improves\ndata turnaround time, enables real-time decision-making, and minimizes the\npotential for human error by eliminating manual user interactions.",
        "authors": "Samuel S. Welborn, Chris Harris, Stephanie M. Ribet, Georgios Varnavides, Colin Ophus, Bjoern Enders, Peter Ercius",
        "translated": "数据管理是现代实验工作流的重要组成部分。随着数据生成率的提高，通过传统的基于文件的方法将数据从采集服务器传输到处理服务器正变得越来越不切实际。美国国家电子显微镜中心(NCEM)的4D 相机以480Gbit/s (87000帧/s)的标称速率生成数据，在15秒内产生700GB 的数据集。为了解决存储和处理这样大量数据的挑战，我们开发了一个流式工作流，利用高速网络将4D 摄像机的数据采集(DAQ)系统连接到国家能源研究科学计算中心(NERSC)的超级计算节点，完全绕过中间文件存储。在这项工作中，我们通过一个小时的实验，生成了超过10TB 的原始数据，产生了适合高级分析的高质量数据集，从而证明了我们的流式流水线在生产环境中的有效性。此外，我们通过对实际用户实验中的历史数据进行事后分析，比较了该流工作流与传统文件传输工作流的效率。我们的研究结果表明，流式工作流可以显著提高数据周转时间，实现实时决策，并通过消除人工用户交互，最大限度地减少潜在的人为错误。"
    },
    {
        "title": "Effective Heterogeneous Federated Learning via Efficient\n  Hypernetwork-based Weight Generation",
        "url": "http://arxiv.org/abs/2407.03086v1",
        "pub_date": "2024-07-03",
        "summary": "While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86 times compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.",
        "authors": "Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim, JeongGil Ko",
        "translated": "虽然联邦学习利用分布式客户端资源，但是由于异构客户端功能，它面临着挑战。这就需要分配适合客户端资源的模型和仔细的参数聚合来适应这种异构性。我们提出了一种新的联邦学习框架 HypeMeFed，它通过将多出口网络结构与基于超网络的模型权重生成相结合来支持客户端的异构性。该方法对异构模型层的特征空间进行对齐，解决了权重聚合过程中各层的信息差异问题。为了实现 HypeMeFed，我们还提出了一种低秩因子分解方法，以最小化与超网络相关的计算和内存开销。我们对现实世界异构设备测试台的评估表明，与天真的超网络方法相比，HypeMeFed 比 FedAvg 提高了5.12% 的准确性，减少了98.22% 的超网络内存需求，并将其操作加速了1.86倍。这些结果证明了 HypeMeFed 在利用和吸引异构客户机进行联合学习方面的有效性。"
    },
    {
        "title": "On the Client Preference of LLM Fine-tuning in Federated Learning",
        "url": "http://arxiv.org/abs/2407.03038v1",
        "pub_date": "2024-07-03",
        "summary": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained\nlarge language model (LLM) using preference datasets, enabling the LLM to\ngenerate outputs that align with human preferences. Given the sensitive nature\nof these preference datasets held by various clients, there is a need to\nimplement RLHF within a federated learning (FL) framework, where clients are\nreluctant to share their data due to privacy concerns. To address this, we\nintroduce a feasible framework in which clients collaboratively train a binary\nselector with their preference datasets using our proposed FedBis. With a\nwell-trained selector, we can further enhance the LLM that generates\nhuman-preferred completions. Meanwhile, we propose a novel algorithm,\nFedBiscuit, that trains multiple selectors by organizing clients into balanced\nand disjoint clusters based on their preferences. Compared to the FedBis,\nFedBiscuit demonstrates superior performance in simulating human preferences\nfor pairwise completions. Our extensive experiments on federated human\npreference datasets -- marking the first benchmark to address heterogeneous\ndata partitioning among clients -- demonstrate that FedBiscuit outperforms\nFedBis and even surpasses traditional centralized training.",
        "authors": "Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Jing Gao",
        "translated": "强化学习反馈(rlHF)使用偏好数据集对预先训练好的大语言模型(LLM)进行微调，使 LLM 能够生成符合人类偏好的输出。鉴于这些偏好数据集由不同的客户端持有的敏感性质，有必要在联邦学习(FL)框架内实现 RLHF，其中客户端由于隐私问题不愿意共享他们的数据。为了解决这个问题，我们引入了一个可行的框架，在这个框架中，客户端使用我们提出的 FedBis 协作地训练一个二进制选择器和他们的偏好数据集。使用训练有素的选择器，我们可以进一步增强生成人类首选完成的 LLM。同时，我们提出了一种新的算法，FedBiscookie，通过将客户端根据他们的偏好组织成平衡的和不相交的集群来训练多个选择器。与 FedBis 相比，FedBiscookie 在模拟人类对配对完成的偏好方面表现出了优越的性能。我们在联邦人类偏好数据集上的广泛实验——标志着解决客户端之间异构数据分区的第一个基准——证明了 FedBiscue 的性能优于 FedBis，甚至超过了传统的集中式训练。"
    },
    {
        "title": "DRLQ: A Deep Reinforcement Learning-based Task Placement for Quantum\n  Cloud Computing",
        "url": "http://arxiv.org/abs/2407.02748v1",
        "pub_date": "2024-07-03",
        "summary": "The quantum cloud computing paradigm presents unique challenges in task\nplacement due to the dynamic and heterogeneous nature of quantum computation\nresources. Traditional heuristic approaches fall short in adapting to the\nrapidly evolving landscape of quantum computing. This paper proposes DRLQ, a\nnovel Deep Reinforcement Learning (DRL)-based technique for task placement in\nquantum cloud computing environments, addressing the optimization of task\ncompletion time and quantum task scheduling efficiency. It leverages the Deep Q\nNetwork (DQN) architecture, enhanced with the Rainbow DQN approach, to create a\ndynamic task placement strategy. This approach is one of the first in the field\nof quantum cloud resource management, enabling adaptive learning and\ndecision-making for quantum cloud environments and effectively optimizing task\nplacement based on changing conditions and resource availability. We conduct\nextensive experiments using the QSimPy simulation toolkit to evaluate the\nperformance of our method, demonstrating substantial improvements in task\nexecution efficiency and a reduction in the need to reschedule quantum tasks.\nOur results show that utilizing the DRLQ approach for task placement can\nsignificantly reduce total quantum task completion time by 37.81% to 72.93% and\nprevent task rescheduling attempts compared to other heuristic approaches.",
        "authors": "Hoa T. Nguyen, Muhammad Usman, Rajkumar Buyya",
        "translated": "由于量子计算资源的动态性和异构性，量子云计算范式在任务配置中提出了独特的挑战。传统的启发式方法不能适应量子计算的快速发展。本文提出了基于深度强化学习的量子云计算任务分配技术 DRLQ，解决了任务完成时间和量子任务调度效率的优化问题。它利用 Deep Q Network (DQN)体系结构，并通过 Rainbow DQN 方法得到了增强，从而创建了一个动态任务分配策略。这种方法是量子云资源管理领域的首创之一，能够为量子云环境提供在线机机器学习和决策支持，并根据不断变化的条件和资源可用性有效地优化任务分配。我们使用 QSimPy 模拟工具包进行了广泛的实验，以评估我们的方法的性能，证明了任务执行效率的实质性改进和重新调度量子任务的需求的减少。实验结果表明，与其他启发式方法相比，利用 DRLQ 方法进行任务分配可以显著减少任务完成总时间37.81% 到72.93% ，并且可以防止任务重调度尝试。"
    },
    {
        "title": "Accelerating Distributed Optimization: A Primal-Dual Perspective on\n  Local Steps",
        "url": "http://arxiv.org/abs/2407.02689v1",
        "pub_date": "2024-07-02",
        "summary": "In distributed machine learning, efficient training across multiple agents\nwith different data distributions poses significant challenges. Even with a\ncentralized coordinator, current algorithms that achieve optimal communication\ncomplexity typically require either large minibatches or compromise on gradient\ncomplexity. In this work, we tackle both centralized and decentralized settings\nacross strongly convex, convex, and nonconvex objectives. We first demonstrate\nthat a basic primal-dual method, (Accelerated) Gradient Ascent Multiple\nStochastic Gradient Descent (GA-MSGD), applied to the Lagrangian of distributed\noptimization inherently incorporates local updates, because the inner loops of\nrunning Stochastic Gradient Descent on the primal variable require no\ninter-agent communication. Notably, for strongly convex objectives, we show\n(Accelerated) GA-MSGD achieves linear convergence in communication rounds\ndespite the Lagrangian being only linear in the dual variables. This is due to\na unique structural property where the dual variable is confined to the span of\nthe coupling matrix, rendering the dual problem strongly concave. When\nintegrated with the Catalyst framework, our approach achieves nearly optimal\ncommunication complexity across various settings without the need for\nminibatches. Moreover, in stochastic decentralized problems, it attains\ncommunication complexities comparable to those in deterministic settings,\nimproving over existing algorithms.",
        "authors": "Junchi Yang, Murat Yildirim, Qiu Feng",
        "translated": "在分布式机器学习中，对具有不同数据分布的多代理进行有效的训练是一个巨大的挑战。即使有一个集中的协调器，当前的算法，以实现最佳的通信复杂度通常需要或者大的微型批量或梯度复杂度折衷。在这项工作中，我们处理集中和分散设置跨强烈凸，凸和非凸的目标。我们首先证明了一个基本的原始-对偶方法，(加速)梯度上升多重随机梯度下降(GA-MSGD) ，应用于分布式优化的拉格朗日方法，固有地结合了局部更新，因为在原始变量上运行的随机梯度下降的内部循环不需要代理之间的通信。值得注意的是，对于强凸目标，我们显示(加速) GA-MSGD 实现线性收敛的通信轮，尽管拉格朗日只是线性对偶变量。这是由于一个独特的结构性质，其中对偶变量限制在耦合矩阵的跨度，使对偶问题强烈凹。当与 Catalyst 框架集成时，我们的方法在不需要小批处理的情况下实现了几乎最佳的跨各种设置的通信复杂性。此外，在随机分散问题中，它获得了与确定性设置中的通信复杂度相当的通信复杂度，比现有算法有所改进。"
    },
    {
        "title": "Towards Federated Learning with On-device Training and Communication in\n  8-bit Floating Point",
        "url": "http://arxiv.org/abs/2407.02610v1",
        "pub_date": "2024-07-02",
        "summary": "Recent work has shown that 8-bit floating point (FP8) can be used for\nefficiently training neural networks with reduced computational overhead\ncompared to training in FP32/FP16. In this work, we investigate the use of FP8\ntraining in a federated learning context. This brings not only the usual\nbenefits of FP8 which are desirable for on-device training at the edge, but\nalso reduces client-server communication costs due to significant weight\ncompression. We present a novel method for combining FP8 client training while\nmaintaining a global FP32 server model and provide convergence analysis.\nExperiments with various machine learning models and datasets show that our\nmethod consistently yields communication reductions of at least 2.9x across a\nvariety of tasks and models compared to an FP32 baseline.",
        "authors": "Bokun Wang, Axel Berg, Durmus Alp Emre Acar, Chuteng Zhou",
        "translated": "最近的研究表明，与 FP32/FP16相比，8位浮点数(FP8)可以用来有效地训练神经网络，减少计算开销。在这项工作中，我们研究了 FP8训练在联邦学习环境中的应用。这不仅带来了 FP8的通常好处，这对边缘设备上的培训是可取的，而且由于显著的权重压缩，还降低了客户机-服务器通信成本。我们提出了一种新的方法，结合 FP8客户端训练，同时维护一个全球性的 FP32服务器模型，并提供了收敛性分析。对各种机器学习模型和数据集的实验表明，与 FP32基线相比，我们的方法在各种任务和模型之间始终产生至少2.9倍的通信减少。"
    },
    {
        "title": "Decentralized Intelligence Network (DIN)",
        "url": "http://arxiv.org/abs/2407.02461v1",
        "pub_date": "2024-07-02",
        "summary": "Decentralized Intelligence Network (DIN) addresses the significant challenges\nof data sovereignty and AI utilization caused by the fragmentation and siloing\nof data across providers and institutions. This comprehensive framework\novercomes access barriers to scalable data sources previously hindered by silos\nby leveraging: 1) personal data stores as a prerequisite for data sovereignty;\n2) a scalable federated learning protocol implemented on a public blockchain\nfor decentralized AI training, where data remains with participants and only\nmodel parameter updates are shared; and 3) a scalable, trustless rewards\nmechanism to incentivize participation and ensure fair reward distribution.\nThis framework ensures that no entity can prevent or control access to training\non data offered by participants or determine financial benefits, as these\nprocesses operate on a public blockchain with an immutable record and without a\nthird party. It supports effective AI training, allowing participants to\nmaintain control over their data, benefit financially, and contribute to a\ndecentralized, scalable ecosystem that leverages collective AI to develop\nbeneficial algorithms.",
        "authors": "Abraham Nash",
        "translated": "分散智能网(DIN)解决了数据主权和人工智能利用方面的重大挑战，这些挑战是由于数据在供应商和机构之间的碎片化和孤立性造成的。这个全面的框架通过利用以下方面克服了以前受到竖井阻碍的可扩展数据源的访问障碍: 1)个人数据存储作为数据主权的先决条件; 2)在公共区块链上实施的可扩展联合学习协议，用于分散的 AI 培训，其中数据保留在参与者身上，只有模型参数更新被共享; 3)可扩展的、不可信任的奖励机制，以激励参与并确保公平的奖励分配。这一框架确保任何实体都无法防止或控制获得参与者提供的数据培训或确定经济利益，因为这些过程是在公共区块链上运作的，具有不可变的记录，没有第三方。它支持有效的人工智能培训，允许参与者保持对他们的数据的控制，从经济上获益，并促进一个分散的，可扩展的生态系统，利用集体人工智能开发有益的算法。"
    },
    {
        "title": "Uncertainty-Aware Decarbonization for Datacenters",
        "url": "http://arxiv.org/abs/2407.02390v1",
        "pub_date": "2024-07-02",
        "summary": "This paper represents the first effort to quantify uncertainty in carbon\nintensity forecasting for datacenter decarbonization. We identify and analyze\ntwo types of uncertainty -- temporal and spatial -- and discuss their system\nimplications. To address the temporal dynamics in quantifying uncertainty for\ncarbon intensity forecasting, we introduce a conformal prediction-based\nframework. Evaluation results show that our technique robustly achieves target\ncoverages in uncertainty quantification across various significance levels. We\nconduct two case studies using production power traces, focusing on temporal\nand spatial load shifting respectively. The results show that incorporating\nuncertainty into scheduling decisions can prevent a 5% and 14% increase in\ncarbon emissions, respectively. These percentages translate to an absolute\nreduction of 2.1 and 10.4 tons of carbon emissions in a 20 MW datacenter\ncluster.",
        "authors": "Amy Li, Sihang Liu, Yi Ding",
        "translated": "本文首次尝试对数据中心脱碳过程中碳强度预测的不确定性进行量化。我们识别和分析了两种类型的不确定性——时间和空间——并讨论了它们的系统含义。为了解决碳强度预测不确定性量化的时间动态问题，我们引入了一个基于保形预测的框架。评价结果表明，该方法在不同的显著性水平上均能稳健地实现不确定性量化的目标覆盖。我们使用生产功率轨迹进行了两个案例研究，分别侧重于时间和空间负荷转移。结果表明，将不确定性纳入调度决策可以分别防止5% 和14% 的碳排放增加。这些百分比意味着在一个20兆瓦的数据中心集群中，碳排放绝对减少了2.1吨和10.4吨。"
    },
    {
        "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across\n  Hybrid Devices",
        "url": "http://arxiv.org/abs/2407.02327v1",
        "pub_date": "2024-07-02",
        "summary": "A number of production deep learning clusters have attempted to explore\ninference hardware for DNN training, at the off-peak serving hours with many\ninference GPUs idling. Conducting DNN training with a combination of\nheterogeneous training and inference GPUs, known as hybrid device training,\npresents considerable challenges due to disparities in compute capability and\nsignificant differences in memory capacity. We propose QSync, a training system\nthat enables efficient synchronous data-parallel DNN training over hybrid\ndevices by strategically exploiting quantized operators. According to each\ndevice's available resource capacity, QSync selects a quantization-minimized\nsetting for operators in the distributed DNN training graph, minimizing model\naccuracy degradation but keeping the training efficiency brought by\nquantization. We carefully design a predictor with a bi-directional\nmixed-precision indicator to reflect the sensitivity of DNN layers on\nfixed-point and floating-point low-precision operators, a replayer with a\nneighborhood-aware cost mapper to accurately estimate the latency of\ndistributed hybrid mixed-precision training, and then an allocator that\nefficiently synchronizes workers with minimized model accuracy degradation.\nQSync bridges the computational graph on PyTorch to an optimized backend for\nquantization kernel performance and flexible support for various GPU\narchitectures. Extensive experiments show that QSync's predictor can accurately\nsimulate distributed mixed-precision training with &lt;5% error, with a consistent\n0.27-1.03% accuracy improvement over the from-scratch training tasks compared\nto uniform precision.",
        "authors": "Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Yibo Zhu, Chuan Wu",
        "translated": "一些生产深度学习集群试图探索 DNN 培训的推理硬件，在非高峰服务时间与许多推理 GPU 闲置。用异构训练和推理 GPU (称为混合设备训练)组合进行 DNN 训练，由于计算能力的差异和内存容量的显著差异，提出了相当大的挑战。我们提出 QSync，一个训练系统，使有效的同步数据并行 DNN 训练超过混合设备的战略利用量化操作员。QSync 根据每个设备的可用资源容量，在分布式 DNN 训练图中为操作者选择一个量化最小化的设置，最小化模型精度的降低，同时保持量化带来的训练效率。我们精心设计了一个双向混合精度指标的预测器来反映 DNN 层对定点和浮点低精度算子的敏感性，一个具有邻域感知成本映射器的中继器来精确估计分布式混合精度训练的延迟，然后一个分配器来有效地同步工人，最小化模型精度退化。QSync 将 PyTorch 上的计算图连接到一个优化的后端，用于量化内核性能和对各种 GPU 架构的灵活支持。大量实验表明，QSync 预测器能够准确地模拟分布式混合精度训练，误差小于5% ，与均匀精度训练相比，准确率提高了0.27 -1.03% 。"
    }
]