[
    {
        "title": "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs",
        "url": "http://arxiv.org/abs/2407.03234v1",
        "pub_date": "2024-07-03",
        "summary": "When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\nmade available at https://github.com/Linlt-leon/Adversarial-Alignments.",
        "translated": "当 LLM 部署在敏感的、面向人的设置中时，关键是不要输出不安全的、有偏见的或侵犯隐私的输出。出于这个原因，模特们都接受过训练，也接受过指导，拒绝回答诸如“告诉我如何制造炸弹”之类的不安全提示我们发现，尽管有这些保护措施，仅仅通过在模型输入的末尾添加一个空格，就有可能打破模型的防御。在一项对八个开源模型的研究中，我们证明了这种攻击足够强大，足以导致大多数模型产生有害的输出，成功率非常高。我们检查了这种行为的原因，发现在标记化的训练数据中出现单个空格的上下文鼓励模型在提示时生成列表，覆盖训练信号以拒绝回答不安全的请求。我们的研究结果强调了当前模型对齐的脆弱状态，并提出了开发更强大的对齐方法的重要性。代码和数据将在 https://github.com/linlt-leon/adversarial-alignments 公布。"
    },
    {
        "title": "Bridging Model Heterogeneity in Federated Learning via Uncertainty-based\n  Asymmetrical Reciprocity Learning",
        "url": "http://arxiv.org/abs/2407.03247v1",
        "pub_date": "2024-07-03",
        "summary": "This paper presents FedType, a simple yet pioneering framework designed to\nfill research gaps in heterogeneous model aggregation within federated learning\n(FL). FedType introduces small identical proxy models for clients, serving as\nagents for information exchange, ensuring model security, and achieving\nefficient communication simultaneously. To transfer knowledge between large\nprivate and small proxy models on clients, we propose a novel uncertainty-based\nasymmetrical reciprocity learning method, eliminating the need for any public\ndata. Comprehensive experiments conducted on benchmark datasets demonstrate the\nefficacy and generalization ability of FedType across diverse settings. Our\napproach redefines federated learning paradigms by bridging model\nheterogeneity, eliminating reliance on public data, prioritizing client\nprivacy, and reducing communication costs.",
        "authors": "Jiaqi Wang, Chenxu Zhao, Lingjuan Lyu, Quanzeng You, Mengdi Huai, Fenglong Ma",
        "translated": "本文介绍了 FedType，这是一个简单而具有开创性的框架，旨在填补联邦学习(FL)中异构模型聚合的研究空白。FedType 为客户端引入了小型相同的代理模型，作为信息交换的代理，保证了模型的安全性，同时实现了高效的通信。针对大型私有代理模型和小型代理模型之间的知识传递问题，提出了一种基于不确定性的非对称互惠学习方法，该方法不需要任何公开数据。在基准数据集上进行的综合实验证明了 FedType 在不同环境下的有效性和推广能力。我们的方法通过桥接模型异构性、消除对公共数据的依赖、优先考虑客户隐私和降低通信成本来重新定义联邦学习范例。"
    },
    {
        "title": "Streaming Large-Scale Electron Microscopy Data to a Supercomputing\n  Facility",
        "url": "http://arxiv.org/abs/2407.03215v1",
        "pub_date": "2024-07-03",
        "summary": "Data management is a critical component of modern experimental workflows. As\ndata generation rates increase, transferring data from acquisition servers to\nprocessing servers via conventional file-based methods is becoming increasingly\nimpractical. The 4D Camera at the National Center for Electron Microscopy\n(NCEM) generates data at a nominal rate of 480 Gbit/s (87,000 frames/s)\nproducing a 700 GB dataset in fifteen seconds. To address the challenges\nassociated with storing and processing such quantities of data, we developed a\nstreaming workflow that utilizes a high-speed network to connect the 4D\nCamera's data acquisition (DAQ) system to supercomputing nodes at the National\nEnergy Research Scientific Computing Center (NERSC), bypassing intermediate\nfile storage entirely. In this work, we demonstrate the effectiveness of our\nstreaming pipeline in a production setting through an hour-long experiment that\ngenerated over 10 TB of raw data, yielding high-quality datasets suitable for\nadvanced analyses. Additionally, we compare the efficacy of this streaming\nworkflow against the conventional file-transfer workflow by conducting a\npost-mortem analysis on historical data from experiments performed by real\nusers. Our findings show that the streaming workflow significantly improves\ndata turnaround time, enables real-time decision-making, and minimizes the\npotential for human error by eliminating manual user interactions.",
        "authors": "Samuel S. Welborn, Chris Harris, Stephanie M. Ribet, Georgios Varnavides, Colin Ophus, Bjoern Enders, Peter Ercius",
        "translated": "数据管理是现代实验工作流的重要组成部分。随着数据生成率的提高，通过传统的基于文件的方法将数据从采集服务器传输到处理服务器正变得越来越不切实际。美国国家电子显微镜中心(NCEM)的4D 相机以480Gbit/s (87000帧/s)的标称速率生成数据，在15秒内产生700GB 的数据集。为了解决存储和处理这样大量数据的挑战，我们开发了一个流式工作流，利用高速网络将4D 摄像机的数据采集(DAQ)系统连接到国家能源研究科学计算中心(NERSC)的超级计算节点，完全绕过中间文件存储。在这项工作中，我们通过一个小时的实验，生成了超过10TB 的原始数据，产生了适合高级分析的高质量数据集，从而证明了我们的流式流水线在生产环境中的有效性。此外，我们通过对实际用户实验中的历史数据进行事后分析，比较了该流工作流与传统文件传输工作流的效率。我们的研究结果表明，流式工作流可以显著提高数据周转时间，实现实时决策，并通过消除人工用户交互，最大限度地减少潜在的人为错误。"
    },
    {
        "title": "Effective Heterogeneous Federated Learning via Efficient\n  Hypernetwork-based Weight Generation",
        "url": "http://arxiv.org/abs/2407.03086v1",
        "pub_date": "2024-07-03",
        "summary": "While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86 times compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.",
        "authors": "Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim, JeongGil Ko",
        "translated": "虽然联邦学习利用分布式客户端资源，但是由于异构客户端功能，它面临着挑战。这就需要分配适合客户端资源的模型和仔细的参数聚合来适应这种异构性。我们提出了一种新的联邦学习框架 HypeMeFed，它通过将多出口网络结构与基于超网络的模型权重生成相结合来支持客户端的异构性。该方法对异构模型层的特征空间进行对齐，解决了权重聚合过程中各层的信息差异问题。为了实现 HypeMeFed，我们还提出了一种低秩因子分解方法，以最小化与超网络相关的计算和内存开销。我们对现实世界异构设备测试台的评估表明，与天真的超网络方法相比，HypeMeFed 比 FedAvg 提高了5.12% 的准确性，减少了98.22% 的超网络内存需求，并将其操作加速了1.86倍。这些结果证明了 HypeMeFed 在利用和吸引异构客户机进行联合学习方面的有效性。"
    },
    {
        "title": "On the Client Preference of LLM Fine-tuning in Federated Learning",
        "url": "http://arxiv.org/abs/2407.03038v1",
        "pub_date": "2024-07-03",
        "summary": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained\nlarge language model (LLM) using preference datasets, enabling the LLM to\ngenerate outputs that align with human preferences. Given the sensitive nature\nof these preference datasets held by various clients, there is a need to\nimplement RLHF within a federated learning (FL) framework, where clients are\nreluctant to share their data due to privacy concerns. To address this, we\nintroduce a feasible framework in which clients collaboratively train a binary\nselector with their preference datasets using our proposed FedBis. With a\nwell-trained selector, we can further enhance the LLM that generates\nhuman-preferred completions. Meanwhile, we propose a novel algorithm,\nFedBiscuit, that trains multiple selectors by organizing clients into balanced\nand disjoint clusters based on their preferences. Compared to the FedBis,\nFedBiscuit demonstrates superior performance in simulating human preferences\nfor pairwise completions. Our extensive experiments on federated human\npreference datasets -- marking the first benchmark to address heterogeneous\ndata partitioning among clients -- demonstrate that FedBiscuit outperforms\nFedBis and even surpasses traditional centralized training.",
        "authors": "Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Jing Gao",
        "translated": "强化学习反馈(rlHF)使用偏好数据集对预先训练好的大语言模型(LLM)进行微调，使 LLM 能够生成符合人类偏好的输出。鉴于这些偏好数据集由不同的客户端持有的敏感性质，有必要在联邦学习(FL)框架内实现 RLHF，其中客户端由于隐私问题不愿意共享他们的数据。为了解决这个问题，我们引入了一个可行的框架，在这个框架中，客户端使用我们提出的 FedBis 协作地训练一个二进制选择器和他们的偏好数据集。使用训练有素的选择器，我们可以进一步增强生成人类首选完成的 LLM。同时，我们提出了一种新的算法，FedBiscookie，通过将客户端根据他们的偏好组织成平衡的和不相交的集群来训练多个选择器。与 FedBis 相比，FedBiscookie 在模拟人类对配对完成的偏好方面表现出了优越的性能。我们在联邦人类偏好数据集上的广泛实验——标志着解决客户端之间异构数据分区的第一个基准——证明了 FedBiscue 的性能优于 FedBis，甚至超过了传统的集中式训练。"
    },
    {
        "title": "DRLQ: A Deep Reinforcement Learning-based Task Placement for Quantum\n  Cloud Computing",
        "url": "http://arxiv.org/abs/2407.02748v1",
        "pub_date": "2024-07-03",
        "summary": "The quantum cloud computing paradigm presents unique challenges in task\nplacement due to the dynamic and heterogeneous nature of quantum computation\nresources. Traditional heuristic approaches fall short in adapting to the\nrapidly evolving landscape of quantum computing. This paper proposes DRLQ, a\nnovel Deep Reinforcement Learning (DRL)-based technique for task placement in\nquantum cloud computing environments, addressing the optimization of task\ncompletion time and quantum task scheduling efficiency. It leverages the Deep Q\nNetwork (DQN) architecture, enhanced with the Rainbow DQN approach, to create a\ndynamic task placement strategy. This approach is one of the first in the field\nof quantum cloud resource management, enabling adaptive learning and\ndecision-making for quantum cloud environments and effectively optimizing task\nplacement based on changing conditions and resource availability. We conduct\nextensive experiments using the QSimPy simulation toolkit to evaluate the\nperformance of our method, demonstrating substantial improvements in task\nexecution efficiency and a reduction in the need to reschedule quantum tasks.\nOur results show that utilizing the DRLQ approach for task placement can\nsignificantly reduce total quantum task completion time by 37.81% to 72.93% and\nprevent task rescheduling attempts compared to other heuristic approaches.",
        "authors": "Hoa T. Nguyen, Muhammad Usman, Rajkumar Buyya",
        "translated": "由于量子计算资源的动态性和异构性，量子云计算范式在任务配置中提出了独特的挑战。传统的启发式方法不能适应量子计算的快速发展。本文提出了基于深度强化学习的量子云计算任务分配技术 DRLQ，解决了任务完成时间和量子任务调度效率的优化问题。它利用 Deep Q Network (DQN)体系结构，并通过 Rainbow DQN 方法得到了增强，从而创建了一个动态任务分配策略。这种方法是量子云资源管理领域的首创之一，能够为量子云环境提供在线机机器学习和决策支持，并根据不断变化的条件和资源可用性有效地优化任务分配。我们使用 QSimPy 模拟工具包进行了广泛的实验，以评估我们的方法的性能，证明了任务执行效率的实质性改进和重新调度量子任务的需求的减少。实验结果表明，与其他启发式方法相比，利用 DRLQ 方法进行任务分配可以显著减少任务完成总时间37.81% 到72.93% ，并且可以防止任务重调度尝试。"
    },
    {
        "title": "Accelerating Distributed Optimization: A Primal-Dual Perspective on\n  Local Steps",
        "url": "http://arxiv.org/abs/2407.02689v1",
        "pub_date": "2024-07-02",
        "summary": "In distributed machine learning, efficient training across multiple agents\nwith different data distributions poses significant challenges. Even with a\ncentralized coordinator, current algorithms that achieve optimal communication\ncomplexity typically require either large minibatches or compromise on gradient\ncomplexity. In this work, we tackle both centralized and decentralized settings\nacross strongly convex, convex, and nonconvex objectives. We first demonstrate\nthat a basic primal-dual method, (Accelerated) Gradient Ascent Multiple\nStochastic Gradient Descent (GA-MSGD), applied to the Lagrangian of distributed\noptimization inherently incorporates local updates, because the inner loops of\nrunning Stochastic Gradient Descent on the primal variable require no\ninter-agent communication. Notably, for strongly convex objectives, we show\n(Accelerated) GA-MSGD achieves linear convergence in communication rounds\ndespite the Lagrangian being only linear in the dual variables. This is due to\na unique structural property where the dual variable is confined to the span of\nthe coupling matrix, rendering the dual problem strongly concave. When\nintegrated with the Catalyst framework, our approach achieves nearly optimal\ncommunication complexity across various settings without the need for\nminibatches. Moreover, in stochastic decentralized problems, it attains\ncommunication complexities comparable to those in deterministic settings,\nimproving over existing algorithms.",
        "authors": "Junchi Yang, Murat Yildirim, Qiu Feng",
        "translated": "在分布式机器学习中，对具有不同数据分布的多代理进行有效的训练是一个巨大的挑战。即使有一个集中的协调器，当前的算法，以实现最佳的通信复杂度通常需要或者大的微型批量或梯度复杂度折衷。在这项工作中，我们处理集中和分散设置跨强烈凸，凸和非凸的目标。我们首先证明了一个基本的原始-对偶方法，(加速)梯度上升多重随机梯度下降(GA-MSGD) ，应用于分布式优化的拉格朗日方法，固有地结合了局部更新，因为在原始变量上运行的随机梯度下降的内部循环不需要代理之间的通信。值得注意的是，对于强凸目标，我们显示(加速) GA-MSGD 实现线性收敛的通信轮，尽管拉格朗日只是线性对偶变量。这是由于一个独特的结构性质，其中对偶变量限制在耦合矩阵的跨度，使对偶问题强烈凹。当与 Catalyst 框架集成时，我们的方法在不需要小批处理的情况下实现了几乎最佳的跨各种设置的通信复杂性。此外，在随机分散问题中，它获得了与确定性设置中的通信复杂度相当的通信复杂度，比现有算法有所改进。"
    },
    {
        "title": "Towards Federated Learning with On-device Training and Communication in\n  8-bit Floating Point",
        "url": "http://arxiv.org/abs/2407.02610v1",
        "pub_date": "2024-07-02",
        "summary": "Recent work has shown that 8-bit floating point (FP8) can be used for\nefficiently training neural networks with reduced computational overhead\ncompared to training in FP32/FP16. In this work, we investigate the use of FP8\ntraining in a federated learning context. This brings not only the usual\nbenefits of FP8 which are desirable for on-device training at the edge, but\nalso reduces client-server communication costs due to significant weight\ncompression. We present a novel method for combining FP8 client training while\nmaintaining a global FP32 server model and provide convergence analysis.\nExperiments with various machine learning models and datasets show that our\nmethod consistently yields communication reductions of at least 2.9x across a\nvariety of tasks and models compared to an FP32 baseline.",
        "authors": "Bokun Wang, Axel Berg, Durmus Alp Emre Acar, Chuteng Zhou",
        "translated": "最近的研究表明，与 FP32/FP16相比，8位浮点数(FP8)可以用来有效地训练神经网络，减少计算开销。在这项工作中，我们研究了 FP8训练在联邦学习环境中的应用。这不仅带来了 FP8的通常好处，这对边缘设备上的培训是可取的，而且由于显著的权重压缩，还降低了客户机-服务器通信成本。我们提出了一种新的方法，结合 FP8客户端训练，同时维护一个全球性的 FP32服务器模型，并提供了收敛性分析。对各种机器学习模型和数据集的实验表明，与 FP32基线相比，我们的方法在各种任务和模型之间始终产生至少2.9倍的通信减少。"
    },
    {
        "title": "Decentralized Intelligence Network (DIN)",
        "url": "http://arxiv.org/abs/2407.02461v1",
        "pub_date": "2024-07-02",
        "summary": "Decentralized Intelligence Network (DIN) addresses the significant challenges\nof data sovereignty and AI utilization caused by the fragmentation and siloing\nof data across providers and institutions. This comprehensive framework\novercomes access barriers to scalable data sources previously hindered by silos\nby leveraging: 1) personal data stores as a prerequisite for data sovereignty;\n2) a scalable federated learning protocol implemented on a public blockchain\nfor decentralized AI training, where data remains with participants and only\nmodel parameter updates are shared; and 3) a scalable, trustless rewards\nmechanism to incentivize participation and ensure fair reward distribution.\nThis framework ensures that no entity can prevent or control access to training\non data offered by participants or determine financial benefits, as these\nprocesses operate on a public blockchain with an immutable record and without a\nthird party. It supports effective AI training, allowing participants to\nmaintain control over their data, benefit financially, and contribute to a\ndecentralized, scalable ecosystem that leverages collective AI to develop\nbeneficial algorithms.",
        "authors": "Abraham Nash",
        "translated": "分散智能网(DIN)解决了数据主权和人工智能利用方面的重大挑战，这些挑战是由于数据在供应商和机构之间的碎片化和孤立性造成的。这个全面的框架通过利用以下方面克服了以前受到竖井阻碍的可扩展数据源的访问障碍: 1)个人数据存储作为数据主权的先决条件; 2)在公共区块链上实施的可扩展联合学习协议，用于分散的 AI 培训，其中数据保留在参与者身上，只有模型参数更新被共享; 3)可扩展的、不可信任的奖励机制，以激励参与并确保公平的奖励分配。这一框架确保任何实体都无法防止或控制获得参与者提供的数据培训或确定经济利益，因为这些过程是在公共区块链上运作的，具有不可变的记录，没有第三方。它支持有效的人工智能培训，允许参与者保持对他们的数据的控制，从经济上获益，并促进一个分散的，可扩展的生态系统，利用集体人工智能开发有益的算法。"
    },
    {
        "title": "Uncertainty-Aware Decarbonization for Datacenters",
        "url": "http://arxiv.org/abs/2407.02390v1",
        "pub_date": "2024-07-02",
        "summary": "This paper represents the first effort to quantify uncertainty in carbon\nintensity forecasting for datacenter decarbonization. We identify and analyze\ntwo types of uncertainty -- temporal and spatial -- and discuss their system\nimplications. To address the temporal dynamics in quantifying uncertainty for\ncarbon intensity forecasting, we introduce a conformal prediction-based\nframework. Evaluation results show that our technique robustly achieves target\ncoverages in uncertainty quantification across various significance levels. We\nconduct two case studies using production power traces, focusing on temporal\nand spatial load shifting respectively. The results show that incorporating\nuncertainty into scheduling decisions can prevent a 5% and 14% increase in\ncarbon emissions, respectively. These percentages translate to an absolute\nreduction of 2.1 and 10.4 tons of carbon emissions in a 20 MW datacenter\ncluster.",
        "authors": "Amy Li, Sihang Liu, Yi Ding",
        "translated": "本文首次尝试对数据中心脱碳过程中碳强度预测的不确定性进行量化。我们识别和分析了两种类型的不确定性——时间和空间——并讨论了它们的系统含义。为了解决碳强度预测不确定性量化的时间动态问题，我们引入了一个基于保形预测的框架。评价结果表明，该方法在不同的显著性水平上均能稳健地实现不确定性量化的目标覆盖。我们使用生产功率轨迹进行了两个案例研究，分别侧重于时间和空间负荷转移。结果表明，将不确定性纳入调度决策可以分别防止5% 和14% 的碳排放增加。这些百分比意味着在一个20兆瓦的数据中心集群中，碳排放绝对减少了2.1吨和10.4吨。"
    },
    {
        "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across\n  Hybrid Devices",
        "url": "http://arxiv.org/abs/2407.02327v1",
        "pub_date": "2024-07-02",
        "summary": "A number of production deep learning clusters have attempted to explore\ninference hardware for DNN training, at the off-peak serving hours with many\ninference GPUs idling. Conducting DNN training with a combination of\nheterogeneous training and inference GPUs, known as hybrid device training,\npresents considerable challenges due to disparities in compute capability and\nsignificant differences in memory capacity. We propose QSync, a training system\nthat enables efficient synchronous data-parallel DNN training over hybrid\ndevices by strategically exploiting quantized operators. According to each\ndevice's available resource capacity, QSync selects a quantization-minimized\nsetting for operators in the distributed DNN training graph, minimizing model\naccuracy degradation but keeping the training efficiency brought by\nquantization. We carefully design a predictor with a bi-directional\nmixed-precision indicator to reflect the sensitivity of DNN layers on\nfixed-point and floating-point low-precision operators, a replayer with a\nneighborhood-aware cost mapper to accurately estimate the latency of\ndistributed hybrid mixed-precision training, and then an allocator that\nefficiently synchronizes workers with minimized model accuracy degradation.\nQSync bridges the computational graph on PyTorch to an optimized backend for\nquantization kernel performance and flexible support for various GPU\narchitectures. Extensive experiments show that QSync's predictor can accurately\nsimulate distributed mixed-precision training with &lt;5% error, with a consistent\n0.27-1.03% accuracy improvement over the from-scratch training tasks compared\nto uniform precision.",
        "authors": "Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Yibo Zhu, Chuan Wu",
        "translated": "一些生产深度学习集群试图探索 DNN 培训的推理硬件，在非高峰服务时间与许多推理 GPU 闲置。用异构训练和推理 GPU (称为混合设备训练)组合进行 DNN 训练，由于计算能力的差异和内存容量的显著差异，提出了相当大的挑战。我们提出 QSync，一个训练系统，使有效的同步数据并行 DNN 训练超过混合设备的战略利用量化操作员。QSync 根据每个设备的可用资源容量，在分布式 DNN 训练图中为操作者选择一个量化最小化的设置，最小化模型精度的降低，同时保持量化带来的训练效率。我们精心设计了一个双向混合精度指标的预测器来反映 DNN 层对定点和浮点低精度算子的敏感性，一个具有邻域感知成本映射器的中继器来精确估计分布式混合精度训练的延迟，然后一个分配器来有效地同步工人，最小化模型精度退化。QSync 将 PyTorch 上的计算图连接到一个优化的后端，用于量化内核性能和对各种 GPU 架构的灵活支持。大量实验表明，QSync 预测器能够准确地模拟分布式混合精度训练，误差小于5% ，与均匀精度训练相比，准确率提高了0.27 -1.03% 。"
    },
    {
        "title": "Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models\n  with Adaptive Expert Placement",
        "url": "http://arxiv.org/abs/2407.04656v1",
        "pub_date": "2024-07-05",
        "summary": "Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly\nbeen adopted to further scale large language models (LLMs) due to its\nsub-linear scaling for computation costs. However, frequent failures still pose\nsignificant challenges as training scales. The cost of even a single failure is\nsignificant, as all GPUs need to wait idle until the failure is resolved,\npotentially losing considerable training progress as training has to restart\nfrom checkpoints. Existing solutions for efficient fault-tolerant training\neither lack elasticity or rely on building resiliency into pipeline\nparallelism, which cannot be applied to MoE models due to the expert\nparallelism strategy adopted by the MoE architecture.\n  We present Lazarus, a system for resilient and elastic training of MoE\nmodels. Lazarus adaptively allocates expert replicas to address the inherent\nimbalance in expert workload and speeds-up training, while a provably optimal\nexpert placement algorithm is developed to maximize the probability of recovery\nupon failures. Through adaptive expert placement and a flexible token\ndispatcher, Lazarus can also fully utilize all available nodes after failures,\nleaving no GPU idle. Our evaluation shows that Lazarus outperforms existing MoE\ntraining systems by up to 5.7x under frequent node failures and 3.4x on a real\nspot instance trace.",
        "authors": "Yongji Wu, Wenjie Qu, Tianyang Tao, Zhuang Wang, Wei Bai, Zhuohao Li, Yuan Tian, Jiaheng Zhang, Matthew Lentz, Danyang Zhuo",
        "translated": "稀疏激活的混合专家(MoE)体系结构由于其计算开销的次线性扩展性，越来越多地被用于进一步扩展大型语言模型(LLM)。然而，频繁的失败仍然给培训规模带来了巨大的挑战。即使是单个故障的代价也是巨大的，因为所有的 GPU 都需要等待空闲，直到故障得到解决，这可能会失去相当大的训练进度，因为训练必须从检查点重新开始。现有的高效容错培训解决方案要么缺乏弹性，要么依赖于将弹性构建为流水线并行性，由于教育部体系结构采用的专家并行策略，这些解决方案不能应用于教育部模型。我们提出 Lazarus，一个系统的弹性和弹性训练的 MoE 模型。Lazarus 自适应地分配专家副本，以解决专家工作量固有的不平衡问题，加快训练速度，同时开发了一个可证明的最优专家配置算法，以最大限度地提高故障恢复的概率。通过自适应专家布局和灵活的令牌调度器，Lazarus 还可以在故障后充分利用所有可用的节点，不让 GPU 处于空闲状态。我们的评估表明，Lazarus 在频繁节点故障下的性能比现有的 MoE 培训系统高出5.7倍，在实际现场实例跟踪上的性能高出3.4倍。"
    },
    {
        "title": "Accelerating Communication in Deep Learning Recommendation Model\n  Training with Dual-Level Adaptive Lossy Compression",
        "url": "http://arxiv.org/abs/2407.04272v1",
        "pub_date": "2024-07-05",
        "summary": "DLRM is a state-of-the-art recommendation system model that has gained\nwidespread adoption across various industry applications. The large size of\nDLRM models, however, necessitates the use of multiple devices/GPUs for\nefficient training. A significant bottleneck in this process is the\ntime-consuming all-to-all communication required to collect embedding data from\nall devices. To mitigate this, we introduce a method that employs error-bounded\nlossy compression to reduce the communication data size and accelerate DLRM\ntraining. We develop a novel error-bounded lossy compression algorithm,\ninformed by an in-depth analysis of embedding data features, to achieve high\ncompression ratios. Moreover, we introduce a dual-level adaptive strategy for\nerror-bound adjustment, spanning both table-wise and iteration-wise aspects, to\nbalance the compression benefits with the potential impacts on accuracy. We\nfurther optimize our compressor for PyTorch tensors on GPUs, minimizing\ncompression overhead. Evaluation shows that our method achieves a 1.38$\\times$\ntraining speedup with a minimal accuracy impact.",
        "authors": "Hao Feng, Boyuan Zhang, Fanjiang Ye, Min Si, Ching-Hsiang Chu, Jiannan Tian, Chunxing Yin,  Zhaoxia,  Deng, Yuchen Hao, Pavan Balaji, Tong Geng, Dingwen Tao",
        "translated": "DLRM 是最先进的推荐系统模型，已经在各种行业应用程序中得到广泛采用。然而，DLRM 模型的大规模需要使用多个设备/GPU 进行有效的培训。这个过程中的一个重要瓶颈是从所有设备收集嵌入数据所需的耗时的全对全通信。为了缓解这种情况，我们引入了一种使用错误限制有损数据压缩的方法，以减少通信数据量并加速 DLRM 培训。我们开发了一个新的错误限制有损数据压缩算法，通过深入分析嵌入数据特征，以实现高压缩比。此外，我们还引入了一个跨越表和迭代两个方面的误差范围调整的双层自适应策略，以平衡压缩优势和对精度的潜在影响。我们在 GPU 上进一步优化我们的 PyTorch 张量压缩器，使压缩开销最小化。评估结果表明，该方法在精度影响最小的情况下，获得了1.38美元的训练加速比。"
    },
    {
        "title": "A High-Quality Workflow for Multi-Resolution Scientific Data Reduction\n  and Visualization",
        "url": "http://arxiv.org/abs/2407.04267v1",
        "pub_date": "2024-07-05",
        "summary": "Multi-resolution methods such as Adaptive Mesh Refinement (AMR) can enhance\nstorage efficiency for HPC applications generating vast volumes of data.\nHowever, their applicability is limited and cannot be universally deployed\nacross all applications. Furthermore, integrating lossy compression with\nmulti-resolution techniques to further boost storage efficiency encounters\nsignificant barriers. To this end, we introduce an innovative workflow that\nfacilitates high-quality multi-resolution data compression for both uniform and\nAMR simulations. Initially, to extend the usability of multi-resolution\ntechniques, our workflow employs a compression-oriented Region of Interest\n(ROI) extraction method, transforming uniform data into a multi-resolution\nformat. Subsequently, to bridge the gap between multi-resolution techniques and\nlossy compressors, we optimize three distinct compressors, ensuring their\noptimal performance on multi-resolution data. Lastly, we incorporate an\nadvanced uncertainty visualization method into our workflow to understand the\npotential impacts of lossy compression. Experimental evaluation demonstrates\nthat our workflow achieves significant compression quality improvements.",
        "authors": "Daoce Wang, Pascal Grosset, Jesus Pulido, Tushar M. Athawale, Jiannan Tian, Kai Zhao, Zarija Lukic, Axel Huebl, Zhe Wang, James Ahrens, Dingwen Tao",
        "translated": "自适应网格细化(AMR)等多分辨率方法可以提高 HPC 应用程序生成大量数据的存储效率。但是，它们的适用性是有限的，并且不能在所有应用程序中普遍部署。此外，将有损数据压缩与多分辨率技术相结合以进一步提高存储效率会遇到重大障碍。为此，我们引入了一个创新的工作流程，为统一模拟和 AMR 模拟提供高质量的多分辨率数据压缩。首先，为了扩展多分辨率技术的可用性，我们的工作流采用了面向压缩的感兴趣区域(ROI)提取方法，将统一的数据转换成多分辨率格式。随后，为了弥补多分辨率技术和有损压缩器之间的差距，我们对三种不同的压缩器进行了优化，以确保它们在多分辨率数据上的最优性能。最后，我们将一种先进的不确定性可视化方法融入到我们的工作流程中，以了解有损数据压缩的潜在影响。实验结果表明，我们的工作流程实现了显著的压缩质量改进。"
    },
    {
        "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
        "url": "http://arxiv.org/abs/2407.04053v1",
        "pub_date": "2024-07-04",
        "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyse data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. The goal of Edge AI is to optimize data processing efficiency\nand velocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research, spanning from 2014 to the present, it has\nshown significant and rapid development over the last five years. In this\narticle, we present a systematic literature review for Edge AI to discuss the\nexisting research, recent advancements, and future research directions. We\ncreated a collaborative edge AI learning system for cloud and edge computing\nanalysis, including an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while also examining its potential influence\nacross many fields through compassing infrastructure, cloud computing, fog\ncomputing, services, use cases, ML and deep learning, and resource management.\nThis study highlights the significance of Edge AI in processing real-time data\nat the edge of the network. Additionally, it emphasizes the research challenges\nencountered by Edge AI systems, including constraints on resources,\nvulnerabilities to security threats, and problems with scalability. Finally,\nthis study highlights the potential future research directions that aim to\naddress the current limitations of Edge AI by providing innovative solutions.",
        "authors": "Sukhpal Singh Gill, Muhammed Golec, Jianmin Hu, Minxian Xu, Junhui Du, Huaming Wu, Guneet Kaur Walia, Subramaniam Subramanian Murugesan, Babar Ali, Mohit Kumar, Kejiang Ye, Prabal Verma, Surendra Kumar, Felix Cuadrado, Steve Uhlig",
        "translated": "边缘人工智能(AI)集成了一个互联系统和设备的网络，这些设备接收、缓存、处理和分析数据，与用 AI 技术捕获数据的位置进行密切通信。人工智能效率的最新进展，物联网(IoT)设备的广泛使用，以及边缘计算的出现，已经开启了边缘人工智能的巨大范围。边缘人工智能的目标是优化数据处理效率和速度，同时确保数据的机密性和完整性。尽管从2014年到现在，它还是一个相对较新的研究领域，但在过去的五年里，它已经显示出重大而迅速的发展。在本文中，我们提出了一个系统的文献综述，边缘人工智能讨论现有的研究，最近的进展和未来的研究方向。我们创建了一个用于云和边缘计算分析的协作式边缘 AI 学习系统，包括对促进这种机制的体系结构的深入研究。边缘人工智能的分类促进了边缘人工智能系统的分类和配置，同时也通过包括基础设施、云计算、雾计算、服务、用例、机器学习和深度学习以及资源管理在许多领域的潜在影响。该研究突出了边缘人工智能在处理网络边缘实时数据中的重要性。此外，它强调了边缘人工智能系统所面临的研究挑战，包括资源约束、安全威胁的脆弱性和可伸缩性问题。最后，本研究强调了未来可能的研究方向，旨在通过提供创新的解决方案来解决边缘人工智能目前的局限性。"
    },
    {
        "title": "Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM\n  Inference on Heterogeneous Systems",
        "url": "http://arxiv.org/abs/2407.04014v1",
        "pub_date": "2024-07-04",
        "summary": "The rapid adoption of large language models (LLMs) has led to significant\nadvances in natural language processing and text generation. However, the\nenergy consumed through LLM model inference remains a major challenge for\nsustainable AI deployment. To address this problem, we model the\nworkload-dependent energy consumption and runtime of LLM inference tasks on\nheterogeneous GPU-CPU systems. By conducting an extensive characterization\nstudy of several state-of-the-art LLMs and analyzing their energy and runtime\nbehavior across different magnitudes of input prompts and output text, we\ndevelop accurate (R^2&gt;0.96) energy and runtime models for each LLM. We employ\nthese models to explore an offline, energy-optimal LLM workload scheduling\nframework. Through a case study, we demonstrate the advantages of energy and\naccuracy aware scheduling compared to existing best practices.",
        "authors": "Grant Wilkins, Srinivasan Keshav, Richard Mortier",
        "translated": "大型语言模型(LLM)的迅速应用导致了自然语言处理和文本生成方面的重大进展。然而，通过 LLM 模型推理所消耗的能量仍然是可持续人工智能部署的一个主要挑战。为了解决这个问题，我们在异构 GPU-CPU 系统上建立了与工作负载相关的 LLM 推理任务的能量消耗和运行时模型。通过对几种最先进的 LLM 进行广泛的角色塑造研究，并分析它们在不同大小的输入提示和输出文本中的能量和运行时行为，我们为每种 LLM 开发了精确的(R ^ 2 > 0.96)能量和运行时模型。我们使用这些模型来探索一个离线的，能量最优的 LLM 工作负载调度框架。通过一个案例研究，我们证明了与现有的最佳实践相比，能源和精确性意识调度的优势。"
    },
    {
        "title": "PaSE: Parallelization Strategies for Efficient DNN Training",
        "url": "http://arxiv.org/abs/2407.04001v1",
        "pub_date": "2024-07-04",
        "summary": "Training a deep neural network (DNN) requires substantial computational and\nmemory requirements. It is common to use multiple devices to train a DNN to\nreduce the overall training time. There are several choices to parallelize each\nlayer in a DNN. Exhaustively searching this list to find an optimal\nparallelization strategy is prohibitively time consuming and impractical. The\nstandard practice is to use data parallelism because of its simplicity.\nHowever, data parallelism is often sub-optimal, and suffers from poor\nperformance and high memory requirement. Expert-designed strategies have been\nproposed on a case-by-case basis using domain specific knowledge. These\nexpert-designed strategies do not generalize well to DNNs other than the ones\nfor which they were designed, and are not always necessarily the best choice.\n  In this paper, we propose an approach to automatically find efficient\nparallelization strategies for DNNs from their computation graphs. We present\nan efficient algorithm to compute these strategies within a reasonable time in\npractice. We evaluate the effectiveness of our approach on various DNNs. We\nalso compare the performance of the strategies identified by our approach\nagainst data parallelism, expert-designed strategies, and the state-of-the-art\napproaches. Our results show that the strategies found using our approach\noutperform the baseline data parallelism strategy in all the cases. In\naddition, our strategies achieve better performance than the expert-designed\nstrategies and the state-of-the-art approaches.",
        "authors": "Venmugil Elango",
        "translated": "训练一个深层神经网络(DNN)需要大量的计算和内存需求。通常使用多种设备来训练一个 DNN，以减少整体训练时间。有几种选择可以并行化 DNN 中的每一层。彻底搜索此列表以找到最佳并行策略是非常耗时和不切实际的。标准做法是使用资料平行，因为它很简单。然而，资料平行往往是次优的，并且存在性能差和内存需求高的问题。利用具体领域的知识，逐案提出了专家设计的战略。这些专家设计的策略并不能很好地推广到 DNN 以外的其他设计策略，并不一定总是最佳选择。本文提出了一种从 DNN 的计算图中自动寻找有效并行策略的方法。我们提出了一个有效的算法，在合理的时间内计算这些策略在实践中。我们评估我们的方法在各种 DNN 上的有效性。我们还将我们的方法与资料平行、专家设计的策略和最先进的方法进行了比较。我们的研究结果表明，在所有情况下，使用我们的方法发现的策略都优于基线资料平行策略。此外，我们的策略比专家设计的策略和最先进的方法取得了更好的效果。"
    },
    {
        "title": "DEVS/SOA: A Cross-Platform Framework for Net-centric Modeling and\n  Simulation in DEVS Unified Process",
        "url": "http://arxiv.org/abs/2407.03686v1",
        "pub_date": "2024-07-04",
        "summary": "Discrete EVent Specification (DEVS) environments are known to be implemented\nover middleware systems such as HLA, RMI, CORBA and others. DEVS exhibits\nconcepts of systems theory and modeling and supports capturing the system\nbehavior from the physical and behavioral perspectives. Further, they are\nimplemented using Object-oriented languages like Java and C++. This research\nwork uses the Java platform to implement DEVS over a Service Oriented\nArchitecture (SOA) framework. Called the DEVS/SOA, the framework supports a\ndevelopment and testing environment known as DEVS Unified Process that is built\non a model-continuity-based life cycle methodology. DEVS Unified Process allows\nDEVS-based Modeling and Simulation (M&amp;S) over net-centric platforms using\nDEVS/SOA. This framework also provides the crucial feature of run-time\ncomposability of coupled systems using SOA. We describe the architecture and\ndesigns of the both the server and the client. The client application\ncommunicates with multiple servers hosting DEVS simulation services. These\nSimulation services are developed using the proposed symmetrical services\narchitecture wherein the server can act as both a service provider and a\nservice consumer contrary to the unidirectional client-server paradigm. We also\ndiscuss how this Services based architecture provides solutions for\ncross-platform distributed M&amp;S. We demonstrate DEVS/SOA framework with a\nscenario of Joint Close Air Support specified in Business Process Modeling\nNotation (BPMN). We also provide a real-world application of Network health\nmonitoring using DEVS/SOA layered architectural framework.",
        "authors": "Saurabh Mittal, José L. Risco-Martín, Bernard P. Zeigler",
        "translated": "众所周知，离散事件规范(DevS)环境是在 HLA、 RMI、 CORBA 等中间件系统上实现的。DevS 展示了系统理论和建模的概念，并支持从物理和行为角度捕获系统行为。此外，它们是使用面向对象的语言(如 Java 和 C + +)实现的。这项研究工作使用 Java 平台在一个面向服务的体系结构(SOA)框架下实现 DEVS。该框架被称为 devS/SOA，它支持一个名为 devS 统一过程的开发和测试环境，该环境建立在基于模型连续性的生命周期方法之上。在以网络为中心的平台上使用 devS/SOA，devS 统一过程允许基于 devS 的建模与模拟(M & S)。此框架还提供了使用 SOA 的耦合系统的运行时可组合性的关键特性。我们描述了服务器和客户端的体系结构和设计。客户端应用程序与多个承载 DevS 模拟服务的服务器通信。这些仿真服务是使用所提出的对称服务体系结构开发的，其中服务器既可以充当服务提供者，也可以充当服务使用者，这与单向客户机-服务器模式相反。我们还讨论了这种基于服务的体系结构如何为跨平台分布式 M & S 提供解决方案。我们用业务流程建模标记法中指定的联合近距离空中支援(bPMN)场景来演示 devS/SOA 框架。我们还提供了一个使用 DevS/SOA 分层体系结构框架的实际网络健康监控应用程序。"
    },
    {
        "title": "Loki: A System for Serving ML Inference Pipelines with Hardware and\n  Accuracy Scaling",
        "url": "http://arxiv.org/abs/2407.03583v1",
        "pub_date": "2024-07-04",
        "summary": "The rapid adoption of machine learning (ML) has underscored the importance of\nserving ML models with high throughput and resource efficiency. Traditional\napproaches to managing increasing query demands have predominantly focused on\nhardware scaling, which involves increasing server count or computing power.\nHowever, this strategy can often be impractical due to limitations in the\navailable budget or compute resources. As an alternative, accuracy scaling\noffers a promising solution by adjusting the accuracy of ML models to\naccommodate fluctuating query demands. Yet, existing accuracy scaling\ntechniques target independent ML models and tend to underperform while managing\ninference pipelines. Furthermore, they lack integration with hardware scaling,\nleading to potential resource inefficiencies during low-demand periods. To\naddress the limitations, this paper introduces Loki, a system designed for\nserving inference pipelines effectively with both hardware and accuracy\nscaling. Loki incorporates an innovative theoretical framework for optimal\nresource allocation and an effective query routing algorithm, aimed at\nimproving system accuracy and minimizing latency deadline violations. Our\nempirical evaluation demonstrates that through accuracy scaling, the effective\ncapacity of a fixed-size cluster can be enhanced by more than $2.7\\times$\ncompared to relying solely on hardware scaling. When compared with\nstate-of-the-art inference-serving systems, Loki achieves up to a $10\\times$\nreduction in Service Level Objective (SLO) violations, with minimal compromises\non accuracy and while fulfilling throughput demands.",
        "authors": "Sohaib Ahmad, Hui Guan, Ramesh K. Sitaraman",
        "translated": "机器学习(ML)的快速应用突出了为机器学习模型提供高吞吐量和资源效率服务的重要性。管理日益增长的查询需求的传统方法主要集中在硬件扩展上，这涉及到增加服务器数量或计算能力。但是，由于可用预算或计算资源的限制，这种策略通常不切实际。作为一种替代方案，精度缩放提供了一个有希望的解决方案，通过调整机器学习模型的精度，以适应波动的查询需求。然而，现有的精度缩放技术针对的是独立的机器学习模型，在管理推理流水线时往往表现不佳。此外，它们缺乏与硬件扩展的集成，导致在低需求时期潜在的资源低效。针对这些局限性，本文介绍了 Loki 系统，该系统设计用于有效地为推理管道提供硬件和精度扩展。Loki 采用了一个创新的资源优化分配理论框架和一个有效的查询路由算法，旨在提高系统的准确性和最大限度地减少延迟最后期限违规。我们的实证评估表明，通过精度扩展，固定规模集群的有效容量可以比单纯依靠硬件扩展提高2.7倍以上。与最先进的推理服务系统相比，Loki 在满足吞吐量需求的同时，在准确性方面做出最小的妥协，从而在服务水平目标(SLO)违规方面实现了高达10倍的减少。"
    },
    {
        "title": "A multigrid reduction framework for domains with symmetries",
        "url": "http://arxiv.org/abs/2407.05930v1",
        "pub_date": "2024-07-08",
        "summary": "Divergence constraints are present in the governing equations of many\nphysical phenomena, and they usually lead to a Poisson equation whose solution\ntypically is the main bottleneck of many simulation codes. Algebraic Multigrid\n(AMG) is arguably the most powerful preconditioner for Poisson's equation, and\nits effectiveness results from the complementary roles played by the smoother,\nresponsible for damping high-frequency error components, and the coarse-grid\ncorrection, which in turn reduces low-frequency modes. This work presents\nseveral strategies to make AMG more compute-intensive by leveraging reflection,\ntranslational and rotational symmetries, often present in academic and\nindustrial configurations. The best-performing method, AMGR, is based on a\nmultigrid reduction framework that introduces an aggressive coarsening to the\nmultigrid hierarchy, reducing the memory footprint, setup and application costs\nof the top-level smoother. While preserving AMG's excellent convergence, AMGR\nallows replacing the standard sparse matrix-vector product with the more\ncompute-intensive sparse matrix-matrix product, yielding significant\naccelerations. Numerical experiments on industrial CFD applications\ndemonstrated up to 70% speed-ups when solving Poisson's equation with AMGR\ninstead of AMG. Additionally, strong and weak scalability analyses revealed no\nsignificant degradation.",
        "authors": "Àdel Alsalti-Baldellou, Carlo Janna, Xavier Álvarez-Farré, F. Xavier Trias",
        "translated": "发散约束存在于许多物理现象的控制方程中，它们通常导致一个泊松方程，而泊松方程的求解通常是许多仿真程序的主要瓶颈。代数多重网格(AMG)可以说是泊松方程最强大的预处理器，其有效性源于平滑器(负责阻尼高频误差分量)和粗网格校正(反过来减少低频模式)的互补作用。这项工作提出了几个策略，使 AMG 更加计算密集型的利用反射，平移和旋转对称性，往往存在于学术和工业配置。最好的方法，AMGR，是基于一个多网格缩减框架，引入了一个积极的粗化多网格层次结构，减少内存占用，设置和应用成本的顶级平滑。在保持 AMG 的优良收敛性的同时，AMGR 允许用计算量更大的稀疏矩阵向量积取代标准的稀疏矩阵向量积，从而产生显著的加速度。工业 CFD 应用的数值实验表明，用 AMGR 代替 AMG 求解泊松方程可以提高70% 的速度。此外，强可伸缩性和弱可伸缩性分析显示没有明显的降解。"
    },
    {
        "title": "DFedSat: Communication-Efficient and Robust Decentralized Federated\n  Learning for LEO Satellite Constellations",
        "url": "http://arxiv.org/abs/2407.05850v1",
        "pub_date": "2024-07-08",
        "summary": "Low Earth Orbit (LEO) satellites play a crucial role in the development of 6G\nmobile networks and space-air-ground integrated systems. Recent advancements in\nspace technology have empowered LEO satellites with the capability to run AI\napplications. However, centralized approaches, where ground stations (GSs) act\nas servers and satellites as clients, often encounter slow convergence and\ninefficiencies due to intermittent connectivity between satellites and GSs. In\ncontrast, decentralized federated learning (DFL) offers a promising alternative\nby facilitating direct communication between satellites (clients) via\ninter-satellite links (ISLs). However, inter-plane ISLs connecting satellites\nfrom different orbital planes are dynamic due to Doppler shifts and pointing\nlimitations. This could impact model propagation and lead to slower\nconvergence. To mitigate these issues, we propose DFedSat, a fully\ndecentralized federated learning framework tailored for LEO satellites. DFedSat\naccelerates the training process by employing two adaptive mechanisms for\nintra-plane and inter-plane model aggregation, respectively. Furthermore, a\nself-compensation mechanism is integrated to enhance the robustness of\ninter-plane ISLs against transmission failure. Additionally, we derive the\nsublinear convergence rate for the non-convex case of DFedSat. Extensive\nexperimental results demonstrate DFedSat's superiority over other DFL baselines\nregarding convergence rate, communication efficiency, and resilience to\nunreliable links.",
        "authors": "Minghao Yang, Jingjing Zhang, Shengyun Liu",
        "translated": "近地轨道(LEO)卫星在6G 移动网络和空地一体化系统的发展中起着至关重要的作用。空间技术的最新进展使低地轨道卫星具有运行人工智能应用的能力。然而，由于地面站(GS)作为服务器，卫星作为客户端，集中式方法经常遇到收敛缓慢和效率低下的问题，这是由于卫星和 GS 之间的间歇性连接。相比之下，分散联邦学习(DFL)通过促进卫星(客户)之间通过卫星间链路(ISL)的直接通信，提供了一个有前途的替代方案。然而，由于多普勒频移和指向限制，从不同轨道平面连接卫星的平面间 ISL 是动态的。这可能会影响模型的传播，导致收敛速度较慢。为了缓解这些问题，我们提出了 DFedSat，一个为低轨道卫星量身定制的完全分散的联邦学习框架。DFedSat 采用两种自适应机制分别对平面内和平面间模型进行聚合，从而加速了训练过程。此外，自我补偿机制的集成，以增强平面间 ISL 对传输故障的鲁棒性。此外，我们还推导了 DFedSat 非凸情形下的次线性收敛速度。大量的实验结果表明，DFedSat 在收敛速度、通信效率和对不可靠链路的恢复能力方面优于其他 DFL 基线。"
    },
    {
        "title": "Cyber Physical Games",
        "url": "http://arxiv.org/abs/2407.05817v1",
        "pub_date": "2024-07-08",
        "summary": "We describe a formulation of multi-agents operating within a Cyber-Physical\nSystem, resulting in collaborative or adversarial games. We show that the\nnon-determinism inherent in the communication medium between agents and the\nunderlying physical environment gives rise to environment evolution that is a\nprobabilistic function of agents' strategies. We name these emergent properties\nCyber Physical Games and study its properties. We present an algorithmic model\nthat determines the most likely system evolution, approximating Cyber Physical\nGames through Probabilistic Finite State Automata, and evaluate it on\ncollaborative and adversarial versions of the Iterated Boolean Game, comparing\ntheoretical results with simulated ones. Results support the validity of the\nproposed model, and suggest several required research directions to continue\nevolving our understanding of Cyber Physical System, as well as how to best\ndesign agents that must operate within such environments.",
        "authors": "Warisa Sritriratanarak, Paulo Garcia",
        "translated": "我们描述了在一个网宇实体系统中运行的多代理的公式，导致了合作或对抗性的游戏。我们指出，代理人与底层物理环境之间的交流媒介所固有的非决定性导致了环境的演化，而环境演化是代理人策略的一个概率函数。我们命名这些涌现性质赛博物理游戏和研究其性质。我们提出了一个算法模型，确定最有可能的系统演化，近似网络物理游戏通过概率有限状态自动机，并评估它的协作和对手版本的迭代布尔游戏，比较理论结果与模拟的。结果支持提出的模型的有效性，并建议几个必要的研究方向，以继续发展我们对网络物理系统的理解，以及如何最佳设计代理必须在这样的环境中操作。"
    },
    {
        "title": "FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep\n  Reinforcement Learning for Medical Imaging",
        "url": "http://arxiv.org/abs/2407.05800v1",
        "pub_date": "2024-07-08",
        "summary": "Despite recent advancements in federated learning (FL) for medical image\ndiagnosis, addressing data heterogeneity among clients remains a significant\nchallenge for practical implementation. A primary hurdle in FL arises from the\nnon-IID nature of data samples across clients, which typically results in a\ndecline in the performance of the aggregated global model. In this study, we\nintroduce FedMRL, a novel federated multi-agent deep reinforcement learning\nframework designed to address data heterogeneity. FedMRL incorporates a novel\nloss function to facilitate fairness among clients, preventing bias in the\nfinal global model. Additionally, it employs a multi-agent reinforcement\nlearning (MARL) approach to calculate the proximal term $(\\mu)$ for the\npersonalized local objective function, ensuring convergence to the global\noptimum. Furthermore, FedMRL integrates an adaptive weight adjustment method\nusing a Self-organizing map (SOM) on the server side to counteract distribution\nshifts among clients' local data distributions. We assess our approach using\ntwo publicly available real-world medical datasets, and the results demonstrate\nthat FedMRL significantly outperforms state-of-the-art techniques, showing its\nefficacy in addressing data heterogeneity in federated learning. The code can\nbe found here~{\\url{https://github.com/Pranabiitp/FedMRL}}.",
        "authors": "Pranab Sahoo, Ashutosh Tripathi, Sriparna Saha, Samrat Mondal",
        "translated": "尽管联邦学习(FL)在医学图像诊断方面取得了最新进展，但是解决客户端之间的数据异构性仍然是实际应用中的一个重大挑战。FL 中的一个主要障碍来自跨客户端的数据样本的非 IID 性质，这通常会导致聚合全局模型的性能下降。在这项研究中，我们介绍了 FedMRL，一个新的联邦多代理深度强化学习框架，旨在解决数据异构性问题。FedMRL 采用了一种新的损失函数，以促进客户之间的公平性，防止最终全球模型的偏见。此外，它还采用多智能体强化学习(MARL)方法计算个性化局部目标函数的近似项 $(mu) $，确保收敛到全局最优。此外，FedMRL 整合了一个自适应权重调整方法，在服务器端使用自组织映射(SOM)来抵消客户本地数据分布之间的分布变化。我们使用两个公开可用的真实世界医学数据集评估我们的方法，结果表明 FedMRL 显着优于最先进的技术，显示其在解决联邦学习中的数据异质性方面的功效。代码可以在这里找到 ~ { url { https://github.com/pranabiitp/fedmrl }}。"
    },
    {
        "title": "Ten Years of Teaching Empirical Software Engineering in the context of\n  Energy-efficient Software",
        "url": "http://arxiv.org/abs/2407.05689v1",
        "pub_date": "2024-07-08",
        "summary": "In this chapter we share our experience in running ten editions of the Green\nLab course at the Vrije Universiteit Amsterdam, the Netherlands. The course is\ngiven in the Software Engineering and Green IT track of the Computer Science\nMaster program of the VU. The course takes place every year over a 2-month\nperiod and teaches Computer Science students the fundamentals of Empirical\nSoftware Engineering in the context of energy-efficient software. The\npeculiarity of the course is its research orientation: at the beginning of the\ncourse the instructor presents a catalog of scientifically relevant goals, and\neach team of students signs up for one of them and works together for 2 months\non their own experiment for achieving the goal. Each team goes over the classic\nsteps of an empirical study, starting from a precise formulation of the goal\nand research questions to context definition, selection of experimental\nsubjects and objects, definition of experimental variables, experiment\nexecution, data analysis, and reporting. Over the years, the course became\nwell-known within the Software Engineering community since it led to several\nscientific studies that have been published at various scientific conferences\nand journals. Also, students execute their experiments using\n\\textit{open-source tools}, which are developed and maintained by researchers\nand other students within the program, thus creating a virtuous community of\nlearners where students exchange ideas, help each other, and learn how to\ncollaboratively contribute to open-source projects in a safe environment.",
        "authors": "Ivano Malavolta, Vincenzo Stoico, Patricia Lago",
        "translated": "在本章中，我们将分享我们在荷兰阿姆斯特丹阿姆斯特丹自由大学举办十个版本的绿色实验课程的经验。该课程是在软件工程和绿色信息技术轨道的计算机科学硕士课程的 VU。本课程每年举行，为期两个月，教授计算机科学专业的学生经验软件工程的基础知识。这门课程的特殊之处在于它的研究方向: 在课程开始的时候，讲师会提出一系列与科学相关的目标，每组学生报名参加其中的一个，然后一起工作2个月，进行他们自己的实验，以达到这个目标。每个研究小组都要复习经典的实证研究步骤，从目标和研究问题的精确表述开始，到背景定义、实验对象和对象的选择、实验变量的定义、实验执行、数据分析和报告。多年来，这门课程在软件工程社区内广为人知，因为它导致了一些已经在各种科学会议和期刊上发表的科学研究。此外，学生使用文本{开源工具}执行他们的实验，这是由研究人员和其他学生在项目中开发和维护的，从而创造了一个良性的学习者社区，在这里学生交流思想，互相帮助，并学习如何在安全的环境中协作贡献开源项目。"
    },
    {
        "title": "Computational Power of Mobile Robots in Synchronous Environment:\n  Discrete Version",
        "url": "http://arxiv.org/abs/2407.05678v1",
        "pub_date": "2024-07-08",
        "summary": "In distributed computing by mobile robots, robots are deployed over a region,\ncontinuous or discrete, operating through a sequence of\n\\textit{look-compute-move} cycles. An extensive study has been carried out to\nunderstand the computational powers of different robot models. The models vary\non the ability to 1)~remember constant size information and 2)~communicate\nconstant size message. Depending on the abilities the different models are\n1)~$\\mathcal{OBLOT}$ (robots are oblivious and silent), 2)~$\\mathcal{FSTA}$\n(robots have finite states but silent), 3)~$\\mathcal{FCOM}$ (robots are\noblivious but can communicate constant size information) and,\n4)~$\\mathcal{LUMI}$ (robots have finite states and can communicate constant\nsize information). Another factor that affects computational ability is the\nscheduler that decides the activation time of the robots. The main three\nschedulers are \\textit{fully-synchronous}, \\textit{semi-synchronous} and\n\\textit{asynchronous}. Combining the models ($M$) with schedulers ($K$), we\nhave twelve combinations $M^K$.\n  In the euclidean domain, the comparisons between these twelve variants have\nbeen done in different works for transparent robots, opaque robots, and robots\nwith limited visibility. There is a vacant space for similar works when robots\nare operating on discrete regions like networks. It demands separate research\nattention because there have been a series of works where robots operate on\ndifferent networks, and there is a fundamental difference when robots are\noperating on a continuous domain versus a discrete domain in terms of robots'\nmovement. This work contributes to filling the space by giving a full\ncomparison table for all models with two synchronous schedulers:\nfully-synchronous and semi-synchronous.",
        "authors": "Avisek Sharma, Pritam Goswami, Buddhadeb Sau",
        "translated": "在移动机器人的分布式计算中，机器人被部署在一个连续或离散的区域，按照一系列的文本{ look-computer-move }周期进行操作。为了理解不同机器人模型的计算能力，人们进行了广泛的研究。这些模型的不同之处在于能够1) ~ 记住固定大小的信息和2) ~ 传递固定大小的信息。根据不同的能力，不同的模型有: 1) ~ $数学{ OBLOT } $(机器人是无意识的和沉默的) ，2) ~ $数学{ FSTA } $(机器人有有限的状态但是沉默的) ，3) ~ $数学{ FCOM } $(机器人是无意识的但是可以传递常数大小的信息) ，4) ~ $数学{ LUMI } $(机器人有有限的状态并且可以传递常数大小的信息)。影响计算能力的另一个因素是决定机器人激活时间的调度器。主要的三个调度程序分别是 texttit {全同步}、 texttit {半同步}和 texttit {异步}。结合模型($m $)和调度器($k $) ，我们有十二种组合 $m ^ k $。在欧几里得整环中，这十二个变种的比较已经在透明机器人、不透明机器人和能见度有限的机器人的不同作品中完成。当机器人在离散的区域(如网络)上操作时，类似的工作有一个空位。它需要单独的研究注意力，因为已经有一系列的工作机器人操作在不同的网络，有一个根本的区别，当机器人操作在一个连续的领域与一个离散的领域就机器人的运动而言。这项工作有助于填补空间，给出了一个完整的比较表的所有模型与两个同步调度器: 完全同步和半同步。"
    },
    {
        "title": "A Blockchain Embedded Peer-to-Peer Access Control Framework for IoT\n  Systems",
        "url": "http://arxiv.org/abs/2407.05506v1",
        "pub_date": "2024-07-07",
        "summary": "We consider access control for IoT systems that involves shared accesses to\nthe IoT devices as well as their data. Since IoT devices are dispersed all over\nthe edge of the Internet, traditional centralized access control has problems.\nBlockchain based decentralized access control is thus the new solution trend.\nHowever, existing blockchain based access control methods do not focus on\nperformance issues and may incur a high communication overhead.\n  In this paper, we develop a Pruned Blockchain based Access Control (PBAC)\nprotocol to cutdown the unnecessary message rounds and achieve high efficiency\nin access validations and policy management. The protocol includes a shortcut\nand a Role and Device Hierarchy-Based Access Control (R&amp;D-BAC) approaches for\ndifferent environment settings. To realize the PBAC protocol, it is necessary\nto carefully engineer the system architecture, which is also discussed in the\npaper. Experiments demonstrate the efficacy of the PBAC protocol, specifically,\nthe shortcut mechanism reduces access time by approximately 43%, and R&amp;D-BAC\noutperforms traditional blockchain based RBAC by more than two folds.",
        "authors": "Yongtao Huang, I-Ling Yen, Farokh Bastani",
        "translated": "我们考虑物联网系统的访问控制，包括对物联网设备及其数据的共享访问。由于物联网设备分散在互联网的各个边缘，传统的集中式访问控制存在问题。基于区块链的分散访问控制是解决这一问题的新趋势。然而，现有的基于区块链的访问控制方法并不关注性能问题，可能会导致较高的通信开销。本文提出了一种基于剪枝区块链的访问控制(PBAC)协议，以减少不必要的消息轮次，实现高效的访问验证和策略管理。该协议包括一个快捷方式和一个用于不同环境设置的基于角色和设备层次结构的访问控制(R & D-BAC)方法。为了实现 PBAC 协议，需要精心设计系统的体系结构，本文还对此进行了讨论。实验证明了 PBAC 协议的有效性，特别是该快捷机制将访问时间减少了约43% ，并且 R & D-BAC 的性能比传统的基于区块链的 RBAC 提高了两倍以上。"
    },
    {
        "title": "The infrastructure powering IBM's Gen AI model development",
        "url": "http://arxiv.org/abs/2407.05467v1",
        "pub_date": "2024-07-07",
        "summary": "AI Infrastructure plays a key role in the speed and cost-competitiveness of\ndeveloping and deploying advanced AI models. The current demand for powerful AI\ninfrastructure for model training is driven by the emergence of generative AI\nand foundational models, where on occasion thousands of GPUs must cooperate on\na single training job for the model to be trained in a reasonable time.\nDelivering efficient and high-performing AI training requires an end-to-end\nsolution that combines hardware, software and holistic telemetry to cater for\nmultiple types of AI workloads. In this report, we describe IBM's hybrid cloud\ninfrastructure that powers our generative AI model development. This\ninfrastructure includes (1) Vela: an AI-optimized supercomputing capability\ndirectly integrated into the IBM Cloud, delivering scalable, dynamic,\nmulti-tenant and geographically distributed infrastructure for large-scale\nmodel training and other AI workflow steps and (2) Blue Vela: a large-scale,\npurpose-built, on-premises hosting environment that is optimized to support our\nlargest and most ambitious AI model training tasks. Vela provides IBM with the\ndual benefit of high performance for internal use along with the flexibility to\nadapt to an evolving commercial landscape. Blue Vela provides us with the\nbenefits of rapid development of our largest and most ambitious models, as well\nas future-proofing against the evolving model landscape in the industry. Taken\ntogether, they provide IBM with the ability to rapidly innovate in the\ndevelopment of both AI models and commercial offerings.",
        "authors": "Talia Gershon, Seetharami Seelam, Brian Belgodere, Milton Bonilla, Lan Hoang, Danny Barnett, I-Hsin Chung, Apoorve Mohan, Ming-Hung Chen, Lixiang Luo, Robert Walkup, Constantinos Evangelinos, Shweta Salaria, Marc Dombrowa, Yoonho Park, Apo Kayi, Liran Schour, Alim Alim, Ali Sydney, Pavlos Maniotis, Laurent Schares, Bernard Metzler, Bengi Karacali-Akyamac, Sophia Wen, Tatsuhiro Chiba, Sunyanan Choochotkaew, Takeshi Yoshimura, Claudia Misale, Tonia Elengikal, Kevin O Connor, Zhuoran Liu, Richard Molina, Lars Schneidenbach, James Caden, Christopher Laibinis, Carlos Fonseca, Vasily Tarasov, Swaminathan Sundararaman, Frank Schmuck, Scott Guthridge, Jeremy Cohn, Marc Eshel, Paul Muench, Runyu Liu, William Pointer, Drew Wyskida, Bob Krull, Ray Rose, Brent Wolfe, William Cornejo, John Walter, Colm Malone, Clifford Perucci, Frank Franco, Nigel Hinds, Bob Calio, Pavel Druyan, Robert Kilduff, John Kienle, Connor McStay, Andrew Figueroa, Matthew Connolly, Edie Fost, Gina Roma, Jake Fonseca, Ido Levy, Michele Payne, Ryan Schenkel, Amir Malki, Lion Schneider, Aniruddha Narkhede, Shekeba Moshref, Alexandra Kisin, Olga Dodin, Bill Rippon, Henry Wrieth, John Ganci, Johnny Colino, Donna Habeger-Rose, Rakesh Pandey, Aditya Gidh, Aditya Gaur, Dennis Patterson, Samsuddin Salmani, Rambilas Varma, Rumana Rumana, Shubham Sharma, Aditya Gaur, Mayank Mishra, Rameswar Panda, Aditya Prasad, Matt Stallone, Gaoyuan Zhang, Yikang Shen, David Cox, Ruchir Puri, Dakshi Agrawal, Drew Thorstensen, Joel Belog, Brent Tang, Saurabh Kumar Gupta, Amitabha Biswas, Anup Maheshwari, Eran Gampel, Jason Van Patten, Matthew Runion, Sai Kaki, Yigal Bogin, Brian Reitz, Steve Pritko, Shahan Najam, Surya Nambala, Radhika Chirra, Rick Welp, Frank DiMitri, Felipe Telles, Amilcar Arvelo, King Chu, Ed Seminaro, Andrew Schram, Felix Eickhoff, William Hanson, Eric Mckeever, Dinakaran Joseph, Piyush Chaudhary, Piyush Shivam, Puneet Chaudhary, Wesley Jones, Robert Guthrie, Chris Bostic, Rezaul Islam, Steve Duersch, Wayne Sawdon, John Lewars, Matthew Klos, Michael Spriggs, Bill McMillan, George Gao, Ashish Kamra, Gaurav Singh, Marc Curry, Tushar Katarki, Joe Talerico, Zenghui Shi, Sai Sindhur Malleni, Erwan Gallen",
        "translated": "人工智能基础设施在开发和部署先进人工智能模型的速度和成本竞争力方面发挥着关键作用。目前对强大的人工智能模型培训基础设施的需求是由生成性人工智能和基础模型的出现驱动的，有时数千个图形处理器必须在一个单一的培训工作上进行合作，以便在合理的时间内对模型进行培训。提供高效和高性能的人工智能培训需要一个端到端的解决方案，结合硬件、软件和整体遥测，以满足多种类型的人工智能工作负荷。在本报告中，我们描述了 IBM 的混合云基础设施，它为我们的生成式人工智能模型开发提供了动力。这些基础设施包括: (1) Vela: 一种人工智能优化的超级计算能力，直接集成到 IBM 云中，为大规模模型培训和其他人工智能工作流程步骤提供可扩展的、动态的、多租户和地理分布的基础设施; (2) Blue Vela: 一个大规模的、专门建造的、内部托管环境，经过优化以支持我们最大和最雄心勃勃的人工智能模型培训任务。Vela 为 IBM 提供了内部使用的高性能以及适应不断变化的商业环境的灵活性的双重优势。蓝色船帆为我们提供了我们的最大和最雄心勃勃的模型快速发展的好处，以及未来证明对不断发展的模型景观的行业。总之，它们为 IBM 提供了在人工智能模型和商业产品开发方面快速创新的能力。"
    },
    {
        "title": "Shared Randomness Helps with Local Distributed Problems",
        "url": "http://arxiv.org/abs/2407.05445v1",
        "pub_date": "2024-07-07",
        "summary": "By prior work, we have many results related to distributed graph algorithms\nfor problems that can be defined with local constraints; the formal framework\nused in prior work is locally checkable labeling problems (LCLs), introduced by\nNaor and Stockmeyer in the 1990s. It is known, for example, that if we have a\ndeterministic algorithm that solves an LCL in $o(\\log n)$ rounds, we can speed\nit up to $O(\\log^*n)$ rounds, and if we have a randomized $O(\\log^*n)$ rounds\nalgorithm, we can derandomize it for free.\n  It is also known that randomness helps with some LCL problems: there are LCL\nproblems with randomized complexity $\\Theta(\\log\\log n)$ and deterministic\ncomplexity $\\Theta(\\log n)$. However, so far there have not been any LCL\nproblems in which the use of shared randomness has been necessary; in all prior\nalgorithms it has been enough that the nodes have access to their own private\nsources of randomness.\n  Could it be the case that shared randomness never helps with LCLs? Could we\nhave a general technique that takes any distributed graph algorithm for any LCL\nthat uses shared randomness, and turns it into an equally fast algorithm where\nprivate randomness is enough?\n  In this work we show that the answer is no. We present an LCL problem $\\Pi$\nsuch that the round complexity of $\\Pi$ is $\\Omega(\\sqrt n)$ in the usual\nrandomized \\local model with private randomness, but if the nodes have access\nto a source of shared randomness, then the complexity drops to $O(\\log n)$.\n  As corollaries, we also resolve several other open questions related to the\nlandscape of distributed computing in the context of LCL problems. In\nparticular, problem $\\Pi$ demonstrates that distributed quantum algorithms for\nLCL problems strictly benefit from a shared quantum state. Problem $\\Pi$ also\ngives a separation between finitely dependent distributions and non-signaling\ndistributions.",
        "authors": "Alkida Balliu, Mohsen Ghaffari, Fabian Kuhn, Augusto Modanese, Dennis Olivetti, Mikaël Rabie, Jukka Suomela, Jara Uitto",
        "translated": "通过以前的工作，我们已经得到了许多与分布式图算法有关的结果，这些问题可以用局部约束来定义; 在以前的工作中使用的形式框架是局部可检查标记问题(LCLs) ，由 Naor 和 Stockmeyer 在20世纪90年代引入。例如，我们知道，如果我们有一个在 $o (log n) $round 中解决一个 LCL 的确定性算法，我们可以将它加速到 $O (log ^ * n) $round，如果我们有一个随机的 $O (log ^ * n) $round 算法，我们可以免费对它进行去随机化。众所周知，随机性有助于解决一些 LCL 问题: 存在具有随机复杂度的 LCL 问题 $Θ (log log n) $和确定性复杂度 $Θ (log n) $。然而，到目前为止还没有任何 LCL 问题需要使用共享的随机性; 在所有先前的算法中，节点已经足够访问它们自己的私有随机性来源。有没有可能共享的随机性从来不会对 LCLs 有帮助呢？我们能否有一个通用的技术，采取任何分布式图算法的任何 LCL 使用共享的随机性，并把它变成一个同样快速的算法，其中私有随机性是足够的？在这项工作中，我们表明，答案是否定的。我们提出了一个 LCL 问题 $Pi $，使得 $Pi $的整数复杂度在通常的具有私有随机性的随机局部模型中为 $Omega (sqrt n) $，但是如果节点能够访问共享随机性的来源，那么复杂度就下降到 $O (log n) $。作为推论，我们还解决了其他几个与拼箱问题背景下的分布式计算景观有关的悬而未决的问题。特别是，问题 $Pi $演示了用于 LCL 问题的分布式量子算法严格受益于共享量子状态。问题 $Pi $还给出了有限相关分布和非信令分布之间的分离。"
    },
    {
        "title": "A Fault Tolerance Mechanism for Hybrid Scientific Workflows",
        "url": "http://arxiv.org/abs/2407.05337v1",
        "pub_date": "2024-07-07",
        "summary": "In large distributed systems, failures are a daily event occurring\nfrequently, especially with growing numbers of computation tasks and locations\non which they are deployed. The advantage of representing an application with a\nworkflow is the possibility of exploiting Workflow Management System (WMS)\nfeatures such as portability. A relevant feature that some WMSs supply is\nreliability. Over recent years, the emergence of hybrid workflows has posed new\nand intriguing challenges by increasing the possibility of distributing\ncomputations involving heterogeneous and independent environments.\nConsequently, the number of possible points of failure in the execution\nincreased, creating different important challenges that are interesting to\nstudy. This paper presents the implementation of a fault tolerance mechanism\nfor hybrid workflows based on the recovery and rollback approach. A\nrepresentation of the hybrid workflows with the formal framework is provided,\ntogether with the experiments demonstrating the functionality of implementing\napproach.",
        "authors": "Alberto Mulone, Doriana Medić, Marco Aldinucci",
        "translated": "在大型分布式系统中，故障是经常发生的日常事件，特别是随着计算任务和部署它们的位置的增加。使用工作流表示应用程序的优势在于可以利用工作流管理系统(WMS)特性，比如可移植性。一些 WMS 提供的一个相关特性是可靠性。近年来，混合工作流的出现提出了新的和有趣的挑战，通过增加分布计算的可能性涉及异构和独立的环境。因此，执行过程中可能的失败点的数量增加了，产生了有趣的不同的重要挑战。提出了一种基于恢复和回滚的混合工作流容错机制的实现方法。提出了一种基于形式化框架的混合工作流表示方法，并通过实验验证了该方法的实现功能。"
    },
    {
        "title": "OpenDiLoCo: An Open-Source Framework for Globally Distributed\n  Low-Communication Training",
        "url": "http://arxiv.org/abs/2407.07852v1",
        "pub_date": "2024-07-10",
        "summary": "OpenDiLoCo is an open-source implementation and replication of the\nDistributed Low-Communication (DiLoCo) training method for large language\nmodels. We provide a reproducible implementation of the DiLoCo experiments,\noffering it within a scalable, decentralized training framework using the\nHivemind library. We demonstrate its effectiveness by training a model across\ntwo continents and three countries, while maintaining 90-95% compute\nutilization. Additionally, we conduct ablations studies focusing on the\nalgorithm's compute efficiency, scalability in the number of workers and show\nthat its gradients can be all-reduced using FP16 without any performance\ndegradation. Furthermore, we scale OpenDiLoCo to 3x the size of the original\nwork, demonstrating its effectiveness for billion parameter models.",
        "authors": "Sami Jaghouar, Jack Min Ong, Johannes Hagemann",
        "translated": "OpenDiLoCo 是用于大型语言模型的分布式低通信(DiLoCo)培训方法的开源实现和复制。我们提供了 DiLoCo 实验的可重复实现，使用 Hivemind 库在一个可伸缩的、分散的培训框架内提供它。我们通过在两个大陆和三个国家训练一个模型，同时保持90-95% 的计算利用率来证明它的有效性。此外，我们进行了消融研究，重点是该算法的计算效率，可扩展性的人数，并表明其梯度可以全部降低使用 FP16没有任何性能下降。此外，我们将 OpenDiLoCo 的规模扩展到原始作品的3倍，证明了其对数十亿参数模型的有效性。"
    },
    {
        "title": "Harnessing Integrated CPU-GPU System Memory for HPC: a first look into\n  Grace Hopper",
        "url": "http://arxiv.org/abs/2407.07850v1",
        "pub_date": "2024-07-10",
        "summary": "Memory management across discrete CPU and GPU physical memory is\ntraditionally achieved through explicit GPU allocations and data copy or\nunified virtual memory. The Grace Hopper Superchip, for the first time,\nsupports an integrated CPU-GPU system page table, hardware-level addressing of\nsystem allocated memory, and cache-coherent NVLink-C2C interconnect, bringing\nan alternative solution for enabling a Unified Memory system. In this work, we\nprovide the first in-depth study of the system memory management on the Grace\nHopper Superchip, in both in-memory and memory oversubscription scenarios. We\nprovide a suite of six representative applications, including the Qiskit\nquantum computing simulator, using system memory and managed memory. Using our\nmemory utilization profiler and hardware counters, we quantify and characterize\nthe impact of the integrated CPU-GPU system page table on GPU applications. Our\nstudy focuses on first-touch policy, page table entry initialization, page\nsizes, and page migration. We identify practical optimization strategies for\ndifferent access patterns. Our results show that as a new solution for unified\nmemory, the system-allocated memory can benefit most use cases with minimal\nporting efforts.",
        "authors": "Gabin Schieffer, Jacob Wahlgren, Jie Ren, Jennifer Faj, Ivy Peng",
        "translated": "跨离散 CPU 和 GPU 物理内存的内存管理传统上是通过显式的 GPU 分配和数据复制或统一的虚拟内存来实现的。Grace Hopper 超级芯片首次支持集成 CPU-GPU 系统页面表、系统分配内存的硬件级寻址以及缓存相关的 NVLink-C2C 互连，为支持统一内存系统带来了一种替代解决方案。在这项工作中，我们首次深入研究了 Grace Hopper 超级芯片的系统内存管理，包括内存和内存超订情况。我们提供了一套六个具有代表性的应用程序，包括 Qiskit 量子计算模拟器，使用系统内存和托管内存。使用我们的内存利用率分析器和硬件计数器，我们量化和描述了集成 CPU-GPU 系统页表对 GPU 应用程序的影响。我们的研究集中在第一触摸策略、页表条目初始化、页面大小和页面迁移。我们为不同的访问模式确定了实用的优化策略。我们的研究结果表明，作为一种新的统一内存解决方案，系统分配的内存可以在最小的移植工作量下使大多数用例受益。"
    },
    {
        "title": "Fine-Tuning Large Language Models with User-Level Differential Privacy",
        "url": "http://arxiv.org/abs/2407.07737v1",
        "pub_date": "2024-07-10",
        "summary": "We investigate practical and scalable algorithms for training large language\nmodels (LLMs) with user-level differential privacy (DP) in order to provably\nsafeguard all the examples contributed by each user. We study two variants of\nDP-SGD with: (1) example-level sampling (ELS) and per-example gradient\nclipping, and (2) user-level sampling (ULS) and per-user gradient clipping. We\nderive a novel user-level DP accountant that allows us to compute provably\ntight privacy guarantees for ELS. Using this, we show that while ELS can\noutperform ULS in specific settings, ULS generally yields better results when\neach user has a diverse collection of examples. We validate our findings\nthrough experiments in synthetic mean estimation and LLM fine-tuning tasks\nunder fixed compute budgets. We find that ULS is significantly better in\nsettings where either (1) strong privacy guarantees are required, or (2) the\ncompute budget is large. Notably, our focus on LLM-compatible training\nalgorithms allows us to scale to models with hundreds of millions of parameters\nand datasets with hundreds of thousands of users.",
        "authors": "Zachary Charles, Arun Ganesh, Ryan McKenna, H. Brendan McMahan, Nicole Mitchell, Krishna Pillutla, Keith Rush",
        "translated": "我们研究实用和可扩展的算法，用于训练大型语言模型(LLM)和用户级差分隐私(DP) ，以便可证明地保护每个用户提供的所有示例。我们研究了 DP-SGD 的两种变体: (1)示例级采样(ELS)和每个示例的梯度裁剪，以及(2)用户级采样(ULS)和每个用户的梯度裁剪。我们推导出一个新颖的用户级 DP 会计，它允许我们计算可证明的紧密的 ELS 隐私保证。使用这个，我们表明，虽然 ELS 可以在特定的设置优于 ULS，ULS 通常产生更好的结果时，每个用户有一个不同的例子集合。在固定计算预算下，通过合成均值估计和 LLM 微调任务的实验验证了我们的研究结果。我们发现 ULS 在需要(1)强隐私保护或(2)计算预算较大的情况下明显更好。值得注意的是，我们对 LLM 兼容的训练算法的关注使我们能够扩展到具有数亿个参数和数据集的模型和数十万个用户。"
    },
    {
        "title": "High-Performance Sorting-Based k-mer Counting in Distributed Memory with\n  Flexible Hybrid Parallelism",
        "url": "http://arxiv.org/abs/2407.07718v1",
        "pub_date": "2024-07-10",
        "summary": "In generating large quantities of DNA data, high-throughput sequencing\ntechnologies require advanced bioinformatics infrastructures for efficient data\nanalysis. k-mer counting, the process of quantifying the frequency of\nfixed-length k DNA subsequences, is a fundamental step in various\nbioinformatics pipelines, including genome assembly and protein prediction. Due\nto the growing volume of data, the scaling of the counting process is critical.\nIn the literature, distributed memory software uses hash tables, which exhibit\npoor cache friendliness and consume excessive memory. They often also lack\nsupport for flexible parallelism, which makes integration into existing\nbioinformatics pipelines difficult. In this work, we propose HySortK, a highly\nefficient sorting-based distributed memory k-mer counter. HySortK reduces the\ncommunication volume through a carefully designed communication scheme and\ndomain-specific optimization strategies. Furthermore, we introduce an abstract\ntask layer for flexible hybrid parallelism to address load imbalances in\ndifferent scenarios. HySortK achieves a 2-10x speedup compared to the GPU\nbaseline on 4 and 8 nodes. Compared to state-of-the-art CPU software, HySortK\nachieves up to 2x speedup while reducing peak memory usage by 30% on 16 nodes.\nFinally, we integrated HySortK into an existing genome assembly pipeline and\nachieved up to 1.8x speedup, proving its flexibility and practicality in\nreal-world scenarios.",
        "authors": "Yifan Li, Giulia Guidi",
        "translated": "在生成大量 DNA 数据时，高通量测序技术需要先进的生物信息学基础设施来进行有效的数据分析。K-mer 计数是对固定长度 k DNA 子序列的频率进行量化的过程，是包括基因组组装和蛋白质预测在内的各种生物信息学管道中的基本步骤。由于数据量的增长，计数过程的缩放是至关重要的。在文献中，分布式存储软件使用哈希表，这表现出很差的缓存友好性和消耗过多的内存。它们还常常缺乏对灵活并行性的支持，这使得很难集成到现有的生物信息学管道中。在这项工作中，我们提出了 HySortK，一个高效的基于排序的分布式内存 k-mer 计数器。HySortK 通过精心设计的通信方案和特定于领域的优化策略来减少通信量。此外，我们还引入了一个抽象的任务层来实现灵活的混合并行性，以解决不同场景下的负载不平衡问题。HySortK 在4和8个节点上实现了2-10倍于 GPU 基线的加速。与最先进的 CPU 软件相比，HySortK 在16个节点上实现了高达2倍的加速，同时将峰值内存使用减少了30% 。最后，我们将 HySortK 集成到一个现有的基因组装配流水线中，实现了1.8倍的加速，证明了它在现实场景中的灵活性和实用性。"
    },
    {
        "title": "A Transverse-Read-assisted Valid-Bit Collection to Accelerate Stochastic\n  Conmputing MAC for Energy-Efficient in-RTM DNNs",
        "url": "http://arxiv.org/abs/2407.07476v1",
        "pub_date": "2024-07-10",
        "summary": "It looks very attractive to coordinate racetrack-memory(RM) and\nstochastic-computing (SC) jointly to build an ultra-low power\nneuron-architecture.However,the above combination has always been questioned in\na fatal weakness that the narrow bit-view of the RM-MTJ\nstructure,a.k.a.shift-and-access pattern,cannot physically match the great\nthroughput of direct-stored stochastic sequences.Fortunately,a recently\ndeveloped Transverse-Read(TR) provides a wider segment-view to RM via detecting\nthe resistance of domain-walls between a couple of MTJs on single\nnanowire,therefore RM can be enhanced with a faster access to the sequences\nwithout any substantial domain-shift.To utilize TR for a power-efficient\nSC-DNNs, in this work, we propose a segment-based compression to leverage\none-cycle TR to only read those kernel segments of stochastic\nsequences,meanwhile,remove a large number of redundant segments for ultra-high\nstorage density.In decompression stage,the low-discrepancy stochastic sequences\ncan be quickly reassembled by a select-and-output loop using kernel segments\nrather than slowly regenerated by costly SNGs.Since TR can provide an ideal\nin-memory acceleration in one-counting, counter-free SC-MACs are designed and\ndeployed near RMs to form a power-efficient neuron-architecture,in which,the\nbinary results of TR are activated straightforward without sluggish APCs.The\nresults show that under the TR aided RM model,the power efficiency,speed,and\nstochastic accuracy of Seed-based Fast Stochastic Computing significantly\nenhance the performance of DNNs.The speed of computation is 2.88x faster in\nLenet-5 and 4.40x faster in VGG-19 compared to the CORUSCANT model.The\nintegration of TR with RTM is deployed near the memory to create a\npower-efficient neuron architecture, eliminating the need for slow Accumulative\nParallel Counters (APCs) and improving access speed to stochastic sequences.",
        "authors": "Jihe Wang, Zhiying Zhang, Xingwu Dong, Danghui Wang",
        "translated": "将赛道存储器(RM)和随机计算(SC)结合起来构建超低功耗的神经元体系结构非常有吸引力。然而，上述组合一直受到质疑，致命的弱点是 RM-MTJ 结构的窄比特视图，即移位和访问模式，不能在物理上匹配直接存储的随机序列的巨大吞吐量。幸运的是，最近开发的 Transverse-Read (TR)通过检测单根纳米线上两个 MTJ 之间的域壁电阻为 RM 提供了更广泛的片段视图，因此 RM 可以通过更快地访问序列而不需要任何实质性的域移位来增强。为了利用 TR 实现高功耗的 SC-DNN，本文提出了一种基于分段的压缩方法，该方法利用单周期 TR 只读取随机序列的核心片段，同时为超高的存储密度去除大量冗余片段。在解压缩阶段，低差异随机序列可以通过使用核心片段的选择-输出循环快速重新组装，而不是由昂贵的 SNG 缓慢重新生成。由于 TR 可以在一次计数中提供理想的内存加速，因此在 RM 附近设计和部署无计数器的 SC-MAC 以形成功率效率高的神经元结构，其中 TR 的二进制结果直接激活而没有缓慢的 APC。结果表明，在 TR 辅助 RM 模型下，基于种子的快速随机计算的功率效率、速度和随机精度显著提高了 DNN 的性能。与 CORUSCANT 模型相比，Lenet-5的计算速度快2.88倍，VGG-19的计算速度快4.40倍。TR 与 RTM 的集成部署在内存附近，以创建一个高效的神经元架构，消除了对缓慢的累积并行计数器(APC)的需要，并提高了对随机序列的访问速度。"
    },
    {
        "title": "Federated PCA on Grassmann Manifold for IoT Anomaly Detection",
        "url": "http://arxiv.org/abs/2407.07421v1",
        "pub_date": "2024-07-10",
        "summary": "With the proliferation of the Internet of Things (IoT) and the rising\ninterconnectedness of devices, network security faces significant challenges,\nespecially from anomalous activities. While traditional machine learning-based\nintrusion detection systems (ML-IDS) effectively employ supervised learning\nmethods, they possess limitations such as the requirement for labeled data and\nchallenges with high dimensionality. Recent unsupervised ML-IDS approaches such\nas AutoEncoders and Generative Adversarial Networks (GAN) offer alternative\nsolutions but pose challenges in deployment onto resource-constrained IoT\ndevices and in interpretability. To address these concerns, this paper proposes\na novel federated unsupervised anomaly detection framework, FedPCA, that\nleverages Principal Component Analysis (PCA) and the Alternating Directions\nMethod Multipliers (ADMM) to learn common representations of distributed\nnon-i.i.d. datasets. Building on the FedPCA framework, we propose two\nalgorithms, FEDPE in Euclidean space and FEDPG on Grassmann manifolds. Our\napproach enables real-time threat detection and mitigation at the device level,\nenhancing network resilience while ensuring privacy. Moreover, the proposed\nalgorithms are accompanied by theoretical convergence rates even under a\nsubsampling scheme, a novel result. Experimental results on the UNSW-NB15 and\nTON-IoT datasets show that our proposed methods offer performance in anomaly\ndetection comparable to nonlinear baselines, while providing significant\nimprovements in communication and memory efficiency, underscoring their\npotential for securing IoT networks.",
        "authors": "Tung-Anh Nguyen, Long Tan Le, Tuan Dung Nguyen, Wei Bao, Suranga Seneviratne, Choong Seon Hong, Nguyen H. Tran",
        "translated": "随着物联网(IoT)的普及和设备互联性的提高，网络安全面临着严峻的挑战，尤其是来自异常活动的挑战。虽然传统的基于机器学习的入侵检测系统(ML-IDS)有效地采用了监督式学习检测方法，但它们也存在一些局限性，例如对标记数据的需求以及高维数的挑战。最近的无监督机器学习入侵检测方法，如自动编码器和生成对抗网络(GAN)提供了替代解决方案，但在部署到资源受限的物联网设备和可解释性方面提出了挑战。为了解决这些问题，本文提出了一种新型的联邦无监督异常检测框架——联邦主成分分析法(fedPCA) ，它利用主成分分析(PCA)和交替方向法乘法器(ADMM)来学习分布式非 i.d. 数据集的通用表示。在 FedPCA 框架的基础上，提出了两种算法: 欧氏空间 FEDPE 算法和 Grassmann 流形 FEDPG 算法。我们的方法能够在设备级实时检测和缓解威胁，增强网络弹性，同时确保隐私。此外，即使在次采样情况下，该算法也具有理论收敛速度，这是一个新的结果。在 UNSW-nb15和 TON-IoT 数据集上的实验结果表明，我们提出的方法在异常检测上提供了与非线性基线相当的性能，同时在通信和存储效率方面提供了显著的改进，强调了它们在保护物联网方面的潜力。"
    },
    {
        "title": "BoostCom: Towards Efficient Universal Fully Homomorphic Encryption by\n  Boosting the Word-wise Comparisons",
        "url": "http://arxiv.org/abs/2407.07308v1",
        "pub_date": "2024-07-10",
        "summary": "Fully Homomorphic Encryption (FHE) allows for the execution of computations\non encrypted data without the need to decrypt it first, offering significant\npotential for privacy-preserving computational operations. Emerging\narithmetic-based FHE schemes (ar-FHE), like BGV, demonstrate even better\nperformance in word-wise comparison operations over non-arithmetic FHE (na-FHE)\nschemes, such as TFHE, especially for basic tasks like comparing values,\nfinding maximums, and minimums. This shows the universality of ar-FHE in\neffectively handling both arithmetic and non-arithmetic operations without the\nexpensive conversion between arithmetic and non-arithmetic FHEs. We refer to\nuniversal arithmetic Fully Homomorphic Encryption as uFHE. The arithmetic\noperations in uFHE remain consistent with those in the original arithmetic FHE,\nwhich have seen significant acceleration. However, its non-arithmetic\ncomparison operations differ, are slow, and have not been as thoroughly studied\nor accelerated. In this paper, we introduce BoostCom, a scheme designed to\nspeed up word-wise comparison operations, enhancing the efficiency of uFHE\nsystems. BoostCom involves a multi-prong optimizations including infrastructure\nacceleration (Multi-level heterogeneous parallelization and GPU-related\nimprovements), and algorithm-aware optimizations (slot compaction, non-blocking\ncomparison semantic). Together, BoostCom achieves an end-to-end performance\nimprovement of more than an order of magnitude (11.1x faster) compared to the\nstate-of-the-art CPU-based uFHE systems, across various FHE parameters and\ntasks.",
        "authors": "Ardhi Wiratama Baskara Yudha, Jiaqi Xue, Qian Lou, Huiyang Zhou, Yan Solihin",
        "translated": "完全同态加密(fHE)允许对加密数据执行计算，而不需要首先解密，为保护隐私的计算操作提供了巨大的潜力。新兴的基于算术的 FHE (ar-FHE)方案，如 BGV，比非算术的 FHE (na-FHE)方案(如 TFHE)在字比较操作方面表现出更好的性能，特别是对于比较值、寻找最大值和最小值等基本任务。这表明了 ar-FHE 在有效地处理算术和非算术操作方面的普遍性，而不需要在算术和非算术 FHE 之间进行昂贵的转换。我们把通用算术完全称为 ufHE 同态加密。UFHE 中的算术运算与原始算术 FHE 中的算术运算保持一致，并有显著的加速度。然而，它的非算术比较操作不同，速度较慢，并且没有得到充分的研究或加速。在本文中，我们介绍了 BoostCom，这是一个设计来加速逐字比较操作，提高 uFHE 系统效率的方案。BoostCom 涉及多方面的优化，包括基础设施加速(多级异构并行化和与 GPU 相关的改进)和算法感知优化(插槽压缩、非阻塞比较语义)。与最先进的基于 CPU 的 uFHE 系统相比，BoostCom 在各种 FHE 参数和任务方面实现了端到端性能的提高，比数量级(11.1倍的速度)快得多。"
    },
    {
        "title": "Metron: Holistic Performance Evaluation Framework for LLM Inference\n  Systems",
        "url": "http://arxiv.org/abs/2407.07000v1",
        "pub_date": "2024-07-09",
        "summary": "Serving large language models (LLMs) in production can incur substantial\ncosts, which has prompted recent advances in inference system optimizations.\nToday, these systems are evaluated against conventional latency and throughput\nmetrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics\nfail to fully capture the nuances of LLM inference, leading to an incomplete\nassessment of user-facing performance crucial for real-time applications such\nas chat and translation. In this paper, we first identify the pitfalls of\ncurrent performance metrics in evaluating LLM inference systems. We then\npropose Metron, a comprehensive performance evaluation framework that includes\nfluidity-index -- a novel metric designed to reflect the intricacies of the LLM\ninference process and its impact on real-time user experience. Finally, we\nevaluate various existing open-source platforms and model-as-a-service\nofferings using Metron, discussing their strengths and weaknesses. Metron is\navailable at https://github.com/project-metron/metron.",
        "authors": "Amey Agrawal, Anmol Agarwal, Nitin Kedia, Jayashree Mohan, Souvik Kundu, Nipun Kwatra, Ramachandran Ramjee, Alexey Tumanov",
        "translated": "在生产环境中使用大型语言模型(LLM)可能会产生巨大的成本，这促进了推理系统优化方面的最新进展。目前，这些系统是根据传统的延迟和吞吐量指标进行评估的(例如。TTFT，TBT，归一化延迟和 TPOT)。然而，这些指标无法完全捕捉 LLM 推理的细微差别，导致对实时应用程序(如聊天和翻译)至关重要的用户面向性能的不完整评估。在本文中，我们首先确定当前性能指标在评估 LLM 推理系统中的缺陷。然后，我们提出了 Metron，一个包括流动性指数的综合性能评估框架——一个新的度量标准，旨在反映 LLM 推理过程的复杂性及其对实时用户体验的影响。最后，我们评估了使用 Metron 的各种现有开源平台和模型即服务(model-as-a-service)产品，讨论了它们的优缺点。麦特隆 https://github.com/project-Metron/Metron 有空。"
    },
    {
        "title": "SP-Chain: Boosting Intra-Shard and Cross-Shard Security and Performance\n  in Blockchain Sharding",
        "url": "http://arxiv.org/abs/2407.06953v1",
        "pub_date": "2024-07-09",
        "summary": "A promising way to overcome the scalability limitations of the current\nblockchain is to use sharding, which is to split the transaction processing\namong multiple, smaller groups of nodes. A well-performed blockchain sharding\nsystem requires both high performance and high security in both intra- and\ncross-shard perspectives. However, existing protocols either have issues on\nprotecting security or trade off great performance for security. In this paper,\nwe propose SP-Chain, a blockchain sharding system with enhanced Security and\nPerformance for both intra- and cross-shard perspectives. For intra-shard\naspect, we design a two-phase concurrent voting scheme to provide high system\nthroughput and low transaction confirmation latency. Moreover, we propose an\nefficient unbiased leader rotation scheme to ensure high performance under\nmalicious behavior. For cross-shard aspect, a proof-assisted efficient\ncross-shard transaction processing mechanism is proposed to guard the\ncross-shard transactions with low overhead. We implement SP-Chain based on\nHarmony, and evaluate its performance via large-scale deployment. Extensive\nevaluations suggest that SP-Chain can process more than 10,000 tx/sec under\nmalicious behaviors with a confirmation latency of 7.6s in a network of 4,000\nnodes.",
        "authors": "Mingzhe Li, You Lin, Wei Wang, Jin Zhang",
        "translated": "克服当前区块链可伸缩性限制的一个有前途的方法是使用分片，即在多个较小的节点组之间分割事务处理。一个良好表现的区块链分片系统需要高性能和高安全性都在内部和跨碎片的观点。然而，现有的协议要么存在保护安全性的问题，要么牺牲了很好的安全性能。在本文中，我们提出了 SP 链，一个区块链分片系统的增强安全性和性能的内部和跨碎片的观点。对于内部分片方面，我们设计了一个两阶段并发投票方案，以提供高系统吞吐量和低事务确认延迟。此外，我们提出了一个有效的无偏领导者轮换方案，以确保在恶意行为下的高性能。在跨碎片方面，提出了一种证明辅助的高效跨碎片事务处理机制，以较低的开销保护跨碎片事务。我们实现了基于 Harmony 的 SP 链，并通过大规模部署对其性能进行了评估。广泛的评估表明，SP-Chain 可以在4000个节点的网络中处理超过10,000 tx/sec 的恶意行为，确认延迟为7.6 s。"
    },
    {
        "title": "DL-Chain: Scalable and Stable Blockchain Sharding with High Concurrency\n  via Dual-Layer Consensus",
        "url": "http://arxiv.org/abs/2407.06882v1",
        "pub_date": "2024-07-09",
        "summary": "Sharding enhances blockchain scalability by partitioning nodes into multiple\ngroups for concurrent transaction processing. Configuring a large number of\n\\emph{small shards} helps improve the transaction concurrency of a sharding\nsystem. However, it increases the fraction of malicious nodes within each\nshard, easily leading to shard corruption and jeopardizing system security.\nSome existing works have attempted to improve concurrency by reducing the shard\nsize while maintaining security. However, they often require frequent and\ntime-consuming recovery of corrupted shards, leading to severe system\nstagnation. Also, they usually require network-wide consensus to guarantee\nsecurity, which limits scalability.\n  To address these issues, we propose DL-Chain, a blockchain sharding system\nthat can securely provide \\emph{high concurrency with stable and scalable\nperformance.} Our core idea is a \\underline{D}ual-\\underline{L}ayer\narchitecture and consensus, which consists of numerous smaller proposer shards\n(PSs) for transaction processing and multiple larger finalizer committees (FCs)\nfor transaction finalization. To avoid system stagnation and thus guarantee\nstable performance, we ensure PSs' liveness even if they are corrupted through\nthe cooperation of PSs and FCs, thus eliminating the recovery process of\ncorrupted PSs. To better trade-off security and scalability, we fine-tune the\nFCs to enable multiple FCs to coexist securely. As a result, DL-Chain allows a\nlarger fraction of malicious nodes in each PS ($&lt;1/2$) and thus can securely\nconfigure smaller shards for boosted stable and scalable concurrency.\nEvaluation results show that DL-Chain achieves up to 10 times improvement in\nthroughput compared to existing solutions and provides stable concurrency with\nup to 2,550 nodes.",
        "authors": "You Lin, Mingzhe Li, Qingsong Wei, Yong Liu, Siow Mong Rick Goh, Jin Zhang",
        "translated": "分片通过将节点划分为多个组进行并发事务处理来增强区块链的可伸缩性。配置大量 emph { small shards }有助于提高分片系统的事务并发性。但是，它增加了每个碎片中恶意节点的比例，很容易导致碎片损坏并危及系统安全。一些现有的工作已经尝试通过在维护安全性的同时减少碎片大小来改进并发性。然而，它们通常需要频繁且耗时地恢复损坏的碎片，从而导致严重的系统停滞。此外，它们通常需要网络范围内的一致同意来保证安全性，这限制了可伸缩性。为了解决这些问题，我们提出了 DL 链，一个区块链分片系统，可以安全地提供高并发性和稳定的可伸缩性能。}我们的核心思想是一个下划线{ D }双下划线{ L }层架构和共识，其由用于事务处理的许多较小的提议者碎片(PSS)和用于事务终结的多个较大的终结者委员会(FC)组成。为了避免系统停滞，从而保证稳定的性能，我们即使通过 PSs 和 FC 的协作，也可以保证 PSs 的活性，从而消除了被损坏的 PSs 的恢复过程。为了更好地兼顾安全性和可扩展性，我们对功能界面进行了微调，使多个功能界面能够安全共存。因此，DL-Chain 允许每个 PS 中有更大比例的恶意节点($< 1/2 $) ，因此可以安全地配置更小的碎片，以提高稳定性和可伸缩性并发性。评估结果表明，与现有解决方案相比，DL-Chain 在吞吐量方面实现了高达10倍的提高，并提供了多达2,550个节点的稳定并发性。"
    }
]