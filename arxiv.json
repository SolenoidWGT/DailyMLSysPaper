[
    {
        "title": "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs",
        "url": "http://arxiv.org/abs/2407.03234v1",
        "pub_date": "2024-07-03",
        "summary": "When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\nmade available at https://github.com/Linlt-leon/Adversarial-Alignments.",
        "translated": "当 LLM 部署在敏感的、面向人的设置中时，关键是不要输出不安全的、有偏见的或侵犯隐私的输出。出于这个原因，模特们都接受过训练，也接受过指导，拒绝回答诸如“告诉我如何制造炸弹”之类的不安全提示我们发现，尽管有这些保护措施，仅仅通过在模型输入的末尾添加一个空格，就有可能打破模型的防御。在一项对八个开源模型的研究中，我们证明了这种攻击足够强大，足以导致大多数模型产生有害的输出，成功率非常高。我们检查了这种行为的原因，发现在标记化的训练数据中出现单个空格的上下文鼓励模型在提示时生成列表，覆盖训练信号以拒绝回答不安全的请求。我们的研究结果强调了当前模型对齐的脆弱状态，并提出了开发更强大的对齐方法的重要性。代码和数据将在 https://github.com/linlt-leon/adversarial-alignments 公布。"
    },
    {
        "title": "Bridging Model Heterogeneity in Federated Learning via Uncertainty-based\n  Asymmetrical Reciprocity Learning",
        "url": "http://arxiv.org/abs/2407.03247v1",
        "pub_date": "2024-07-03",
        "summary": "This paper presents FedType, a simple yet pioneering framework designed to\nfill research gaps in heterogeneous model aggregation within federated learning\n(FL). FedType introduces small identical proxy models for clients, serving as\nagents for information exchange, ensuring model security, and achieving\nefficient communication simultaneously. To transfer knowledge between large\nprivate and small proxy models on clients, we propose a novel uncertainty-based\nasymmetrical reciprocity learning method, eliminating the need for any public\ndata. Comprehensive experiments conducted on benchmark datasets demonstrate the\nefficacy and generalization ability of FedType across diverse settings. Our\napproach redefines federated learning paradigms by bridging model\nheterogeneity, eliminating reliance on public data, prioritizing client\nprivacy, and reducing communication costs.",
        "authors": "Jiaqi Wang, Chenxu Zhao, Lingjuan Lyu, Quanzeng You, Mengdi Huai, Fenglong Ma",
        "translated": "本文介绍了 FedType，这是一个简单而具有开创性的框架，旨在填补联邦学习(FL)中异构模型聚合的研究空白。FedType 为客户端引入了小型相同的代理模型，作为信息交换的代理，保证了模型的安全性，同时实现了高效的通信。针对大型私有代理模型和小型代理模型之间的知识传递问题，提出了一种基于不确定性的非对称互惠学习方法，该方法不需要任何公开数据。在基准数据集上进行的综合实验证明了 FedType 在不同环境下的有效性和推广能力。我们的方法通过桥接模型异构性、消除对公共数据的依赖、优先考虑客户隐私和降低通信成本来重新定义联邦学习范例。"
    },
    {
        "title": "Streaming Large-Scale Electron Microscopy Data to a Supercomputing\n  Facility",
        "url": "http://arxiv.org/abs/2407.03215v1",
        "pub_date": "2024-07-03",
        "summary": "Data management is a critical component of modern experimental workflows. As\ndata generation rates increase, transferring data from acquisition servers to\nprocessing servers via conventional file-based methods is becoming increasingly\nimpractical. The 4D Camera at the National Center for Electron Microscopy\n(NCEM) generates data at a nominal rate of 480 Gbit/s (87,000 frames/s)\nproducing a 700 GB dataset in fifteen seconds. To address the challenges\nassociated with storing and processing such quantities of data, we developed a\nstreaming workflow that utilizes a high-speed network to connect the 4D\nCamera's data acquisition (DAQ) system to supercomputing nodes at the National\nEnergy Research Scientific Computing Center (NERSC), bypassing intermediate\nfile storage entirely. In this work, we demonstrate the effectiveness of our\nstreaming pipeline in a production setting through an hour-long experiment that\ngenerated over 10 TB of raw data, yielding high-quality datasets suitable for\nadvanced analyses. Additionally, we compare the efficacy of this streaming\nworkflow against the conventional file-transfer workflow by conducting a\npost-mortem analysis on historical data from experiments performed by real\nusers. Our findings show that the streaming workflow significantly improves\ndata turnaround time, enables real-time decision-making, and minimizes the\npotential for human error by eliminating manual user interactions.",
        "authors": "Samuel S. Welborn, Chris Harris, Stephanie M. Ribet, Georgios Varnavides, Colin Ophus, Bjoern Enders, Peter Ercius",
        "translated": "数据管理是现代实验工作流的重要组成部分。随着数据生成率的提高，通过传统的基于文件的方法将数据从采集服务器传输到处理服务器正变得越来越不切实际。美国国家电子显微镜中心(NCEM)的4D 相机以480Gbit/s (87000帧/s)的标称速率生成数据，在15秒内产生700GB 的数据集。为了解决存储和处理这样大量数据的挑战，我们开发了一个流式工作流，利用高速网络将4D 摄像机的数据采集(DAQ)系统连接到国家能源研究科学计算中心(NERSC)的超级计算节点，完全绕过中间文件存储。在这项工作中，我们通过一个小时的实验，生成了超过10TB 的原始数据，产生了适合高级分析的高质量数据集，从而证明了我们的流式流水线在生产环境中的有效性。此外，我们通过对实际用户实验中的历史数据进行事后分析，比较了该流工作流与传统文件传输工作流的效率。我们的研究结果表明，流式工作流可以显著提高数据周转时间，实现实时决策，并通过消除人工用户交互，最大限度地减少潜在的人为错误。"
    },
    {
        "title": "Effective Heterogeneous Federated Learning via Efficient\n  Hypernetwork-based Weight Generation",
        "url": "http://arxiv.org/abs/2407.03086v1",
        "pub_date": "2024-07-03",
        "summary": "While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86 times compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.",
        "authors": "Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim, JeongGil Ko",
        "translated": "虽然联邦学习利用分布式客户端资源，但是由于异构客户端功能，它面临着挑战。这就需要分配适合客户端资源的模型和仔细的参数聚合来适应这种异构性。我们提出了一种新的联邦学习框架 HypeMeFed，它通过将多出口网络结构与基于超网络的模型权重生成相结合来支持客户端的异构性。该方法对异构模型层的特征空间进行对齐，解决了权重聚合过程中各层的信息差异问题。为了实现 HypeMeFed，我们还提出了一种低秩因子分解方法，以最小化与超网络相关的计算和内存开销。我们对现实世界异构设备测试台的评估表明，与天真的超网络方法相比，HypeMeFed 比 FedAvg 提高了5.12% 的准确性，减少了98.22% 的超网络内存需求，并将其操作加速了1.86倍。这些结果证明了 HypeMeFed 在利用和吸引异构客户机进行联合学习方面的有效性。"
    },
    {
        "title": "On the Client Preference of LLM Fine-tuning in Federated Learning",
        "url": "http://arxiv.org/abs/2407.03038v1",
        "pub_date": "2024-07-03",
        "summary": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained\nlarge language model (LLM) using preference datasets, enabling the LLM to\ngenerate outputs that align with human preferences. Given the sensitive nature\nof these preference datasets held by various clients, there is a need to\nimplement RLHF within a federated learning (FL) framework, where clients are\nreluctant to share their data due to privacy concerns. To address this, we\nintroduce a feasible framework in which clients collaboratively train a binary\nselector with their preference datasets using our proposed FedBis. With a\nwell-trained selector, we can further enhance the LLM that generates\nhuman-preferred completions. Meanwhile, we propose a novel algorithm,\nFedBiscuit, that trains multiple selectors by organizing clients into balanced\nand disjoint clusters based on their preferences. Compared to the FedBis,\nFedBiscuit demonstrates superior performance in simulating human preferences\nfor pairwise completions. Our extensive experiments on federated human\npreference datasets -- marking the first benchmark to address heterogeneous\ndata partitioning among clients -- demonstrate that FedBiscuit outperforms\nFedBis and even surpasses traditional centralized training.",
        "authors": "Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Jing Gao",
        "translated": "强化学习反馈(rlHF)使用偏好数据集对预先训练好的大语言模型(LLM)进行微调，使 LLM 能够生成符合人类偏好的输出。鉴于这些偏好数据集由不同的客户端持有的敏感性质，有必要在联邦学习(FL)框架内实现 RLHF，其中客户端由于隐私问题不愿意共享他们的数据。为了解决这个问题，我们引入了一个可行的框架，在这个框架中，客户端使用我们提出的 FedBis 协作地训练一个二进制选择器和他们的偏好数据集。使用训练有素的选择器，我们可以进一步增强生成人类首选完成的 LLM。同时，我们提出了一种新的算法，FedBiscookie，通过将客户端根据他们的偏好组织成平衡的和不相交的集群来训练多个选择器。与 FedBis 相比，FedBiscookie 在模拟人类对配对完成的偏好方面表现出了优越的性能。我们在联邦人类偏好数据集上的广泛实验——标志着解决客户端之间异构数据分区的第一个基准——证明了 FedBiscue 的性能优于 FedBis，甚至超过了传统的集中式训练。"
    },
    {
        "title": "DRLQ: A Deep Reinforcement Learning-based Task Placement for Quantum\n  Cloud Computing",
        "url": "http://arxiv.org/abs/2407.02748v1",
        "pub_date": "2024-07-03",
        "summary": "The quantum cloud computing paradigm presents unique challenges in task\nplacement due to the dynamic and heterogeneous nature of quantum computation\nresources. Traditional heuristic approaches fall short in adapting to the\nrapidly evolving landscape of quantum computing. This paper proposes DRLQ, a\nnovel Deep Reinforcement Learning (DRL)-based technique for task placement in\nquantum cloud computing environments, addressing the optimization of task\ncompletion time and quantum task scheduling efficiency. It leverages the Deep Q\nNetwork (DQN) architecture, enhanced with the Rainbow DQN approach, to create a\ndynamic task placement strategy. This approach is one of the first in the field\nof quantum cloud resource management, enabling adaptive learning and\ndecision-making for quantum cloud environments and effectively optimizing task\nplacement based on changing conditions and resource availability. We conduct\nextensive experiments using the QSimPy simulation toolkit to evaluate the\nperformance of our method, demonstrating substantial improvements in task\nexecution efficiency and a reduction in the need to reschedule quantum tasks.\nOur results show that utilizing the DRLQ approach for task placement can\nsignificantly reduce total quantum task completion time by 37.81% to 72.93% and\nprevent task rescheduling attempts compared to other heuristic approaches.",
        "authors": "Hoa T. Nguyen, Muhammad Usman, Rajkumar Buyya",
        "translated": "由于量子计算资源的动态性和异构性，量子云计算范式在任务配置中提出了独特的挑战。传统的启发式方法不能适应量子计算的快速发展。本文提出了基于深度强化学习的量子云计算任务分配技术 DRLQ，解决了任务完成时间和量子任务调度效率的优化问题。它利用 Deep Q Network (DQN)体系结构，并通过 Rainbow DQN 方法得到了增强，从而创建了一个动态任务分配策略。这种方法是量子云资源管理领域的首创之一，能够为量子云环境提供在线机机器学习和决策支持，并根据不断变化的条件和资源可用性有效地优化任务分配。我们使用 QSimPy 模拟工具包进行了广泛的实验，以评估我们的方法的性能，证明了任务执行效率的实质性改进和重新调度量子任务的需求的减少。实验结果表明，与其他启发式方法相比，利用 DRLQ 方法进行任务分配可以显著减少任务完成总时间37.81% 到72.93% ，并且可以防止任务重调度尝试。"
    },
    {
        "title": "Accelerating Distributed Optimization: A Primal-Dual Perspective on\n  Local Steps",
        "url": "http://arxiv.org/abs/2407.02689v1",
        "pub_date": "2024-07-02",
        "summary": "In distributed machine learning, efficient training across multiple agents\nwith different data distributions poses significant challenges. Even with a\ncentralized coordinator, current algorithms that achieve optimal communication\ncomplexity typically require either large minibatches or compromise on gradient\ncomplexity. In this work, we tackle both centralized and decentralized settings\nacross strongly convex, convex, and nonconvex objectives. We first demonstrate\nthat a basic primal-dual method, (Accelerated) Gradient Ascent Multiple\nStochastic Gradient Descent (GA-MSGD), applied to the Lagrangian of distributed\noptimization inherently incorporates local updates, because the inner loops of\nrunning Stochastic Gradient Descent on the primal variable require no\ninter-agent communication. Notably, for strongly convex objectives, we show\n(Accelerated) GA-MSGD achieves linear convergence in communication rounds\ndespite the Lagrangian being only linear in the dual variables. This is due to\na unique structural property where the dual variable is confined to the span of\nthe coupling matrix, rendering the dual problem strongly concave. When\nintegrated with the Catalyst framework, our approach achieves nearly optimal\ncommunication complexity across various settings without the need for\nminibatches. Moreover, in stochastic decentralized problems, it attains\ncommunication complexities comparable to those in deterministic settings,\nimproving over existing algorithms.",
        "authors": "Junchi Yang, Murat Yildirim, Qiu Feng",
        "translated": "在分布式机器学习中，对具有不同数据分布的多代理进行有效的训练是一个巨大的挑战。即使有一个集中的协调器，当前的算法，以实现最佳的通信复杂度通常需要或者大的微型批量或梯度复杂度折衷。在这项工作中，我们处理集中和分散设置跨强烈凸，凸和非凸的目标。我们首先证明了一个基本的原始-对偶方法，(加速)梯度上升多重随机梯度下降(GA-MSGD) ，应用于分布式优化的拉格朗日方法，固有地结合了局部更新，因为在原始变量上运行的随机梯度下降的内部循环不需要代理之间的通信。值得注意的是，对于强凸目标，我们显示(加速) GA-MSGD 实现线性收敛的通信轮，尽管拉格朗日只是线性对偶变量。这是由于一个独特的结构性质，其中对偶变量限制在耦合矩阵的跨度，使对偶问题强烈凹。当与 Catalyst 框架集成时，我们的方法在不需要小批处理的情况下实现了几乎最佳的跨各种设置的通信复杂性。此外，在随机分散问题中，它获得了与确定性设置中的通信复杂度相当的通信复杂度，比现有算法有所改进。"
    },
    {
        "title": "Towards Federated Learning with On-device Training and Communication in\n  8-bit Floating Point",
        "url": "http://arxiv.org/abs/2407.02610v1",
        "pub_date": "2024-07-02",
        "summary": "Recent work has shown that 8-bit floating point (FP8) can be used for\nefficiently training neural networks with reduced computational overhead\ncompared to training in FP32/FP16. In this work, we investigate the use of FP8\ntraining in a federated learning context. This brings not only the usual\nbenefits of FP8 which are desirable for on-device training at the edge, but\nalso reduces client-server communication costs due to significant weight\ncompression. We present a novel method for combining FP8 client training while\nmaintaining a global FP32 server model and provide convergence analysis.\nExperiments with various machine learning models and datasets show that our\nmethod consistently yields communication reductions of at least 2.9x across a\nvariety of tasks and models compared to an FP32 baseline.",
        "authors": "Bokun Wang, Axel Berg, Durmus Alp Emre Acar, Chuteng Zhou",
        "translated": "最近的研究表明，与 FP32/FP16相比，8位浮点数(FP8)可以用来有效地训练神经网络，减少计算开销。在这项工作中，我们研究了 FP8训练在联邦学习环境中的应用。这不仅带来了 FP8的通常好处，这对边缘设备上的培训是可取的，而且由于显著的权重压缩，还降低了客户机-服务器通信成本。我们提出了一种新的方法，结合 FP8客户端训练，同时维护一个全球性的 FP32服务器模型，并提供了收敛性分析。对各种机器学习模型和数据集的实验表明，与 FP32基线相比，我们的方法在各种任务和模型之间始终产生至少2.9倍的通信减少。"
    },
    {
        "title": "Decentralized Intelligence Network (DIN)",
        "url": "http://arxiv.org/abs/2407.02461v1",
        "pub_date": "2024-07-02",
        "summary": "Decentralized Intelligence Network (DIN) addresses the significant challenges\nof data sovereignty and AI utilization caused by the fragmentation and siloing\nof data across providers and institutions. This comprehensive framework\novercomes access barriers to scalable data sources previously hindered by silos\nby leveraging: 1) personal data stores as a prerequisite for data sovereignty;\n2) a scalable federated learning protocol implemented on a public blockchain\nfor decentralized AI training, where data remains with participants and only\nmodel parameter updates are shared; and 3) a scalable, trustless rewards\nmechanism to incentivize participation and ensure fair reward distribution.\nThis framework ensures that no entity can prevent or control access to training\non data offered by participants or determine financial benefits, as these\nprocesses operate on a public blockchain with an immutable record and without a\nthird party. It supports effective AI training, allowing participants to\nmaintain control over their data, benefit financially, and contribute to a\ndecentralized, scalable ecosystem that leverages collective AI to develop\nbeneficial algorithms.",
        "authors": "Abraham Nash",
        "translated": "分散智能网(DIN)解决了数据主权和人工智能利用方面的重大挑战，这些挑战是由于数据在供应商和机构之间的碎片化和孤立性造成的。这个全面的框架通过利用以下方面克服了以前受到竖井阻碍的可扩展数据源的访问障碍: 1)个人数据存储作为数据主权的先决条件; 2)在公共区块链上实施的可扩展联合学习协议，用于分散的 AI 培训，其中数据保留在参与者身上，只有模型参数更新被共享; 3)可扩展的、不可信任的奖励机制，以激励参与并确保公平的奖励分配。这一框架确保任何实体都无法防止或控制获得参与者提供的数据培训或确定经济利益，因为这些过程是在公共区块链上运作的，具有不可变的记录，没有第三方。它支持有效的人工智能培训，允许参与者保持对他们的数据的控制，从经济上获益，并促进一个分散的，可扩展的生态系统，利用集体人工智能开发有益的算法。"
    },
    {
        "title": "Uncertainty-Aware Decarbonization for Datacenters",
        "url": "http://arxiv.org/abs/2407.02390v1",
        "pub_date": "2024-07-02",
        "summary": "This paper represents the first effort to quantify uncertainty in carbon\nintensity forecasting for datacenter decarbonization. We identify and analyze\ntwo types of uncertainty -- temporal and spatial -- and discuss their system\nimplications. To address the temporal dynamics in quantifying uncertainty for\ncarbon intensity forecasting, we introduce a conformal prediction-based\nframework. Evaluation results show that our technique robustly achieves target\ncoverages in uncertainty quantification across various significance levels. We\nconduct two case studies using production power traces, focusing on temporal\nand spatial load shifting respectively. The results show that incorporating\nuncertainty into scheduling decisions can prevent a 5% and 14% increase in\ncarbon emissions, respectively. These percentages translate to an absolute\nreduction of 2.1 and 10.4 tons of carbon emissions in a 20 MW datacenter\ncluster.",
        "authors": "Amy Li, Sihang Liu, Yi Ding",
        "translated": "本文首次尝试对数据中心脱碳过程中碳强度预测的不确定性进行量化。我们识别和分析了两种类型的不确定性——时间和空间——并讨论了它们的系统含义。为了解决碳强度预测不确定性量化的时间动态问题，我们引入了一个基于保形预测的框架。评价结果表明，该方法在不同的显著性水平上均能稳健地实现不确定性量化的目标覆盖。我们使用生产功率轨迹进行了两个案例研究，分别侧重于时间和空间负荷转移。结果表明，将不确定性纳入调度决策可以分别防止5% 和14% 的碳排放增加。这些百分比意味着在一个20兆瓦的数据中心集群中，碳排放绝对减少了2.1吨和10.4吨。"
    },
    {
        "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across\n  Hybrid Devices",
        "url": "http://arxiv.org/abs/2407.02327v1",
        "pub_date": "2024-07-02",
        "summary": "A number of production deep learning clusters have attempted to explore\ninference hardware for DNN training, at the off-peak serving hours with many\ninference GPUs idling. Conducting DNN training with a combination of\nheterogeneous training and inference GPUs, known as hybrid device training,\npresents considerable challenges due to disparities in compute capability and\nsignificant differences in memory capacity. We propose QSync, a training system\nthat enables efficient synchronous data-parallel DNN training over hybrid\ndevices by strategically exploiting quantized operators. According to each\ndevice's available resource capacity, QSync selects a quantization-minimized\nsetting for operators in the distributed DNN training graph, minimizing model\naccuracy degradation but keeping the training efficiency brought by\nquantization. We carefully design a predictor with a bi-directional\nmixed-precision indicator to reflect the sensitivity of DNN layers on\nfixed-point and floating-point low-precision operators, a replayer with a\nneighborhood-aware cost mapper to accurately estimate the latency of\ndistributed hybrid mixed-precision training, and then an allocator that\nefficiently synchronizes workers with minimized model accuracy degradation.\nQSync bridges the computational graph on PyTorch to an optimized backend for\nquantization kernel performance and flexible support for various GPU\narchitectures. Extensive experiments show that QSync's predictor can accurately\nsimulate distributed mixed-precision training with &lt;5% error, with a consistent\n0.27-1.03% accuracy improvement over the from-scratch training tasks compared\nto uniform precision.",
        "authors": "Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Yibo Zhu, Chuan Wu",
        "translated": "一些生产深度学习集群试图探索 DNN 培训的推理硬件，在非高峰服务时间与许多推理 GPU 闲置。用异构训练和推理 GPU (称为混合设备训练)组合进行 DNN 训练，由于计算能力的差异和内存容量的显著差异，提出了相当大的挑战。我们提出 QSync，一个训练系统，使有效的同步数据并行 DNN 训练超过混合设备的战略利用量化操作员。QSync 根据每个设备的可用资源容量，在分布式 DNN 训练图中为操作者选择一个量化最小化的设置，最小化模型精度的降低，同时保持量化带来的训练效率。我们精心设计了一个双向混合精度指标的预测器来反映 DNN 层对定点和浮点低精度算子的敏感性，一个具有邻域感知成本映射器的中继器来精确估计分布式混合精度训练的延迟，然后一个分配器来有效地同步工人，最小化模型精度退化。QSync 将 PyTorch 上的计算图连接到一个优化的后端，用于量化内核性能和对各种 GPU 架构的灵活支持。大量实验表明，QSync 预测器能够准确地模拟分布式混合精度训练，误差小于5% ，与均匀精度训练相比，准确率提高了0.27 -1.03% 。"
    },
    {
        "title": "Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models\n  with Adaptive Expert Placement",
        "url": "http://arxiv.org/abs/2407.04656v1",
        "pub_date": "2024-07-05",
        "summary": "Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly\nbeen adopted to further scale large language models (LLMs) due to its\nsub-linear scaling for computation costs. However, frequent failures still pose\nsignificant challenges as training scales. The cost of even a single failure is\nsignificant, as all GPUs need to wait idle until the failure is resolved,\npotentially losing considerable training progress as training has to restart\nfrom checkpoints. Existing solutions for efficient fault-tolerant training\neither lack elasticity or rely on building resiliency into pipeline\nparallelism, which cannot be applied to MoE models due to the expert\nparallelism strategy adopted by the MoE architecture.\n  We present Lazarus, a system for resilient and elastic training of MoE\nmodels. Lazarus adaptively allocates expert replicas to address the inherent\nimbalance in expert workload and speeds-up training, while a provably optimal\nexpert placement algorithm is developed to maximize the probability of recovery\nupon failures. Through adaptive expert placement and a flexible token\ndispatcher, Lazarus can also fully utilize all available nodes after failures,\nleaving no GPU idle. Our evaluation shows that Lazarus outperforms existing MoE\ntraining systems by up to 5.7x under frequent node failures and 3.4x on a real\nspot instance trace.",
        "authors": "Yongji Wu, Wenjie Qu, Tianyang Tao, Zhuang Wang, Wei Bai, Zhuohao Li, Yuan Tian, Jiaheng Zhang, Matthew Lentz, Danyang Zhuo",
        "translated": "稀疏激活的混合专家(MoE)体系结构由于其计算开销的次线性扩展性，越来越多地被用于进一步扩展大型语言模型(LLM)。然而，频繁的失败仍然给培训规模带来了巨大的挑战。即使是单个故障的代价也是巨大的，因为所有的 GPU 都需要等待空闲，直到故障得到解决，这可能会失去相当大的训练进度，因为训练必须从检查点重新开始。现有的高效容错培训解决方案要么缺乏弹性，要么依赖于将弹性构建为流水线并行性，由于教育部体系结构采用的专家并行策略，这些解决方案不能应用于教育部模型。我们提出 Lazarus，一个系统的弹性和弹性训练的 MoE 模型。Lazarus 自适应地分配专家副本，以解决专家工作量固有的不平衡问题，加快训练速度，同时开发了一个可证明的最优专家配置算法，以最大限度地提高故障恢复的概率。通过自适应专家布局和灵活的令牌调度器，Lazarus 还可以在故障后充分利用所有可用的节点，不让 GPU 处于空闲状态。我们的评估表明，Lazarus 在频繁节点故障下的性能比现有的 MoE 培训系统高出5.7倍，在实际现场实例跟踪上的性能高出3.4倍。"
    },
    {
        "title": "Accelerating Communication in Deep Learning Recommendation Model\n  Training with Dual-Level Adaptive Lossy Compression",
        "url": "http://arxiv.org/abs/2407.04272v1",
        "pub_date": "2024-07-05",
        "summary": "DLRM is a state-of-the-art recommendation system model that has gained\nwidespread adoption across various industry applications. The large size of\nDLRM models, however, necessitates the use of multiple devices/GPUs for\nefficient training. A significant bottleneck in this process is the\ntime-consuming all-to-all communication required to collect embedding data from\nall devices. To mitigate this, we introduce a method that employs error-bounded\nlossy compression to reduce the communication data size and accelerate DLRM\ntraining. We develop a novel error-bounded lossy compression algorithm,\ninformed by an in-depth analysis of embedding data features, to achieve high\ncompression ratios. Moreover, we introduce a dual-level adaptive strategy for\nerror-bound adjustment, spanning both table-wise and iteration-wise aspects, to\nbalance the compression benefits with the potential impacts on accuracy. We\nfurther optimize our compressor for PyTorch tensors on GPUs, minimizing\ncompression overhead. Evaluation shows that our method achieves a 1.38$\\times$\ntraining speedup with a minimal accuracy impact.",
        "authors": "Hao Feng, Boyuan Zhang, Fanjiang Ye, Min Si, Ching-Hsiang Chu, Jiannan Tian, Chunxing Yin,  Zhaoxia,  Deng, Yuchen Hao, Pavan Balaji, Tong Geng, Dingwen Tao",
        "translated": "DLRM 是最先进的推荐系统模型，已经在各种行业应用程序中得到广泛采用。然而，DLRM 模型的大规模需要使用多个设备/GPU 进行有效的培训。这个过程中的一个重要瓶颈是从所有设备收集嵌入数据所需的耗时的全对全通信。为了缓解这种情况，我们引入了一种使用错误限制有损数据压缩的方法，以减少通信数据量并加速 DLRM 培训。我们开发了一个新的错误限制有损数据压缩算法，通过深入分析嵌入数据特征，以实现高压缩比。此外，我们还引入了一个跨越表和迭代两个方面的误差范围调整的双层自适应策略，以平衡压缩优势和对精度的潜在影响。我们在 GPU 上进一步优化我们的 PyTorch 张量压缩器，使压缩开销最小化。评估结果表明，该方法在精度影响最小的情况下，获得了1.38美元的训练加速比。"
    },
    {
        "title": "A High-Quality Workflow for Multi-Resolution Scientific Data Reduction\n  and Visualization",
        "url": "http://arxiv.org/abs/2407.04267v1",
        "pub_date": "2024-07-05",
        "summary": "Multi-resolution methods such as Adaptive Mesh Refinement (AMR) can enhance\nstorage efficiency for HPC applications generating vast volumes of data.\nHowever, their applicability is limited and cannot be universally deployed\nacross all applications. Furthermore, integrating lossy compression with\nmulti-resolution techniques to further boost storage efficiency encounters\nsignificant barriers. To this end, we introduce an innovative workflow that\nfacilitates high-quality multi-resolution data compression for both uniform and\nAMR simulations. Initially, to extend the usability of multi-resolution\ntechniques, our workflow employs a compression-oriented Region of Interest\n(ROI) extraction method, transforming uniform data into a multi-resolution\nformat. Subsequently, to bridge the gap between multi-resolution techniques and\nlossy compressors, we optimize three distinct compressors, ensuring their\noptimal performance on multi-resolution data. Lastly, we incorporate an\nadvanced uncertainty visualization method into our workflow to understand the\npotential impacts of lossy compression. Experimental evaluation demonstrates\nthat our workflow achieves significant compression quality improvements.",
        "authors": "Daoce Wang, Pascal Grosset, Jesus Pulido, Tushar M. Athawale, Jiannan Tian, Kai Zhao, Zarija Lukic, Axel Huebl, Zhe Wang, James Ahrens, Dingwen Tao",
        "translated": "自适应网格细化(AMR)等多分辨率方法可以提高 HPC 应用程序生成大量数据的存储效率。但是，它们的适用性是有限的，并且不能在所有应用程序中普遍部署。此外，将有损数据压缩与多分辨率技术相结合以进一步提高存储效率会遇到重大障碍。为此，我们引入了一个创新的工作流程，为统一模拟和 AMR 模拟提供高质量的多分辨率数据压缩。首先，为了扩展多分辨率技术的可用性，我们的工作流采用了面向压缩的感兴趣区域(ROI)提取方法，将统一的数据转换成多分辨率格式。随后，为了弥补多分辨率技术和有损压缩器之间的差距，我们对三种不同的压缩器进行了优化，以确保它们在多分辨率数据上的最优性能。最后，我们将一种先进的不确定性可视化方法融入到我们的工作流程中，以了解有损数据压缩的潜在影响。实验结果表明，我们的工作流程实现了显著的压缩质量改进。"
    },
    {
        "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
        "url": "http://arxiv.org/abs/2407.04053v1",
        "pub_date": "2024-07-04",
        "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyse data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. The goal of Edge AI is to optimize data processing efficiency\nand velocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research, spanning from 2014 to the present, it has\nshown significant and rapid development over the last five years. In this\narticle, we present a systematic literature review for Edge AI to discuss the\nexisting research, recent advancements, and future research directions. We\ncreated a collaborative edge AI learning system for cloud and edge computing\nanalysis, including an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while also examining its potential influence\nacross many fields through compassing infrastructure, cloud computing, fog\ncomputing, services, use cases, ML and deep learning, and resource management.\nThis study highlights the significance of Edge AI in processing real-time data\nat the edge of the network. Additionally, it emphasizes the research challenges\nencountered by Edge AI systems, including constraints on resources,\nvulnerabilities to security threats, and problems with scalability. Finally,\nthis study highlights the potential future research directions that aim to\naddress the current limitations of Edge AI by providing innovative solutions.",
        "authors": "Sukhpal Singh Gill, Muhammed Golec, Jianmin Hu, Minxian Xu, Junhui Du, Huaming Wu, Guneet Kaur Walia, Subramaniam Subramanian Murugesan, Babar Ali, Mohit Kumar, Kejiang Ye, Prabal Verma, Surendra Kumar, Felix Cuadrado, Steve Uhlig",
        "translated": "边缘人工智能(AI)集成了一个互联系统和设备的网络，这些设备接收、缓存、处理和分析数据，与用 AI 技术捕获数据的位置进行密切通信。人工智能效率的最新进展，物联网(IoT)设备的广泛使用，以及边缘计算的出现，已经开启了边缘人工智能的巨大范围。边缘人工智能的目标是优化数据处理效率和速度，同时确保数据的机密性和完整性。尽管从2014年到现在，它还是一个相对较新的研究领域，但在过去的五年里，它已经显示出重大而迅速的发展。在本文中，我们提出了一个系统的文献综述，边缘人工智能讨论现有的研究，最近的进展和未来的研究方向。我们创建了一个用于云和边缘计算分析的协作式边缘 AI 学习系统，包括对促进这种机制的体系结构的深入研究。边缘人工智能的分类促进了边缘人工智能系统的分类和配置，同时也通过包括基础设施、云计算、雾计算、服务、用例、机器学习和深度学习以及资源管理在许多领域的潜在影响。该研究突出了边缘人工智能在处理网络边缘实时数据中的重要性。此外，它强调了边缘人工智能系统所面临的研究挑战，包括资源约束、安全威胁的脆弱性和可伸缩性问题。最后，本研究强调了未来可能的研究方向，旨在通过提供创新的解决方案来解决边缘人工智能目前的局限性。"
    },
    {
        "title": "Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM\n  Inference on Heterogeneous Systems",
        "url": "http://arxiv.org/abs/2407.04014v1",
        "pub_date": "2024-07-04",
        "summary": "The rapid adoption of large language models (LLMs) has led to significant\nadvances in natural language processing and text generation. However, the\nenergy consumed through LLM model inference remains a major challenge for\nsustainable AI deployment. To address this problem, we model the\nworkload-dependent energy consumption and runtime of LLM inference tasks on\nheterogeneous GPU-CPU systems. By conducting an extensive characterization\nstudy of several state-of-the-art LLMs and analyzing their energy and runtime\nbehavior across different magnitudes of input prompts and output text, we\ndevelop accurate (R^2&gt;0.96) energy and runtime models for each LLM. We employ\nthese models to explore an offline, energy-optimal LLM workload scheduling\nframework. Through a case study, we demonstrate the advantages of energy and\naccuracy aware scheduling compared to existing best practices.",
        "authors": "Grant Wilkins, Srinivasan Keshav, Richard Mortier",
        "translated": "大型语言模型(LLM)的迅速应用导致了自然语言处理和文本生成方面的重大进展。然而，通过 LLM 模型推理所消耗的能量仍然是可持续人工智能部署的一个主要挑战。为了解决这个问题，我们在异构 GPU-CPU 系统上建立了与工作负载相关的 LLM 推理任务的能量消耗和运行时模型。通过对几种最先进的 LLM 进行广泛的角色塑造研究，并分析它们在不同大小的输入提示和输出文本中的能量和运行时行为，我们为每种 LLM 开发了精确的(R ^ 2 > 0.96)能量和运行时模型。我们使用这些模型来探索一个离线的，能量最优的 LLM 工作负载调度框架。通过一个案例研究，我们证明了与现有的最佳实践相比，能源和精确性意识调度的优势。"
    },
    {
        "title": "PaSE: Parallelization Strategies for Efficient DNN Training",
        "url": "http://arxiv.org/abs/2407.04001v1",
        "pub_date": "2024-07-04",
        "summary": "Training a deep neural network (DNN) requires substantial computational and\nmemory requirements. It is common to use multiple devices to train a DNN to\nreduce the overall training time. There are several choices to parallelize each\nlayer in a DNN. Exhaustively searching this list to find an optimal\nparallelization strategy is prohibitively time consuming and impractical. The\nstandard practice is to use data parallelism because of its simplicity.\nHowever, data parallelism is often sub-optimal, and suffers from poor\nperformance and high memory requirement. Expert-designed strategies have been\nproposed on a case-by-case basis using domain specific knowledge. These\nexpert-designed strategies do not generalize well to DNNs other than the ones\nfor which they were designed, and are not always necessarily the best choice.\n  In this paper, we propose an approach to automatically find efficient\nparallelization strategies for DNNs from their computation graphs. We present\nan efficient algorithm to compute these strategies within a reasonable time in\npractice. We evaluate the effectiveness of our approach on various DNNs. We\nalso compare the performance of the strategies identified by our approach\nagainst data parallelism, expert-designed strategies, and the state-of-the-art\napproaches. Our results show that the strategies found using our approach\noutperform the baseline data parallelism strategy in all the cases. In\naddition, our strategies achieve better performance than the expert-designed\nstrategies and the state-of-the-art approaches.",
        "authors": "Venmugil Elango",
        "translated": "训练一个深层神经网络(DNN)需要大量的计算和内存需求。通常使用多种设备来训练一个 DNN，以减少整体训练时间。有几种选择可以并行化 DNN 中的每一层。彻底搜索此列表以找到最佳并行策略是非常耗时和不切实际的。标准做法是使用资料平行，因为它很简单。然而，资料平行往往是次优的，并且存在性能差和内存需求高的问题。利用具体领域的知识，逐案提出了专家设计的战略。这些专家设计的策略并不能很好地推广到 DNN 以外的其他设计策略，并不一定总是最佳选择。本文提出了一种从 DNN 的计算图中自动寻找有效并行策略的方法。我们提出了一个有效的算法，在合理的时间内计算这些策略在实践中。我们评估我们的方法在各种 DNN 上的有效性。我们还将我们的方法与资料平行、专家设计的策略和最先进的方法进行了比较。我们的研究结果表明，在所有情况下，使用我们的方法发现的策略都优于基线资料平行策略。此外，我们的策略比专家设计的策略和最先进的方法取得了更好的效果。"
    },
    {
        "title": "DEVS/SOA: A Cross-Platform Framework for Net-centric Modeling and\n  Simulation in DEVS Unified Process",
        "url": "http://arxiv.org/abs/2407.03686v1",
        "pub_date": "2024-07-04",
        "summary": "Discrete EVent Specification (DEVS) environments are known to be implemented\nover middleware systems such as HLA, RMI, CORBA and others. DEVS exhibits\nconcepts of systems theory and modeling and supports capturing the system\nbehavior from the physical and behavioral perspectives. Further, they are\nimplemented using Object-oriented languages like Java and C++. This research\nwork uses the Java platform to implement DEVS over a Service Oriented\nArchitecture (SOA) framework. Called the DEVS/SOA, the framework supports a\ndevelopment and testing environment known as DEVS Unified Process that is built\non a model-continuity-based life cycle methodology. DEVS Unified Process allows\nDEVS-based Modeling and Simulation (M&amp;S) over net-centric platforms using\nDEVS/SOA. This framework also provides the crucial feature of run-time\ncomposability of coupled systems using SOA. We describe the architecture and\ndesigns of the both the server and the client. The client application\ncommunicates with multiple servers hosting DEVS simulation services. These\nSimulation services are developed using the proposed symmetrical services\narchitecture wherein the server can act as both a service provider and a\nservice consumer contrary to the unidirectional client-server paradigm. We also\ndiscuss how this Services based architecture provides solutions for\ncross-platform distributed M&amp;S. We demonstrate DEVS/SOA framework with a\nscenario of Joint Close Air Support specified in Business Process Modeling\nNotation (BPMN). We also provide a real-world application of Network health\nmonitoring using DEVS/SOA layered architectural framework.",
        "authors": "Saurabh Mittal, José L. Risco-Martín, Bernard P. Zeigler",
        "translated": "众所周知，离散事件规范(DevS)环境是在 HLA、 RMI、 CORBA 等中间件系统上实现的。DevS 展示了系统理论和建模的概念，并支持从物理和行为角度捕获系统行为。此外，它们是使用面向对象的语言(如 Java 和 C + +)实现的。这项研究工作使用 Java 平台在一个面向服务的体系结构(SOA)框架下实现 DEVS。该框架被称为 devS/SOA，它支持一个名为 devS 统一过程的开发和测试环境，该环境建立在基于模型连续性的生命周期方法之上。在以网络为中心的平台上使用 devS/SOA，devS 统一过程允许基于 devS 的建模与模拟(M & S)。此框架还提供了使用 SOA 的耦合系统的运行时可组合性的关键特性。我们描述了服务器和客户端的体系结构和设计。客户端应用程序与多个承载 DevS 模拟服务的服务器通信。这些仿真服务是使用所提出的对称服务体系结构开发的，其中服务器既可以充当服务提供者，也可以充当服务使用者，这与单向客户机-服务器模式相反。我们还讨论了这种基于服务的体系结构如何为跨平台分布式 M & S 提供解决方案。我们用业务流程建模标记法中指定的联合近距离空中支援(bPMN)场景来演示 devS/SOA 框架。我们还提供了一个使用 DevS/SOA 分层体系结构框架的实际网络健康监控应用程序。"
    },
    {
        "title": "Loki: A System for Serving ML Inference Pipelines with Hardware and\n  Accuracy Scaling",
        "url": "http://arxiv.org/abs/2407.03583v1",
        "pub_date": "2024-07-04",
        "summary": "The rapid adoption of machine learning (ML) has underscored the importance of\nserving ML models with high throughput and resource efficiency. Traditional\napproaches to managing increasing query demands have predominantly focused on\nhardware scaling, which involves increasing server count or computing power.\nHowever, this strategy can often be impractical due to limitations in the\navailable budget or compute resources. As an alternative, accuracy scaling\noffers a promising solution by adjusting the accuracy of ML models to\naccommodate fluctuating query demands. Yet, existing accuracy scaling\ntechniques target independent ML models and tend to underperform while managing\ninference pipelines. Furthermore, they lack integration with hardware scaling,\nleading to potential resource inefficiencies during low-demand periods. To\naddress the limitations, this paper introduces Loki, a system designed for\nserving inference pipelines effectively with both hardware and accuracy\nscaling. Loki incorporates an innovative theoretical framework for optimal\nresource allocation and an effective query routing algorithm, aimed at\nimproving system accuracy and minimizing latency deadline violations. Our\nempirical evaluation demonstrates that through accuracy scaling, the effective\ncapacity of a fixed-size cluster can be enhanced by more than $2.7\\times$\ncompared to relying solely on hardware scaling. When compared with\nstate-of-the-art inference-serving systems, Loki achieves up to a $10\\times$\nreduction in Service Level Objective (SLO) violations, with minimal compromises\non accuracy and while fulfilling throughput demands.",
        "authors": "Sohaib Ahmad, Hui Guan, Ramesh K. Sitaraman",
        "translated": "机器学习(ML)的快速应用突出了为机器学习模型提供高吞吐量和资源效率服务的重要性。管理日益增长的查询需求的传统方法主要集中在硬件扩展上，这涉及到增加服务器数量或计算能力。但是，由于可用预算或计算资源的限制，这种策略通常不切实际。作为一种替代方案，精度缩放提供了一个有希望的解决方案，通过调整机器学习模型的精度，以适应波动的查询需求。然而，现有的精度缩放技术针对的是独立的机器学习模型，在管理推理流水线时往往表现不佳。此外，它们缺乏与硬件扩展的集成，导致在低需求时期潜在的资源低效。针对这些局限性，本文介绍了 Loki 系统，该系统设计用于有效地为推理管道提供硬件和精度扩展。Loki 采用了一个创新的资源优化分配理论框架和一个有效的查询路由算法，旨在提高系统的准确性和最大限度地减少延迟最后期限违规。我们的实证评估表明，通过精度扩展，固定规模集群的有效容量可以比单纯依靠硬件扩展提高2.7倍以上。与最先进的推理服务系统相比，Loki 在满足吞吐量需求的同时，在准确性方面做出最小的妥协，从而在服务水平目标(SLO)违规方面实现了高达10倍的减少。"
    }
]